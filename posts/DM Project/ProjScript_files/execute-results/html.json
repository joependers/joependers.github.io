{
  "hash": "58d9744a130b2003245bc5ae54451369",
  "result": {
    "markdown": "---\ntitle: \"Traffic Accidents in the Twin Cities Before and After COVID\"\nAuthor: \"Joe Penders\"\nformat: html\ncategory: projects\n---\n\n\n\n\nThis dataset contains records of traffic accidents for the seven-county region overseen by the metropolitan council of the Twin Cities.\n\nTraffic Accident data source: Sobhan Moosavi. (2023). <i>US Accidents (2016 - 2023)</i> \\[Data set\\]. Kaggle. https://doi.org/10.34740/KAGGLE/DS/199387\n\nMinnesota shape files came from https://gisdata.mn.gov/organization/us-mn-state-dot\n\n# Loading the data and creating new variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read in the original, national dataset\n# US_Accidents_March23 <- read_csv(\"US_Accidents_March23.csv\")\n\n# subset Minnesota\n# mn <- US_Accidents_March23 %>%\n# filter(State == \"MN\")\n# write_csv(mn, \"mn.csv\")\n# mn <- read_csv(\"mn.csv\")\n# mn <- mn %>% select(-\"ID\", -\"Source\", -\"State\", -\"Country\")\n\n# subset counties in the Twin Cities metro area\n# met <- mn %>%\n#  filter(County %in% c('Anoka', 'Carver', 'Dakota', 'Hennepin', 'Ramsey',\n#                       'Scott', 'Washington'))\n# write_csv(met, \"met.csv\")\nmet = read_csv(\"met.csv\")\n\n# Sample 1000 random rows for testing purposes\n# met_1k <- met %>% sample_n(1000)\n# write_csv(met_1k, \"met_1k.csv\")\n\n# Load the shapefile for streets\nstreets <- st_read(\"streets/STREETS_LOAD.shp\")\n\n# Load the shape file for county boundaries\ncounties <- st_read(\"counties/mn_county_boundaries_1000.shp\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creating New Variables\n# Create COVID variable\nmet$post_covid <- ifelse(met$Start_Time < as.POSIXct(\"2020-03-01\", tz = \"UTC-6\"), \n                      0, 1)\n\n# Day of week variable (0 = Sunday, 6 = Saturday)\nmet$Day_of_Week <- as.integer(format(met$Start_Time, \"%w\"))\n\n# Create the binary 'weekday' variable (1 for weekdays, 0 for weekends)\nmet$is_weekday <- ifelse(met$Day_of_Week >= 1 & met$Day_of_Week <= 5, 1, 0)\n\n# Month variable\nmet$Month <- as.integer(format(met$Start_Time, \"%m\"))\n\n# Create the 'season' variable\nmet$season <- ifelse(met$Month %in% c(3, 4, 5), \"Spring\",\n                ifelse(met$Month %in% c(6, 7, 8), \"Summer\",\n                  ifelse(met$Month %in% c(9, 10, 11), \"Autumn\",\n                    \"Winter\")))\n\n# Create rush hour variable\n# Extract hour from the Start_Time column\nmet$start_hour <- as.integer(format(met$Start_Time, \"%H\"))\n\n# rush hour variable, 1 if the hour is between 6 and 9 or between 15 and 18 on weekdays, 0 otherwise\nmet$rush_hr <- ifelse(met$is_weekday == 1 & (met$start_hour >= 6 & met$start_hour <= 9 | met$start_hour >= 15 & met$start_hour <= 18), 1, 0)\n\n# Drop no longer needed variables\nmet$Day_of_Week <- NULL\nmet$Month <- NULL\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert the data frame to an sf object\nmet_sf <- st_as_sf(met, coords = c(\"Start_Lng\", \"Start_Lat\"), crs = 4326, agr = \"constant\")\n\n# Check the CRS for both datasets\ncrs_streets <- st_crs(streets)\ncrs_met <- st_crs(met_sf)\n\n# Transform the projection of the accidents data to match the streets data, if different\nif (crs_streets$epsg != crs_met$epsg) {\n  met_sf <- st_transform(met_sf, crs_streets)\n}\n\n# Remove extra dimensions from spatial data\nstreets <- sf::st_zm(streets)\ncounties <- sf::st_zm(counties)\nmet_sf <- sf::st_zm(met_sf)\n```\n:::\n\n\n\n# Exploratory data analysis\n\nIn Spring of 2020 there was a noticeable shift in traffic patterns as a result of the COVID-19 pandemic. Many people were no longer commuting to work everyday, and years later this trend continues as working from home becomes more accepted as a permanent change. Understanding how the shift in traffic patterns affects the characteristics of car accidents will allow us to better allocate public funds for traffic engineering projects and hopefully minimize the rate and severity of accidents.\n\n### Locations of Traffic Accidents in Twin Cities Metro (June 2016 through March 2023)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the bounding box of the accidents data\nbbox_met <- st_bbox(met_sf)\n\n# Accident Map TC metro\nggplot() +\n  geom_sf(data = streets) + \n  geom_sf(data = counties, color = \"green4\", fill = NA, size = 5) +\n  geom_sf(data = met_sf, aes(color = factor(post_covid)), size = .8, alpha = .5) +\n  scale_color_manual(\n    values = c(\"0\" = \"#0066FF\", \"1\" = \"red\"),\n    name = \"COVID Period\",\n    labels = c(\"0\" = \"Pre-COVID\", \"1\" = \"Post-COVID\")\n  ) +\n  geom_sf_label(data = counties, aes(label = CTY_NAME), size = 4, color = \"green4\") +\n  coord_sf(xlim = bbox_met[c('xmin', 'xmax')], ylim = bbox_met[c('ymin', 'ymax')], expand = FALSE) +\n  ggtitle(\"Locations of Traffic Accidents in Twin Cities Metro (June 2016 through March 2023)\") +\n  theme(\n    plot.title = element_text(size = rel(1.5)),\n    axis.title.x = element_blank(), \n    axis.title.y = element_blank(),\n    legend.text = element_text(size = 12),\n    legend.title = element_text(size = 14)) +\n  guides(color = guide_legend(override.aes = list(size = 5)))\n```\n\n::: {.cell-output-display}\n![](ProjScript_files/figure-html/unnamed-chunk-4-1.png){width=1152}\n:::\n:::\n\n\nIt appears that after COVID, a greater proportion of accidents may be happening in the suburbs as fewer people commute to the core cities. Indeed, pre-COVID 72.2% of accidents in the metro occurred in just Hennepin and Ramsey counties, while post-COVID the proportion was down to 69.1%\n\n\n\n\n\n### Top 20 Streets by Number of Accidents, Before & After COVID\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Filter accidents pre-covid\naccidents_pre_covid <- met %>%\n  filter(post_covid == 0)\n\n# Filter accidents post-covid\naccidents_post_covid <- met %>%\n  filter(post_covid == 1)\n\n# Get the count of accidents for each street pre-covid\ntop20_pre_covid <- accidents_pre_covid %>%\n  count(Street) %>% \n  top_n(20, n)\n\n# Get the count of accidents for each street post-covid\ntop20_post_covid <- accidents_post_covid %>%\n  count(Street) %>% \n  top_n(20, n) \n\n# Find the maximum number of accidents to set the same x-axis limit for both plots\nmax_accidents <- max(c(top20_pre_covid$n, top20_post_covid$n))\n\n# Pre-COVID plot\nplot_pre_covid <- ggplot(top20_pre_covid, aes(x = reorder(Street, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"#0066FF\") +\n  geom_text(aes(label = Street), vjust = 0.4, hjust = -0.2, color = \"black\", size = 3.0) +\n  coord_flip() +\n  labs(title = \"Top 20 Streets by Number of Accidents (Pre-COVID)\", x = NULL, y = NULL) +\n  ylim(0, max_accidents) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 0),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank()\n  )\n\n\n# Post-COVID plot\nplot_post_covid <- ggplot(top20_post_covid, aes(x = reorder(Street, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"red\") +\n  geom_text(aes(label = Street), vjust = 0.4, hjust = -0.2, color = \"black\", size = 3.0) +\n  coord_flip() +\n  labs(title = \"Top 20 Streets by Number of Accidents (Post-COVID)\", x = NULL, y = \"Number of Accidents\") +\n  ylim(0, max_accidents) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 0),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank()\n  )\n\n# Align the plots vertically\ngrid.arrange(plot_pre_covid, plot_post_covid, ncol = 1)\n```\n\n::: {.cell-output-display}\n![](ProjScript_files/figure-html/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\nThese counts by street are for the entire periods of June 2016 through February 2020, and March 2020 through March 2023, pertaining to the pre and post COVID periods, respectively. This means there are 45 months worth of data for the pre-COVID period and 37 months of data for the post-COVID period.\n\n### Weekly Accidents Rate in the Twin Cities Metro\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get weekly incidents by season\nweekly_incidents_by_season <- met %>%\n  mutate(Week = as.Date(floor_date(Start_Time, \"week\"))) %>%\n  group_by(Week, season, post_covid) %>%\n  summarise(Incidents = n(), .groups = 'drop') %>%\n  ungroup() %>%\n  arrange(Week, season, post_covid)\n\n# Reorder the 'season' variable\nweekly_incidents_by_season$season <- factor(weekly_incidents_by_season$season, \n                                            levels = c(\"Spring\", \"Summer\", \"Autumn\", \"Winter\"))\n\n# Define date limits for the plot\nstart_date <- as.Date(\"2016-06-01\")\nend_date <- as.Date(\"2023-03-31\")  \n\n# Line plot\nggplot(weekly_incidents_by_season, aes(x = Week, y = Incidents)) +\n  geom_segment(aes(xend = lead(Week), yend = lead(Incidents), color = season), size = .5) +\n  scale_color_manual(name = \"Season\", \n                     values = c(\"Spring\" = \"green\", \"Summer\" = \"red\", \n                                \"Autumn\" = \"orange\", \"Winter\" = \"blue\")) +\n  ggtitle(\"Weekly Accident Rate in the Twin Cities Metro\") +\n  xlab(\"\") +\n  ylab(\"Number of Accidents\") +\n  scale_x_date(date_breaks = '1 month', date_labels = \"%b %Y\",\n               limits = c(start_date, end_date),\n               expand = c(0, 0)) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5, size = rel(0.8))) +\n  geom_smooth(data = subset(weekly_incidents_by_season, post_covid == 0),\n              aes(x = Week, y = Incidents, linetype = \"Pre-Covid\"), \n              method = \"lm\", se = FALSE, color = \"darkgray\") +\n   geom_smooth(data = subset(weekly_incidents_by_season, post_covid == 1),\n              aes(x = Week, y = Incidents, linetype = \"Post-Covid\"), \n              method = \"lm\", se = FALSE, color = \"black\") +\n   scale_linetype_manual(name = \"Trend Lines\",\n                        values = c(\"Pre-Covid\" = \"dashed\", \"Post-Covid\" = \"dashed\"),\n                        labels = c(\"Pre-Covid\", \"Post-Covid\"),\n                        guide = guide_legend(override.aes = list(color = c(\"Pre-Covid\" = \"darkgray\",   \"Post-Covid\" = \"black\"))))\n```\n\n::: {.cell-output-display}\n![](ProjScript_files/figure-html/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\nThe weekly accident rate was trending upwards pre-COVID, but there's a noticeable shift during the post-COVID period as the trend seems to flatten out while the seasonality becomes more pronounced.\n\n### Number of Accidents by Hour of Day\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create data frames with counts of incidents for each hour, pre and post Covid\nhourly_incidents_pre_covid <- met %>%\n  filter(post_covid == 0) %>%\n  group_by(start_hour) %>%\n  summarise(Count = n())\n\nhourly_incidents_post_covid <- met %>%\n  filter(post_covid == 1) %>%\n  group_by(start_hour) %>%\n  summarise(Count = n())\n\n# Combine the data\ncombined_hourly_incidents <- bind_rows(\n  mutate(hourly_incidents_pre_covid, Period = \"Pre-Covid\"),\n  mutate(hourly_incidents_post_covid, Period = \"Post-Covid\")\n)\n\n# Convert 'Period' to a factor with levels in the desired order\ncombined_hourly_incidents$Period <- factor(combined_hourly_incidents$Period, \n                                           levels = c(\"Pre-Covid\", \"Post-Covid\"))\n\n# Line plot of incidents by hour of the day\nggplot(combined_hourly_incidents, aes(x=start_hour, y=Count, color=Period)) +\n  geom_smooth(se=F, method = \"loess\", span = 0.2) + \n  scale_color_manual(values=c(\"#0066FF\", \"red\")) +  \n  ggtitle(\"Number of Accidents by Hour of the Day\") +\n  xlab(\"Hour of the Day\") +\n  ylab(\"Number of Accidents\") +\n  theme_minimal() +\n  theme(legend.position=\"top\")\n```\n\n::: {.cell-output-display}\n![](ProjScript_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nReminder: there are 45 months worth of data for the pre-COVID period and 37 months of data for the post-COVID period. I believe the length of these two periods are close enough for general comparison purposes, but it's important to note that all else being equal, there should be higher counts in the pre-COVID period. However, there are actually 65,459 accidents recorded post-COVID compared with only 56,054 pre-COVID.\n\n\n\n\n\n### Length of Section of Road Affected When Accidents Occur\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Boxplots for length of road affected\nggplot(met, aes(y = factor(post_covid, levels = c(1, 0)), x = `Distance(mi)`, fill = factor(post_covid, levels = c(1, 0)))) +\n  geom_boxplot(outlier.size = 1, orientation = \"y\") +\n  scale_fill_manual(values = c(\"red\", \"#0066FF\")) +\n  scale_y_discrete(labels = c(\"1\" = \"Post-Covid\", \"0\" = \"Pre-Covid\")) +\n  labs(title = \"Length of Section of Road Affected When Accidents Occur\",\n       y = \"\",\n       x = \"Distance (mi)\") +\n  theme_minimal() +\n  theme(\n    axis.title.y = element_blank(),\n    axis.text.y = element_text(angle = 45, hjust = 1),\n    legend.position = \"none\"\n  ) +\n  coord_cartesian(xlim = c(0, 2.5))\n```\n\n::: {.cell-output-display}\n![](ProjScript_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nFor each observed accident, the length in miles of the stretch of road that was affected was recorded. Before COVID, the median distance was 0 miles and the mean was 0.48 miles. After COVID, the median distance affected was 0.43 miles while the mean was 0.85 miles.\n\n\n\n\n\n# Preparing to Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Converting logical types to binary numbers\n# Identifying logical columns\nlogical_cols <- sapply(met, is.logical)\n\n# Converting logical columns to binary (0 and 1)\nmet[, logical_cols] <- lapply(met[, logical_cols], as.integer)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select features of interest\nmet_selected <- met %>%\n  select(Severity, rush_hr, is_weekday, season, post_covid,\n         County, `Temperature(F)`, `Humidity(%)`, `Visibility(mi)`, `Wind_Speed(mph)`,\n         `Precipitation(in)`, Amenity, Bump, Crossing, Give_Way, Junction, No_Exit,\n         Railway, Roundabout, Station, Stop, Traffic_Calming, Traffic_Signal,\n         Turning_Loop) %>%\n  mutate(\n    County = factor(County),\n    season = factor(season)) %>%\n  dummy_cols(select_columns = c(\"County\", \"season\"), remove_selected_columns = TRUE)\n\n# Transform target variable\nmet_selected <- met_selected %>%\n    mutate(severe = as.factor(ifelse(Severity %in% 1:2, 0, 1))) %>%\n    select(-Severity)\n\n# Drop Spring season and Ramsey County\nmet_selected$season_Spring <- NULL\nmet_selected$County_Ramsey <- NULL\n```\n:::\n\n\n\n\n## Features of interest\n\nFor my models I'm selecting the following variables to start:\n\n`severe`: The original dataset measured the severity of accidents on a scale of 1 - 4, with 4 being the most severe. The source documentation indicates severity is based on the length of time traffic was impacted. I turned this into a binary variable with accidents measuring at a 3 or 4 on the original scale as 'severe'.\n\n`rush_hr`: Computed binary variable for accidents which occured between 6 and 9 am or 3 and 6 pm.\n\n`is_weekday`: Binary variable indicating if addicent occured on weekday or not.\n\n`season`: Spring, summer, fall, winter\n\n`post_covid`: binary variable, with post_covid being define as during or after March 1st, 2020.\n\n`County`: Shows the county in the address field.\n\n`Temperature(F)`: Shows the temperature (in Fahrenheit).\n\n`Humidity(%)`: Shows the humidity (in percentage).\n\n`Visibility(mi)`: Shows visibility (in miles).\n\n`Wind_Speed(mph)`: Shows wind speed (in miles per hour).\n\n`Precipitation(in)`: Shows precipitation amount in inches, if there is any.\n\n`Amenity`: A POI (Point of Interest) annotation which indicates the presence of an amenity in a nearby location.\n\n`Bump`: A POI annotation which indicates the presence of a speed bump or hump in a nearby location.\n\n`Crossing`: A POI annotation which indicates the presence of a crossing in a nearby location.\n\n`Give_Way`: A POI annotation which indicates the presence of a give_way sign in a nearby location.\n\n`Junction`: A POI annotation which indicates the presence of a junction in a nearby location.\n\n`No_Exit`: A POI annotation which indicates the presence of a no_exit sign in a nearby location.\n\n`Railway`: A POI annotation which indicates the presence of a railway in a nearby location.\n\n`Roundabout`: A POI annotation which indicates the presence of a roundabout in a nearby location.\n\n`Station`: A POI annotation which indicates the presence of a station (such as a bus or train station) in a nearby location.\n\n`Stop`: A POI annotation which indicates the presence of a stop sign in a nearby location.\n\n`Traffic_Calming`: A POI annotation which indicates the presence of traffic calming measures (such as speed humps) in a nearby location.\n\n`Traffic_Signal`: A POI annotation which indicates the presence of a traffic signal in a nearby location.\n\n`Turning_Loop`: A POI annotation which indicates the presence of a turning loop in a nearby location.\n\n## Data cleaning\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute the number and percentage of NAs for each column\nna_summary <- met_selected %>%\n  summarise_all(~sum(is.na(.))) %>%\n  gather(column, na_count) %>%\n  mutate(na_percentage = (na_count / nrow(met)) * 100) %>%\n  filter(na_count > 1)\nprint(na_summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 3\n  column            na_count na_percentage\n  <chr>                <int>         <dbl>\n1 Temperature(F)         632         0.520\n2 Humidity(%)            722         0.594\n3 Visibility(mi)         617         0.508\n4 Wind_Speed(mph)       3590         2.95 \n5 Precipitation(in)    29028        23.9  \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remove NAs\nmet_clean <- na.omit(met_selected)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Scaling numerical variables, except for Start_Hour\nnumerical_vars <- c(\"Temperature(F)\", \"Humidity(%)\",\n         \"Visibility(mi)\", \"Wind_Speed(mph)\", \"Precipitation(in)\")\n\nmet_clean[numerical_vars] <- scale(met_clean[numerical_vars])\n```\n:::\n\n\nNumerical variables except for `Start_Hour` were scaled. NAs were removed because imputation seemed inappropriate given the large amount of missing data for `Precipitation(in)`. The number of NAs for other features was very small.\n\n## Partitioning the data for training and testing\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Partition the data.\nset.seed(720)\nsamp = createDataPartition(met_clean$severe,\n                           p = 0.7,\n                           list = FALSE)\ntrain = met_clean[samp, ]\ntest = met_clean[-samp, ]\nrm(samp)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check for class imbalance problems. \ntrain %>%\n  select(severe) %>%\n  table() %>%\n  as.data.frame() %>%\n  mutate(Proportion = Freq / sum(Freq)) %>%\n  rename(Counts = Freq) %>%\n  t()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]        [,2]       \nsevere     \"0\"         \"1\"        \nCounts     \"56029\"     \" 8078\"    \nProportion \"0.8739919\" \"0.1260081\"\n```\n:::\n:::\n\n\n## SMOTE\n\nThere is a class imbalance to address: only about 12% of observations were in the severe category.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Apply SMOTE to the training data to only oversample the minority class\nsmoted <- performanceEstimation::smote(severe ~ ., data = train, \n                                            perc.over = .7, k = 5, perc.under = 7)\n# Over/under values:\n# .3 and 21 gave 50883 and 10501\n\n# .5 and 11 gave 44429 and 12117\n  # Sensitivity: .439\n\n# .7 and 7 gave 39578 and 13732\n  # Sensitivity: .539\n\n# Checking the class distribution in the final dataset\nsmoted %>%\n  select(severe) %>%\n  table() %>%\n  as.data.frame() %>%\n  mutate(Proportion = Freq / sum(Freq)) %>%\n  rename(Counts = Freq) %>%\n  t()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]        [,2]       \nsevere     \"0\"         \"1\"        \nCounts     \"39578\"     \"13732\"    \nProportion \"0.7424123\" \"0.2575877\"\n```\n:::\n:::\n\n\nI used smote to generate additional records for the severe accidents, while downsampling the less severe accidents.\n\n# LASSO\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# LASSO\n# Separate predictors and response\ny <- as.vector(smoted$severe)\nX <- as.matrix(smoted %>% dplyr::select(-severe))\n\n# Use cross-validation to find the best lambda\ncv.lasso <- cv.glmnet(X, y, family=\"binomial\", alpha=1, thresh=1e-7)\n\n# Extract best lambda\nbest_lambda <- cv.lasso$lambda.1se\n\n# Fit the model using the best lambda\nLASSO_model <- glmnet(X, y, family=\"binomial\", alpha=1, lambda=best_lambda,\n                      maxit = 1e6)\n\n# View the coefficients\ncoef(LASSO_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n31 x 1 sparse Matrix of class \"dgCMatrix\"\n                           s0\n(Intercept)        0.96778457\nrush_hr           -0.29167085\nis_weekday         0.29222535\npost_covid        -2.31490644\nTemperature(F)     0.08330439\nHumidity(%)       -0.16332337\nVisibility(mi)    -0.19788784\nWind_Speed(mph)    0.18725222\nPrecipitation(in)  0.00925945\nAmenity           -0.32559935\nBump               .         \nCrossing          -0.49021614\nGive_Way          -0.12711155\nJunction          -0.30332686\nNo_Exit            .         \nRailway            .         \nRoundabout         .         \nStation           -0.45659369\nStop              -0.78992782\nTraffic_Calming    .         \nTraffic_Signal     0.30013649\nTurning_Loop       .         \nCounty_Anoka      -0.44565011\nCounty_Carver     -1.49365059\nCounty_Dakota     -0.46861598\nCounty_Hennepin   -0.15591875\nCounty_Scott      -1.66569542\nCounty_Washington -0.50940604\nseason_Autumn     -0.67726857\nseason_Summer      .         \nseason_Winter     -1.48866098\n```\n:::\n:::\n\n\nLASSO indicated variables `Bump`, `No_Exit`, `Railway`, `Roundabout`, `Traffic_Calming`, `Turning_Loop` and `season_Summer` aren't significant predictors of accident severity.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# LASSO output\ntest_predictors = as.matrix(test %>% dplyr::select(-severe))\nLASSO_test_class = predict(LASSO_model, newx = test_predictors, s = best_lambda, type=\"class\")\nLASSO_test_prob = predict(LASSO_model, newx = test_predictors, s = best_lambda, type=\"response\")[,1]\n\nLASSO_test_class_factor = factor(LASSO_test_class, levels = c(\"0\", \"1\"))\nresponse_vector_factor = factor(test$severe, levels = c(\"0\", \"1\"))\n\n# Create and print the confusion matrix\nLASSO_cm = confusionMatrix(LASSO_test_class_factor, response_vector_factor, positive = \"1\")\n\n# Print the CM for LASSO\nprint(LASSO_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 21923  1977\n         1  2089  1485\n                                          \n               Accuracy : 0.852           \n                 95% CI : (0.8478, 0.8562)\n    No Information Rate : 0.874           \n    P-Value [Acc > NIR] : 1.00000         \n                                          \n                  Kappa : 0.3373          \n                                          \n Mcnemar's Test P-Value : 0.08173         \n                                          \n            Sensitivity : 0.42894         \n            Specificity : 0.91300         \n         Pos Pred Value : 0.41550         \n         Neg Pred Value : 0.91728         \n             Prevalence : 0.12601         \n         Detection Rate : 0.05405         \n   Detection Prevalence : 0.13009         \n      Balanced Accuracy : 0.67097         \n                                          \n       'Positive' Class : 1               \n                                          \n```\n:::\n:::\n\n\n# Logistic Regression\n\nI'm doing a logistic regression using only the variables which were recommended from LASSO.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create subset with LASSO suggested variables\nlasso_rec <- smoted %>%\n  select(-\"Bump\", -\"No_Exit\", -\"Railway\", -\"Roundabout\", -\"Traffic_Calming\",\n         -\"Turning_Loop\", -\"season_Summer\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the logistic regression model\nlogistic_trained <- glm(severe ~ ., data = lasso_rec, family = \"binomial\")\n\n# View the summary of the model\nsummary(logistic_trained)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = severe ~ ., family = \"binomial\", data = lasso_rec)\n\nCoefficients:\n                    Estimate Std. Error z value             Pr(>|z|)    \n(Intercept)          1.17608    0.04538  25.916 < 0.0000000000000002 ***\nrush_hr             -0.37734    0.02687 -14.045 < 0.0000000000000002 ***\nis_weekday           0.40639    0.03543  11.471 < 0.0000000000000002 ***\npost_covid          -2.42756    0.02699 -89.947 < 0.0000000000000002 ***\n`Temperature(F)`     0.07891    0.02014   3.918        0.00008926941 ***\n`Humidity(%)`       -0.20542    0.01447 -14.193 < 0.0000000000000002 ***\n`Visibility(mi)`    -0.24300    0.01471 -16.521 < 0.0000000000000002 ***\n`Wind_Speed(mph)`    0.19603    0.01256  15.605 < 0.0000000000000002 ***\n`Precipitation(in)`  0.02209    0.01069   2.066               0.0389 *  \nAmenity             -1.33048    0.52246  -2.547               0.0109 *  \nCrossing            -0.69479    0.06428 -10.809 < 0.0000000000000002 ***\nGive_Way            -0.80961    0.37149  -2.179               0.0293 *  \nJunction            -0.38697    0.03913  -9.888 < 0.0000000000000002 ***\nStation             -0.67456    0.10405  -6.483        0.00000000009 ***\nStop                -1.32825    0.27650  -4.804        0.00000155654 ***\nTraffic_Signal       0.51928    0.05269   9.854 < 0.0000000000000002 ***\nCounty_Anoka        -0.69712    0.05840 -11.936 < 0.0000000000000002 ***\nCounty_Carver       -2.13187    0.16641 -12.811 < 0.0000000000000002 ***\nCounty_Dakota       -0.67564    0.04530 -14.916 < 0.0000000000000002 ***\nCounty_Hennepin     -0.32572    0.02913 -11.182 < 0.0000000000000002 ***\nCounty_Scott        -2.20874    0.12505 -17.663 < 0.0000000000000002 ***\nCounty_Washington   -0.75777    0.05729 -13.226 < 0.0000000000000002 ***\nseason_Autumn       -0.78572    0.03320 -23.668 < 0.0000000000000002 ***\nseason_Winter       -1.63841    0.04443 -36.879 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 60829  on 53309  degrees of freedom\nResidual deviance: 45115  on 53286  degrees of freedom\nAIC: 45163\n\nNumber of Fisher Scoring iterations: 5\n```\n:::\n\n```{.r .cell-code}\nexp(coef(logistic_trained))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        (Intercept)             rush_hr          is_weekday          post_covid \n         3.24165095          0.68568500          1.50138549          0.08825223 \n   `Temperature(F)`       `Humidity(%)`    `Visibility(mi)`   `Wind_Speed(mph)` \n         1.08211158          0.81430686          0.78427452          1.21656900 \n`Precipitation(in)`             Amenity            Crossing            Give_Way \n         1.02233218          0.26435035          0.49917963          0.44503313 \n           Junction             Station                Stop      Traffic_Signal \n         0.67911201          0.50938099          0.26494148          1.68081000 \n       County_Anoka       County_Carver       County_Dakota     County_Hennepin \n         0.49801609          0.11861585          0.50883069          0.72200819 \n       County_Scott   County_Washington       season_Autumn       season_Winter \n         0.10983862          0.46871162          0.45579195          0.19428808 \n```\n:::\n\n```{.r .cell-code}\npR2(logistic_trained)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfitting null model for pseudo-r2\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n           llh        llhNull             G2       McFadden           r2ML \n-22557.3221517 -30414.3448784  15714.0454535      0.2583328      0.2552951 \n          r2CU \n     0.3751513 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict on test set for regular LOGISTIC REGRESSION\npredicted_probs_logit <- predict(logistic_trained, newdata = test, type = \"response\")\nthreshold <- 0.5\npredicted_classes_logit <- ifelse(predicted_probs_logit > threshold, 1, 0)\n\n# Convert predicted_classes and actual classes to factors\npredicted_classes_factor_logit <- factor(predicted_classes_logit, levels = c(\"0\", \"1\"))\nactual_classes_factor_logit <- factor(test$severe, levels = c(\"0\", \"1\"))\n\n# Generate the confusion matrix\nconfusion_matrix_logit <- confusionMatrix(predicted_classes_factor_logit,\n                                          actual_classes_factor_logit, positive = \"1\")\n\n# Print the full CM for logistic regression\nprint(confusion_matrix_logit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 21925  1998\n         1  2087  1464\n                                          \n               Accuracy : 0.8513          \n                 95% CI : (0.8471, 0.8555)\n    No Information Rate : 0.874           \n    P-Value [Acc > NIR] : 1.0000          \n                                          \n                  Kappa : 0.3323          \n                                          \n Mcnemar's Test P-Value : 0.1686          \n                                          \n            Sensitivity : 0.42288         \n            Specificity : 0.91309         \n         Pos Pred Value : 0.41228         \n         Neg Pred Value : 0.91648         \n             Prevalence : 0.12601         \n         Detection Rate : 0.05329         \n   Detection Prevalence : 0.12925         \n      Balanced Accuracy : 0.66798         \n                                          \n       'Positive' Class : 1               \n                                          \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create Logit Explainer\nlogit_explain = DALEX::explain(model = logistic_trained,\n                             data = test,\n                             y = as.numeric(test$severe==\"1\"),\n                             type = \"classification\",\n                             label = \"Logit\")\n```\n:::\n\n\n# GBM Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train a gradient boost model\ngbm_smoted <- smoted\nsmoted$severe <- factor(gbm_smoted$severe, levels = c(\"0\", \"1\"), labels = c(\"Level_0\", \"Level_1\"))\n\n#set.seed(469)\n#gbm_model = train(\n#  y = gbm_smoted$severe,\n#  x = select(gbm_smoted, -severe),\n#  method = \"gbm\",\n#  verbose = FALSE,\n#  trControl = trainControl(method = \"repeatedcv\", number = 5, repeats = 5, classProbs = TRUE),\n#  tuneLength = 10\n#)\n#saveRDS(gbm_model, \"met_gbm.5.5.10.rds\")\ngbm_model = readRDS(\"met_gbm.5.5.10.rds\")\nplot(gbm_model)\n```\n\n::: {.cell-output-display}\n![](ProjScript_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create GBM Explainer\ngbm_explain = DALEX::explain(model = gbm_model,\n                             data = test,\n                             y = as.numeric(test$severe==\"1\"),\n                             type = \"classification\",\n                             label = \"GBM\")\n```\n:::\n\n\n# Random Forest\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train a random forest model\n#set.seed(776)\n#rf_model = train(\n#   y = smoted$severe,\n#   x = select(smoted, -severe),\n#   method = \"rf\",\n#   trControl = trainControl(method = \"repeatedcv\", number = 5, repeats = 5),\n#   tuneLength = 10\n# )\n\n#saveRDS(rf_model, \"met_rf.5.5.10.rds\")\nrf_model = readRDS(\"met_rf.5.5.10.rds\")\nplot(rf_model)\n```\n\n::: {.cell-output-display}\n![](ProjScript_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create rf explainer\nrf_explain = DALEX::explain(model = rf_model,\n                            data = test,\n                            y = as.numeric(test$severe==\"1\"),\n                            type = \"classification\",\n                            label = \"Random Forest\")\n```\n:::\n\n\n# Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Performance of GBM and logistic regression\nlogit_perf = DALEX::model_performance(logit_explain, cutoff = 0.5)\ngbm_perf = DALEX::model_performance(gbm_explain, cutoff = 0.5)\nrf_perf = DALEX::model_performance(rf_explain, cutoff = 0.5)\n\n\nprint(\"GBM Performance\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"GBM Performance\"\n```\n:::\n\n```{.r .cell-code}\ngbm_perf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMeasures for:  classification\nrecall     : 0.39197 \nprecision  : 0.586684 \nf1         : 0.4699567 \naccuracy   : 0.8885856 \nauc        : 0.8433761\n\nResiduals:\n         0%         10%         20%         30%         40%         50% \n-0.99268132 -0.28070930 -0.17592324 -0.12791891 -0.08659678 -0.05395055 \n        60%         70%         80%         90%        100% \n-0.03217305 -0.02341745 -0.01708539  0.31384591  0.99363416 \n```\n:::\n\n```{.r .cell-code}\nprint(\"Logistic Regression Performance\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Logistic Regression Performance\"\n```\n:::\n\n```{.r .cell-code}\nlogit_perf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMeasures for:  classification\nrecall     : 0.4228769 \nprecision  : 0.4122782 \nf1         : 0.4175103 \naccuracy   : 0.851314 \nauc        : 0.8031435\n\nResiduals:\n         0%         10%         20%         30%         40%         50% \n-0.92855204 -0.42585652 -0.27172086 -0.19792200 -0.13620647 -0.08774507 \n        60%         70%         80%         90%        100% \n-0.05659205 -0.03819438 -0.02362172  0.30245231  0.99453333 \n```\n:::\n\n```{.r .cell-code}\nprint(\"Random Forest Performance\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Random Forest Performance\"\n```\n:::\n\n```{.r .cell-code}\nrf_perf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMeasures for:  classification\nrecall     : 0.4829578 \nprecision  : 0.5100671 \nf1         : 0.4961424 \naccuracy   : 0.8763922 \nauc        : 0.8354438\n\nResiduals:\n    0%    10%    20%    30%    40%    50%    60%    70%    80%    90%   100% \n-1.000 -0.364 -0.202 -0.122 -0.068 -0.034 -0.016 -0.004  0.000  0.220  1.000 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nroc = plot(logit_perf, gbm_perf, rf_perf, geom = \"roc\")\nprc = plot(logit_perf, gbm_perf, rf_perf, geom = \"prc\")\n\nroc + prc\n```\n\n::: {.cell-output-display}\n![](ProjScript_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\nThe Receiver Operator Characteristic chart shows the tradeoff my models made between a high true positive rate and a low false positive rate. Having a curve closer to the upper left corner of the plot is ideal, while a curve closer to the diagonal line in the middle of the chart indicates a model which performs about as well as randomly guessing. The Precision Recall Curve shows the tradeoff between precision, which is the proportion of my positive predictions that were correct, and recall, which is another word for true positive rate. Recall can be increased by lowering precision and vice versa. In this plot a line close to the upper right corner is ideal. Overall, the Gradient Boosted Machine model appears to perform the best. The Random Forest model has very similar performance, but it was more computationally expensive to create.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngbm_mp = DALEX::model_parts(gbm_explain,\n                            B = 50)\nplot(gbm_mp)\n```\n\n::: {.cell-output-display}\n![](ProjScript_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n# Conclusion\n\nIn the Feature Importance plot for the GBM model we see that `post_covid` was the most important feature in our GBM model for predicting the severity of accidents. This model had an accuracy rate of 88%, recall rate of 39% and a precision rate of 58%.\n\nEarlier, my exploratory data analysis indicated some key differences in traffic accidents between the pre and post COVID periods. These findings must be taken with a grain of salt as in this dataset there is no way to know how the practice of recording traffic accidents may have changed over the years.\n\nTo summarize:\n\n* A greater proportion of accidents occurred in the suburbs post-COVID.\n\n* I-94E went from having the 7th highest number of accidents pre-COVID to having the greatest number of accidents for any road post-COVID.\n\n* The weekly accident rate was trending upwards pre-COVID, but there's a noticeable shift during the post-COVID period as the trend seems to flatten out while the seasonality becomes more pronounced.\n\n* The number of accidents during morning rush hour decreased post-COVID, but increased during evening rush hour (which shifted to starting slightly earlier in the afternoon. Pre and post rush hour accidents also increased after COVID.\n\n* Post-COVID, when accidents do occur they seem to affect a significantly longer portion of road than they did pre-COVID.\n\n* Although the rate of accidents increased post-COVID, only 5% were severe. Pre-COVID 42% were in the severe category.\n\nTaken together, these findings indicate that assumptions and processes related to traffic management which were implemented before COVID may no longer be optimal.\n\n",
    "supporting": [
      "ProjScript_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}