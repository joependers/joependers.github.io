[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joe Penders",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nThe page your viewing is the “index” page, or the landing page of your site. The site is just a quarto document. And you can put anything on it that could be in a quarto document.\nThere are also settings to get a list of site items you may want people to have easy access to. For example, a list of your blog posts, projects, etc.\nThere are different styles of landing pages. In this particular style, the index.qmd page is used as an about page. Instead of immediately showcasing your work, it showcases you! You can introduce yourself and include your contact information right by your image and description.\nYou can then also decide to have some content show up below the about portion as well."
  },
  {
    "objectID": "posts/Demo Post 1/index.html",
    "href": "posts/Demo Post 1/index.html",
    "title": "Demo Post 1",
    "section": "",
    "text": "This is a demo post in which we begin the blog. The idea here is that you create one post with this quarto document. The quarto document for a post will be named “index.qmd” insides of folder with the name of the post. For example, if I wanted my post to be titled “Demo Post 1” then I would do the following.\nAfter doing that, you can then edit the index.qmd document for that new post to your heart’s content. Lets do a little of that now so you can see how this might work."
  },
  {
    "objectID": "posts/Demo Post 1/index.html#including-resources",
    "href": "posts/Demo Post 1/index.html#including-resources",
    "title": "Demo Post 1",
    "section": "Including Resources",
    "text": "Including Resources\nSuppose you wanted to discuss something, like the CRISP-DM process for analytics projects. You might wish to refer to an image of the process and you could include the image in the “Demo Post 1” folder and reference it here in the document.\n\n\n\n\n\nYou can easily insert the image through the visual editor in Posit / RStudio."
  },
  {
    "objectID": "posts/Demo Post 1/index.html#data-and-output",
    "href": "posts/Demo Post 1/index.html#data-and-output",
    "title": "Demo Post 1",
    "section": "Data and Output",
    "text": "Data and Output\nLets look at some data.\n\nlibrary(tidyverse)\nlibrary(ggthemes)\ndata(\"USArrests\")\n\nUSArrests %&gt;%\n  ggplot(aes(x = Assault, y = Murder)) +\n  geom_point(pch = 21, color = \"coral3\", bg = \"coral\", size=3) +\n  labs(title = \"Arrests for Murder vs. Assault in US States\",\n       x = \"Arrests for assault per 100,000\",\n       y = \"Arrests for murder per 100,000\") +\n  theme_clean()\n\n\n\n\nThis would show us a relationship that we could then spend some paragraphs analyzing and interpreting."
  },
  {
    "objectID": "posts/Demo Post 2/index.html",
    "href": "posts/Demo Post 2/index.html",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %&gt;%\n  mutate(predicted_murder = predict(dt, USArrests)) %&gt;%\n  select(Murder, predicted_murder) %&gt;%\n  cor() -&gt; corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "posts/Demo Post 2/index.html#understanding-the-data",
    "href": "posts/Demo Post 2/index.html#understanding-the-data",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %&gt;%\n  mutate(predicted_murder = predict(dt, USArrests)) %&gt;%\n  select(Murder, predicted_murder) %&gt;%\n  cor() -&gt; corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects are different than posts. Projects should be more expansive, impressive and generally more professional in nature compared to posts. Posts can be works in progress. Small ideas or things you did that you thought were interesting. Projects should really showcase your professional abilities. You don’t need to have too many, just make them good. And try to always have one “in the works” so that employers and collaborators can see that you’re driven.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Problem Set 6\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nProblem Set 4\n\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2023\n\n\nJoe Penders\n\n\n\n\n\n\n  \n\n\n\n\nDemo Post 2\n\n\n\n\n\n\n\ndecision trees\n\n\nmachine learning\n\n\narrests\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nJane Doe\n\n\n\n\n\n\n  \n\n\n\n\nDemo Post 1\n\n\n\n\n\n\n\nquarto\n\n\ncrisp-dm\n\n\nscatterplot\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nJane Doe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Problem Set 3/index.html",
    "href": "posts/Problem Set 3/index.html",
    "title": "Problem Set 3",
    "section": "",
    "text": "1. Create a new Rmarkdown document that will show case your data exploration, modeling, and error testing.\n\nrm(list = ls())\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8\n\nlibrary(lubridate)\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(dummy)\n\ndummy 0.1.3\ndummyNews()\n\nlibrary(lattice)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(iml)\nlibrary(patchwork)\ntc = read_csv(\"ToyotaCorolla.csv\")\n\nRows: 1436 Columns: 39\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Model, Fuel_Type, Color\ndbl (36): Id, Price, Age_08_04, Mfg_Month, Mfg_Year, KM, HP, Met_Color, Auto...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n2. Explore the data and determine the number of variables and the quantity of any missing values.\nIf values are missing, prescribe a plan to deal with the problem.\n\nstr(tc)\n\nspc_tbl_ [1,436 × 39] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Id               : num [1:1436] 1 2 3 4 5 6 7 8 9 10 ...\n $ Model            : chr [1:1436] \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" ...\n $ Price            : num [1:1436] 13500 13750 13950 14950 13750 ...\n $ Age_08_04        : num [1:1436] 23 23 24 26 30 32 27 30 27 23 ...\n $ Mfg_Month        : num [1:1436] 10 10 9 7 3 1 6 3 6 10 ...\n $ Mfg_Year         : num [1:1436] 2002 2002 2002 2002 2002 ...\n $ KM               : num [1:1436] 46986 72937 41711 48000 38500 ...\n $ Fuel_Type        : chr [1:1436] \"Diesel\" \"Diesel\" \"Diesel\" \"Diesel\" ...\n $ HP               : num [1:1436] 90 90 90 90 90 90 90 90 192 69 ...\n $ Met_Color        : num [1:1436] 1 1 1 0 0 0 1 1 0 0 ...\n $ Color            : chr [1:1436] \"Blue\" \"Silver\" \"Blue\" \"Black\" ...\n $ Automatic        : num [1:1436] 0 0 0 0 0 0 0 0 0 0 ...\n $ CC               : num [1:1436] 2000 2000 2000 2000 2000 2000 2000 2000 1800 1900 ...\n $ Doors            : num [1:1436] 3 3 3 3 3 3 3 3 3 3 ...\n $ Cylinders        : num [1:1436] 4 4 4 4 4 4 4 4 4 4 ...\n $ Gears            : num [1:1436] 5 5 5 5 5 5 5 5 5 5 ...\n $ Quarterly_Tax    : num [1:1436] 210 210 210 210 210 210 210 210 100 185 ...\n $ Weight           : num [1:1436] 1165 1165 1165 1165 1170 ...\n $ Mfr_Guarantee    : num [1:1436] 0 0 1 1 1 0 0 1 0 0 ...\n $ BOVAG_Guarantee  : num [1:1436] 1 1 1 1 1 1 1 1 1 1 ...\n $ Guarantee_Period : num [1:1436] 3 3 3 3 3 3 3 3 3 3 ...\n $ ABS              : num [1:1436] 1 1 1 1 1 1 1 1 1 1 ...\n $ Airbag_1         : num [1:1436] 1 1 1 1 1 1 1 1 1 1 ...\n $ Airbag_2         : num [1:1436] 1 1 1 1 1 1 1 1 0 1 ...\n $ Airco            : num [1:1436] 0 1 0 0 1 1 1 1 1 1 ...\n $ Automatic_airco  : num [1:1436] 0 0 0 0 0 0 0 0 0 0 ...\n $ Boardcomputer    : num [1:1436] 1 1 1 1 1 1 1 1 0 1 ...\n $ CD_Player        : num [1:1436] 0 1 0 0 0 0 0 1 0 0 ...\n $ Central_Lock     : num [1:1436] 1 1 0 0 1 1 1 1 1 0 ...\n $ Powered_Windows  : num [1:1436] 1 0 0 0 1 1 1 1 1 0 ...\n $ Power_Steering   : num [1:1436] 1 1 1 1 1 1 1 1 1 1 ...\n $ Radio            : num [1:1436] 0 0 0 0 0 0 0 0 1 0 ...\n $ Mistlamps        : num [1:1436] 0 0 0 0 1 1 0 0 0 0 ...\n $ Sport_Model      : num [1:1436] 0 0 0 0 0 0 1 0 0 0 ...\n $ Backseat_Divider : num [1:1436] 1 1 1 1 1 1 1 1 0 1 ...\n $ Metallic_Rim     : num [1:1436] 0 0 0 0 0 0 0 0 1 0 ...\n $ Radio_cassette   : num [1:1436] 0 0 0 0 0 0 0 0 1 0 ...\n $ Parking_Assistant: num [1:1436] 0 0 0 0 0 0 0 0 0 0 ...\n $ Tow_Bar          : num [1:1436] 0 0 0 0 0 0 0 0 0 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Id = col_double(),\n  ..   Model = col_character(),\n  ..   Price = col_double(),\n  ..   Age_08_04 = col_double(),\n  ..   Mfg_Month = col_double(),\n  ..   Mfg_Year = col_double(),\n  ..   KM = col_double(),\n  ..   Fuel_Type = col_character(),\n  ..   HP = col_double(),\n  ..   Met_Color = col_double(),\n  ..   Color = col_character(),\n  ..   Automatic = col_double(),\n  ..   CC = col_double(),\n  ..   Doors = col_double(),\n  ..   Cylinders = col_double(),\n  ..   Gears = col_double(),\n  ..   Quarterly_Tax = col_double(),\n  ..   Weight = col_double(),\n  ..   Mfr_Guarantee = col_double(),\n  ..   BOVAG_Guarantee = col_double(),\n  ..   Guarantee_Period = col_double(),\n  ..   ABS = col_double(),\n  ..   Airbag_1 = col_double(),\n  ..   Airbag_2 = col_double(),\n  ..   Airco = col_double(),\n  ..   Automatic_airco = col_double(),\n  ..   Boardcomputer = col_double(),\n  ..   CD_Player = col_double(),\n  ..   Central_Lock = col_double(),\n  ..   Powered_Windows = col_double(),\n  ..   Power_Steering = col_double(),\n  ..   Radio = col_double(),\n  ..   Mistlamps = col_double(),\n  ..   Sport_Model = col_double(),\n  ..   Backseat_Divider = col_double(),\n  ..   Metallic_Rim = col_double(),\n  ..   Radio_cassette = col_double(),\n  ..   Parking_Assistant = col_double(),\n  ..   Tow_Bar = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\ncolSums(is.na(tc))\n\n               Id             Model             Price         Age_08_04 \n                0                 0                 0                 0 \n        Mfg_Month          Mfg_Year                KM         Fuel_Type \n                0                 0                 0                 0 \n               HP         Met_Color             Color         Automatic \n                0                 0                 0                 0 \n               CC             Doors         Cylinders             Gears \n                0                 0                 0                 0 \n    Quarterly_Tax            Weight     Mfr_Guarantee   BOVAG_Guarantee \n                0                 0                 0                 0 \n Guarantee_Period               ABS          Airbag_1          Airbag_2 \n                0                 0                 0                 0 \n            Airco   Automatic_airco     Boardcomputer         CD_Player \n                0                 0                 0                 0 \n     Central_Lock   Powered_Windows    Power_Steering             Radio \n                0                 0                 0                 0 \n        Mistlamps       Sport_Model  Backseat_Divider      Metallic_Rim \n                0                 0                 0                 0 \n   Radio_cassette Parking_Assistant           Tow_Bar \n                0                 0                 0 \n\n\nWe have 1436 observations across 39 variables. There are no missing values to deal with.\n\n\n3. Analyze whether the Price variable is appropriate for a linear regression model and discuss its distribution.\nAre there any transformations that we might apply to the price variable?\n\n# Price Distribution\n\nhist(tc$Price)\n\n\n\n# Rearrange the columns to place Log_Price immediately after Price\n\ntc &lt;- tc %&gt;%\n  mutate(Log_Price = log(Price)) %&gt;%\n  select(1:3, Log_Price, 4:ncol(tc))\n  \nhist(tc$Log_Price)\n\n\n\n\nPrice is definitely skewed to the right, therefore I used a log transformation. After the transformation, the distribution of price is much closer to being symmetric.\n\n\n4. Is there a relationship between any of the features in the data and the Price feature?\nPerform some exploratory analysis to determine some features that are related using a feature plot.\n\n# Factor categorical variables, get rid of some unnecessary ones.\n  \ntc &lt;- tc %&gt;% \n  select(-Id, -Model, -Mfg_Month, -Cylinders, -Quarterly_Tax) %&gt;%\n  rename(Age = Age_08_04) %&gt;%\n  mutate(CC = if_else(CC == 16000, 2000, CC)) %&gt;%\n  mutate_at(vars(-one_of(\n    c('Price',\n      'Log_Price',\n      'Age',\n      'KM',\n      'HP',\n      'CC',\n      'Weight')\n  )), .funs = factor)\n\nOne Corolla has a 16000 CC engine, and that does not make sense. At the very least, it’s a massive outlier. I replaced this outlier with the value of 2000, which is the maximum otherwise.\n\n# Separate predictors and response\n\ntc_numeric &lt;- tc[, sapply(tc, is.numeric)]\nx &lt;- tc_numeric[, setdiff(names(tc_numeric), \"Log_Price\")]\ny &lt;- tc_numeric$Log_Price\n\n# Use featurePlot\n\ncaret::featurePlot(x, y, plot = \"scatter\")\n\n\n\n\nIt appears there is a negative relationship between Age, KM andLog_Price. There may be a positive relationship between Weight and Log_Price. I speculate that Weight may be a confounding variable with other variables that increase both weight and the price of the car, such as additional safety features.\n\ntc_temp &lt;- tc\n\n# Add a new column to the data that categorizes cars as \"30 years or newer\" or \"older than 30 years\"\ntc_temp$AgeGroup &lt;- ifelse(tc_temp$Age &lt;= 30, \"30 years or newer\", \"older than 30 years\")\n\n# Plot the Weight vs Age\nggplot(tc_temp, aes(x=Age, y=Weight, color=AgeGroup)) + \n  geom_point() +\n  geom_smooth(data=subset(tc, Age &lt;= 30), aes(group=1), method=\"lm\", formula=y~1, se=FALSE, linetype=\"solid\", color=\"blue\") +\n  geom_smooth(data=subset(tc, Age &gt; 30), aes(group=2), method=\"lm\", formula=y~1, se=FALSE, linetype=\"solid\", color=\"orange\") +\n  labs(title=\"Weight vs. Age of Cars\", x=\"Age\", y=\"Weight\") +\n  scale_color_manual(values=c(\"30 years or newer\"=\"blue\", \"older than 30 years\"=\"red\"), name=\"Age Group\") +\n  theme_minimal()\n\n\n\nrm(tc_temp)\n\nIt is very clear based on the Weight vs. Age of Cars graph that newer cars are significantly heavier. Therefore, I am removing the weight variable and leaving age.\n\ntc &lt;- select(tc, -'Weight')\n\n\n# Check categorical variables\n\ncategorical_list &lt;- list()\n\nfactor_cols &lt;- names(tc)[sapply(tc, is.factor)]\n\nfor(i in factor_cols) {\n  categorical_list[[i]] &lt;- table(tc[[i]])\n}\n\n# Convert all factors to dummy vars.\ntc_dum = dummy(tc, int = TRUE)\ntc_num = tc %&gt;%\n  keep(is.numeric)\ntc = bind_cols(tc_num, tc_dum)\nrm(tc_dum, tc_num)\n\n# remove one dummy from each categorical var\ntc &lt;- tc %&gt;%\n  select(-Mfg_Year_1998,\n-Mfg_Year_1999,\n-Mfg_Year_2000,\n-Mfg_Year_2001,\n-Mfg_Year_2002,\n-Mfg_Year_2003,\n-Mfg_Year_2004,\n-Fuel_Type_CNG,\n-Met_Color_0,\n-Color_Beige,\n-Color_Yellow,\n-Color_Violet,\n-Automatic_0,\n-Doors_2,\n-Gears_3,\n-Mfr_Guarantee_0,\n-BOVAG_Guarantee_0,\n-Guarantee_Period_13,\n-Guarantee_Period_18,\n-Guarantee_Period_20,\n-Guarantee_Period_24,\n-Guarantee_Period_28,\n-Guarantee_Period_36,\n-ABS_0,\n-Airbag_1_0,\n-Airbag_2_0,\n-Airco_0,\n-Automatic_airco_0,\n-Boardcomputer_0,\n-CD_Player_0,\n-Central_Lock_0,\n-Powered_Windows_0,\n-Power_Steering_0,\n-Radio_0,\n-Mistlamps_0,\n-Sport_Model_0,\n-Backseat_Divider_0,\n-Metallic_Rim_0,\n-Radio_cassette_0,\n-Parking_Assistant_0,\n-Tow_Bar_0)\n\n\n\n5. Are there any predictor variables in the data that are potentially too strongly related to each other?\nMake sure to use reference any visualizations, tables, or numbers to show this.\n\n# Compute the correlation matrix\ntc_minus_price &lt;- select(tc, -\"Price\", -\"Log_Price\")\ncor_matrix &lt;- cor(tc_minus_price, use = \"pairwise.complete.obs\")\n\n# Compute the correlation matrix\ncor_matrix &lt;- cor(tc_minus_price, use=\"complete.obs\", method=\"pearson\")\n\n# Find pairs with correlation coefficient greater than 0.7 (in absolute value)\nhigh_cor &lt;- which(abs(cor_matrix) &gt; 0.8 & cor_matrix != 1, arr.ind = TRUE)\n\n# Extract pairs and their corresponding correlation values\nresult &lt;- data.frame(\n  Variable1 = rownames(cor_matrix)[high_cor[, 1]],\n  Variable2 = rownames(cor_matrix)[high_cor[, 2]],\n  Correlation = cor_matrix[high_cor]\n)\n\n# remove duplicates\nresult &lt;- result[!duplicated(result$Correlation), ]\nprint(result)\n\n          Variable1        Variable2 Correlation\n1  Fuel_Type_Petrol Fuel_Type_Diesel  -0.9429759\n3           Doors_5          Doors_3  -0.8221205\n5           Gears_6          Gears_5  -0.9657999\n7 Powered_Windows_1   Central_Lock_1   0.8755525\n9  Radio_cassette_1          Radio_1   0.9916210\n\nrm(tc_minus_price)\n\nRadio_cassette_1 and Radio_1 are extremely correlated, having an r value of 0.99, therefore I am dropping Radio_1. Corollas that come with a cassette player almost always have a radio and vice versa. Similarly, we see a strong correlation of r = .88 between Powered_Windows_1 and Central_Lock_1, indicating those features typically come together. So, I am dropping Central_Lock_1 as well.\n\ntc &lt;- select(tc, -'Radio_1', -'Central_Lock_1')\n\n\n\n6. Partition your data into a training set with 70% of the observations and a testing set with the remaining 30%.\n\n# Partition the data.\nset.seed(1001)\nsamp = createDataPartition(tc$Log_Price, p = 0.7, list = FALSE)\ntraining = tc[samp, ]\ntraining &lt;- (select(training, -\"Price\"))\ntesting = tc[-samp, ]\ntesting &lt;- select(testing, -\"Price\")\nrm(samp)\n\n\n\n7. Based on your results and relationships in questions (4) and (5), build a regression tree model to predict car prices. Make sure to conduct cross validation to evaluate the model and choose the best cost complexity parameter for this problem (use default values for minsplit, minbucket, maxdepth, etc. But choose grid of cp values to tune over). Use rpart.plot to view your tree and discuss its complexity, usefulness, etc. What role is pre-pruning and post-pruning playing here?\n\n# Use cross-validation to determine the best cp value\ncontrol &lt;- trainControl(method = \"cv\", number = 10) \ncpGrid &lt;- expand.grid(.cp = seq(0.001, 0.05, by = 0.001))\ntrainResult &lt;- caret::train(Log_Price ~ ., data = training, method = \"rpart\", \n                     trControl = control, tuneGrid = cpGrid)\n\nbestCp &lt;- trainResult$bestTune$cp\nprint(bestCp)\n\n[1] 0.001\n\n# Train the regression tree model using the best cp value\ntreeModel &lt;- rpart(Log_Price ~ ., data = training, control = rpart.control(cp = bestCp))\n\n# Plot the tree\nrpart.plot(treeModel)\n\n\n\n\nThe best cp value to use for the model is .001. The regression tree has 8 levels. Pre-pruning occurs here by choosing a larger value for cp to get a smaller tree.\n\n\n8. Look at the feature importance (using permuted feature importance in “iml” package, with loss = “rmse” and compare = “ratio”) and determine which features have the biggest effect, and which might be okay to remove.\n\ntree_predictor = iml::Predictor$new(treeModel, data = training)\ntree_imp = iml::FeatureImp$new(tree_predictor, loss = \"rmse\", compare = \"ratio\", n.repetitions = 30)\nplot(tree_imp)\n\n\n\n\n\ntree_imp$results\n\n               feature importance.05 importance importance.95 permutation.error\n1                  Age      3.453734   3.541249      3.623929         0.3602261\n2                   KM      1.244454   1.267938      1.292348         0.1289783\n3                   HP      1.217618   1.246837      1.281325         0.1268318\n4      Mfr_Guarantee_1      1.017297   1.027802      1.037196         0.1045510\n5   Backseat_Divider_1      1.014711   1.023065      1.046020         0.1040691\n6    Powered_Windows_1      1.014042   1.023047      1.030123         0.1040673\n7              Airco_1      1.011434   1.020294      1.027566         0.1037872\n8    Automatic_airco_1      1.015513   1.017502      1.020919         0.1035033\n9              Doors_5      1.006945   1.016358      1.027445         0.1033869\n10      Metallic_Rim_1      1.009220   1.014366      1.020588         0.1031842\n11         Mistlamps_1      1.007987   1.011906      1.016627         0.1029340\n12   BOVAG_Guarantee_1      1.003002   1.008347      1.014604         0.1025719\n13  Guarantee_Period_6      1.002825   1.006808      1.010863         0.1024154\n14                  CC      1.000000   1.000000      1.000000         0.1017229\n15    Fuel_Type_Diesel      1.000000   1.000000      1.000000         0.1017229\n16    Fuel_Type_Petrol      1.000000   1.000000      1.000000         0.1017229\n17         Met_Color_1      1.000000   1.000000      1.000000         0.1017229\n18         Color_Black      1.000000   1.000000      1.000000         0.1017229\n19          Color_Blue      1.000000   1.000000      1.000000         0.1017229\n20         Color_Green      1.000000   1.000000      1.000000         0.1017229\n21          Color_Grey      1.000000   1.000000      1.000000         0.1017229\n22           Color_Red      1.000000   1.000000      1.000000         0.1017229\n23        Color_Silver      1.000000   1.000000      1.000000         0.1017229\n24         Color_White      1.000000   1.000000      1.000000         0.1017229\n25         Automatic_1      1.000000   1.000000      1.000000         0.1017229\n26             Doors_3      1.000000   1.000000      1.000000         0.1017229\n27             Doors_4      1.000000   1.000000      1.000000         0.1017229\n28             Gears_4      1.000000   1.000000      1.000000         0.1017229\n29             Gears_5      1.000000   1.000000      1.000000         0.1017229\n30             Gears_6      1.000000   1.000000      1.000000         0.1017229\n31  Guarantee_Period_3      1.000000   1.000000      1.000000         0.1017229\n32 Guarantee_Period_12      1.000000   1.000000      1.000000         0.1017229\n33               ABS_1      1.000000   1.000000      1.000000         0.1017229\n34          Airbag_1_1      1.000000   1.000000      1.000000         0.1017229\n35          Airbag_2_1      1.000000   1.000000      1.000000         0.1017229\n36     Boardcomputer_1      1.000000   1.000000      1.000000         0.1017229\n37         CD_Player_1      1.000000   1.000000      1.000000         0.1017229\n38    Power_Steering_1      1.000000   1.000000      1.000000         0.1017229\n39       Sport_Model_1      1.000000   1.000000      1.000000         0.1017229\n40    Radio_cassette_1      1.000000   1.000000      1.000000         0.1017229\n41 Parking_Assistant_1      1.000000   1.000000      1.000000         0.1017229\n42           Tow_Bar_1      1.000000   1.000000      1.000000         0.1017229\n\n\nAge, KM, and HP are the most important features. Any features with an importance value of 1 would probably be okay to be removed.\n\n\n9. Parsimony is about obtaining the simplest model possible, without oversimplifying. Remove a few of the less useful features and retrain / cross validate / tune your tree.\n\ntraining_simp &lt;- subset(training, select = -c(Tow_Bar_1, Parking_Assistant_1, Metallic_Rim_1, Boardcomputer_1,\n                                              BOVAG_Guarantee_1, Automatic_airco_1)) \ntesting_simp &lt;- subset(testing, select = -c(Tow_Bar_1, Parking_Assistant_1, Metallic_Rim_1, Boardcomputer_1,\n                                              BOVAG_Guarantee_1, Automatic_airco_1)) \n\n# Use cross-validation to determine the best cp value\ncontrol &lt;- trainControl(method = \"cv\", number = 10) \ncpGrid &lt;- expand.grid(.cp = seq(0.001, 0.05, by = 0.001))\ntrainResult &lt;- caret::train(Log_Price ~ ., data = training_simp, method = \"rpart\", \n                     trControl = control, tuneGrid = cpGrid)\n\nbestCp &lt;- trainResult$bestTune$cp\nprint(bestCp)\n\n[1] 0.001\n\n#  Train the regression tree model using the best cp value\ntreeModel &lt;- rpart(Log_Price ~ ., data = training_simp, control = rpart.control(cp = bestCp))\n\n# 5. Plot the tree\nrpart.plot(treeModel)\n\n\n\n\n\n\n10. Use the model resulting from question 9 and test predictions on the testing data. Compare the cross validation error and and testing data. Spend some time interpreting what this prediction error means for your pricing model and its use for CorollaCrowd.\n\n#  Use the trained regression tree model to make predictions on the testing data\npredictions &lt;- predict(treeModel, newdata = testing_simp)\n\n# Calculate the root mean squared error (RMSE) for the testing data\nrmse_test &lt;- sqrt(mean((predictions - testing_simp$Log_Price)^2))\n\n# Obtain the cross-validation error (RMSE) from the training phase\nrmse_cv &lt;- trainResult$results[which.min(trainResult$results$RMSE),]$RMSE\n\n# Print the results\nprint(paste(\"Cross-validation RMSE: \", rmse_cv))\n\n[1] \"Cross-validation RMSE:  0.121312437887304\"\n\nprint(paste(\"Testing RMSE: \", rmse_test))\n\n[1] \"Testing RMSE:  0.125736281299052\"\n\n# Compare the errors\nif (rmse_test &lt; rmse_cv) {\n  print(\"Testing error is lower than cross-validation error.\")\n} else if (rmse_test &gt; rmse_cv) {\n  print(\"Testing error is higher than cross-validation error.\")\n} else {\n  print(\"Testing error and cross-validation error are the same.\")\n}\n\n[1] \"Testing error is higher than cross-validation error.\"\n\n\nThe model performed slightly worse on the testing data, but the difference isn’t substantial. For CorollaCrowd, this model could be used to estimate car value."
  },
  {
    "objectID": "posts/Problem Set 4/index.html",
    "href": "posts/Problem Set 4/index.html",
    "title": "Problem Set 4",
    "section": "",
    "text": "The objective of the NVO is to predict a binary outcome: whether a person will respond to a mailing or not. Classification is the right approach to accomplish this task. By utilizing classification, the organization can take advantage of historical data to identify patterns or characteristics that are indicative of a positive response. This will enable the NVO to target individuals who are more likely to respond, increasing the overall response rate."
  },
  {
    "objectID": "posts/Problem Set 4/index.html#section",
    "href": "posts/Problem Set 4/index.html#section",
    "title": "Problem Set 4",
    "section": "",
    "text": "The objective of the NVO is to predict a binary outcome: whether a person will respond to a mailing or not. Classification is the right approach to accomplish this task. By utilizing classification, the organization can take advantage of historical data to identify patterns or characteristics that are indicative of a positive response. This will enable the NVO to target individuals who are more likely to respond, increasing the overall response rate."
  },
  {
    "objectID": "posts/Problem Set 4/index.html#section-1",
    "href": "posts/Problem Set 4/index.html#section-1",
    "title": "Problem Set 4",
    "section": "2.",
    "text": "2.\nOnce a classifier is built, the National Veterans Organization (NVO) can use it to predict the likelihood of potential donors responding to their mailings. Figuring out which variables contribute to a person being more likely to respond enables the organization to optimize their outreach efforts. If the classifier is accurate, this approach can lead to a higher response rate and a more efficient allocation of resources compared to blanket mailings. Additionally, by relying on data-driven predictions, the NVO can minimize biases or assumptions that might have influenced their previous outreach strategies. Having a more systematic and objective method to identify potential donors could lead to the NVO collecting more donations while also reducing the cost of outreach."
  },
  {
    "objectID": "posts/Problem Set 4/index.html#section-2",
    "href": "posts/Problem Set 4/index.html#section-2",
    "title": "Problem Set 4",
    "section": "3.",
    "text": "3.\nTo evaluate the classifiers performance I will look at several measures from the confusion matrix: precision, sensitivity and the F-measure. The precision of the classifier will tell me the proportion of the positive response predictions that are actually true. Sensitivity will indicate how well the classifier can detect positive responses overall. The F-measure is number derived from precision and sensitivity. By focusing on these metrics, NVO can ensure that they’re both maximizing the number of donation opportunities and ensuring that their outreach efforts are effective.\n\n# Import the data, and view\ndonors &lt;- read_csv(\"donors.csv\")\nstr(donors)\n\nspc_tbl_ [95,412 × 22] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ age                    : num [1:95412] 60 46 NA 70 78 NA 38 NA NA 65 ...\n $ numberChildren         : num [1:95412] NA 1 NA NA 1 NA 1 NA NA NA ...\n $ incomeRating           : num [1:95412] NA 6 3 1 3 NA 4 2 3 NA ...\n $ wealthRating           : num [1:95412] NA 9 1 4 2 NA 6 9 2 NA ...\n $ mailOrderPurchases     : num [1:95412] 0 16 2 2 60 0 0 1 0 0 ...\n $ totalGivingAmount      : num [1:95412] 240 47 202 109 254 51 107 31 199 28 ...\n $ numberGifts            : num [1:95412] 31 3 27 16 37 4 14 5 11 3 ...\n $ smallestGiftAmount     : num [1:95412] 5 10 2 2 3 10 3 5 10 3 ...\n $ largestGiftAmount      : num [1:95412] 12 25 16 11 15 16 12 11 22 15 ...\n $ averageGiftAmount      : num [1:95412] 7.74 15.67 7.48 6.81 6.86 ...\n $ yearsSinceFirstDonation: num [1:95412] 8 3 7 10 11 3 10 3 9 3 ...\n $ monthsSinceLastDonation: num [1:95412] 14 14 14 14 13 20 22 18 19 22 ...\n $ inHouseDonor           : logi [1:95412] FALSE FALSE FALSE FALSE TRUE FALSE ...\n $ plannedGivingDonor     : logi [1:95412] FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ sweepstakesDonor       : logi [1:95412] FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ P3Donor                : logi [1:95412] FALSE FALSE FALSE FALSE TRUE FALSE ...\n $ state                  : chr [1:95412] \"IL\" \"CA\" \"NC\" \"CA\" ...\n $ urbanicity             : chr [1:95412] \"town\" \"suburb\" \"rural\" \"rural\" ...\n $ socioEconomicStatus    : chr [1:95412] \"average\" \"highest\" \"average\" \"average\" ...\n $ isHomeowner            : logi [1:95412] NA TRUE NA NA TRUE NA ...\n $ gender                 : chr [1:95412] \"female\" \"male\" \"male\" \"female\" ...\n $ respondedMailing       : logi [1:95412] FALSE FALSE FALSE FALSE FALSE FALSE ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   age = col_double(),\n  ..   numberChildren = col_double(),\n  ..   incomeRating = col_double(),\n  ..   wealthRating = col_double(),\n  ..   mailOrderPurchases = col_double(),\n  ..   totalGivingAmount = col_double(),\n  ..   numberGifts = col_double(),\n  ..   smallestGiftAmount = col_double(),\n  ..   largestGiftAmount = col_double(),\n  ..   averageGiftAmount = col_double(),\n  ..   yearsSinceFirstDonation = col_double(),\n  ..   monthsSinceLastDonation = col_double(),\n  ..   inHouseDonor = col_logical(),\n  ..   plannedGivingDonor = col_logical(),\n  ..   sweepstakesDonor = col_logical(),\n  ..   P3Donor = col_logical(),\n  ..   state = col_character(),\n  ..   urbanicity = col_character(),\n  ..   socioEconomicStatus = col_character(),\n  ..   isHomeowner = col_logical(),\n  ..   gender = col_character(),\n  ..   respondedMailing = col_logical()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(donors)\n\n      age        numberChildren   incomeRating    wealthRating  \n Min.   : 1.00   Min.   :1.00    Min.   :1.000   Min.   :0.00   \n 1st Qu.:48.00   1st Qu.:1.00    1st Qu.:2.000   1st Qu.:3.00   \n Median :62.00   Median :1.00    Median :4.000   Median :6.00   \n Mean   :61.61   Mean   :1.53    Mean   :3.886   Mean   :5.35   \n 3rd Qu.:75.00   3rd Qu.:2.00    3rd Qu.:5.000   3rd Qu.:8.00   \n Max.   :98.00   Max.   :7.00    Max.   :7.000   Max.   :9.00   \n NA's   :23665   NA's   :83026   NA's   :21286   NA's   :44732  \n mailOrderPurchases totalGivingAmount  numberGifts      smallestGiftAmount\n Min.   :  0.000    Min.   :  13.0    Min.   :  1.000   Min.   :   0.000  \n 1st Qu.:  0.000    1st Qu.:  40.0    1st Qu.:  3.000   1st Qu.:   3.000  \n Median :  0.000    Median :  78.0    Median :  7.000   Median :   5.000  \n Mean   :  3.321    Mean   : 104.5    Mean   :  9.602   Mean   :   7.934  \n 3rd Qu.:  3.000    3rd Qu.: 131.0    3rd Qu.: 13.000   3rd Qu.:  10.000  \n Max.   :241.000    Max.   :9485.0    Max.   :237.000   Max.   :1000.000  \n                                                                          \n largestGiftAmount averageGiftAmount  yearsSinceFirstDonation\n Min.   :   5      Min.   :   1.286   Min.   : 0.000         \n 1st Qu.:  14      1st Qu.:   8.385   1st Qu.: 2.000         \n Median :  17      Median :  11.636   Median : 5.000         \n Mean   :  20      Mean   :  13.348   Mean   : 5.596         \n 3rd Qu.:  23      3rd Qu.:  15.478   3rd Qu.: 9.000         \n Max.   :5000      Max.   :1000.000   Max.   :13.000         \n                                                             \n monthsSinceLastDonation inHouseDonor    plannedGivingDonor sweepstakesDonor\n Min.   : 0.00           Mode :logical   Mode :logical      Mode :logical   \n 1st Qu.:12.00           FALSE:88709     FALSE:95298        FALSE:93795     \n Median :14.00           TRUE :6703      TRUE :114          TRUE :1617      \n Mean   :14.36                                                              \n 3rd Qu.:17.00                                                              \n Max.   :23.00                                                              \n                                                                            \n  P3Donor           state            urbanicity        socioEconomicStatus\n Mode :logical   Length:95412       Length:95412       Length:95412       \n FALSE:93395     Class :character   Class :character   Class :character   \n TRUE :2017      Mode  :character   Mode  :character   Mode  :character   \n                                                                          \n                                                                          \n                                                                          \n                                                                          \n isHomeowner       gender          respondedMailing\n Mode:logical   Length:95412       Mode :logical   \n TRUE:52354     Class :character   FALSE:90569     \n NA's:43058     Mode  :character   TRUE :4843      \n                                                   \n                                                   \n                                                   \n                                                   \n\n# Compute the number and percentage of NAs for each column\nna_summary &lt;- donors %&gt;%\n  summarise_all(~sum(is.na(.))) %&gt;%\n  gather(column, na_count) %&gt;%\n  mutate(na_percentage = (na_count / nrow(donors)) * 100) %&gt;%\n  filter(na_count &gt; 1)\nprint(na_summary)\n\n# A tibble: 8 × 3\n  column              na_count na_percentage\n  &lt;chr&gt;                  &lt;int&gt;         &lt;dbl&gt;\n1 age                    23665         24.8 \n2 numberChildren         83026         87.0 \n3 incomeRating           21286         22.3 \n4 wealthRating           44732         46.9 \n5 urbanicity              2316          2.43\n6 socioEconomicStatus     2316          2.43\n7 isHomeowner            43058         45.1 \n8 gender                  4676          4.90\n\n\n\n# Drop number of children\ndonors &lt;- donors %&gt;% select(-numberChildren) %&gt;%\n  na.omit(donors)\n\n87% of the data in numberChildren is missing, so I dropped it due to it not being useful as a predictor. Since we have such a large amount of data I have decided to drop every record which has missing values. There is a lot of missing data, but even after omitting every incomplete record we still have 33230 observations to work with.\n\n# Change categorical types to factor variables\ndonors &lt;- donors %&gt;%\n  mutate_at(vars(incomeRating, wealthRating, socioEconomicStatus, \n                 gender, state, urbanicity, respondedMailing), .funs=factor)\n\n# Remove rows where respondedMailing or any numeric column is NA\ndonors_num_cleaned &lt;- donors %&gt;%\n  filter(!is.na(respondedMailing)) %&gt;%\n  select(age, mailOrderPurchases, \n         totalGivingAmount, numberGifts, smallestGiftAmount, largestGiftAmount, \n         averageGiftAmount, yearsSinceFirstDonation, monthsSinceLastDonation, \n         respondedMailing) %&gt;%\n              na.omit()\n\n\n## Histogram Plots\n\n# Set a transparent theme for better visualization\ntransparentTheme(trans = 0.9)\n\n# Create Histogram plots\nfeaturePlot(x = donors_num_cleaned %&gt;% select(-respondedMailing),\n            y = donors_num_cleaned$respondedMailing,\n            plot = \"density\",\n            scale = list(x = list(relation='free'),\n                         y = list(relation='free')),\n            adjust = 1.5,\n            pch = \"|\",\n            layout = c(3,4),\n            auto.key = list(columns = 2))\n\n\n\n\n\n# Boxplots\ncaret::featurePlot(x = donors_num_cleaned %&gt;% select(-respondedMailing),\n                   y = donors_num_cleaned$respondedMailing,\n                   plot = \"box\",\n                   scales = list(y = list(relation='free'),\n                                 x = list(rot=90)),\n                   layout = c(3,4),\n                   auto.key = list(columns = 2))\n\n\n\n\n\n# Association between wealth and income ratings\ntable(donors$incomeRating, donors$wealthRating)\n\n   \n       0    1    2    3    4    5    6    7    8    9\n  1  282  335  310  259  241  258  228  211  150   84\n  2  338  558  616  574  597  548  552  441  316  134\n  3  158  252  343  363  387  420  459  453  384  238\n  4  208  364  461  498  654  806  872  974  931  699\n  5  175  284  409  546  711  914 1019 1228 1406 1205\n  6   46   77  117  138  206  313  481  568  867 1389\n  7   49   58   80   95  138  183  277  526  866 1903\n\nvcd::assocstats(table(donors$incomeRating, donors$wealthRating))\n\n                    X^2 df P(&gt; X^2)\nLikelihood Ratio 7960.4 54        0\nPearson          8155.4 54        0\n\nPhi-Coefficient   : NA \nContingency Coeff.: 0.444 \nCramer's V        : 0.202 \n\n\nI suspected there would be a relationship between incomeRating and wealthRating. A Cramer’s V value of 0.202 indicates that there is a weak to moderate relationship between these two variables. I am going to leave them both in.\n\n# Look for correlation between numeric variables\n\ndonors %&gt;%\n  keep(is.numeric) %&gt;%\n  cor(use = \"pairwise.complete.obs\") %&gt;%\n  corrplot::corrplot()\n\n\n\n\nWe see positive correlation between averageGiftAmount with both largestGiftAmount and smallestGiftAmount. There’s also positive correlation between yearsSinceFirstDonation and numberGifts. These are all medium strength correlations, so for now I will leave them in."
  },
  {
    "objectID": "posts/Problem Set 4/index.html#build-a-logistic-lasso-model-using-cross-validation-on-the-training-data-to-select-the-best-𝞴.-view-the-coefficients-at-that-chosen-𝞴-and-see-what-features-are-in-the-model.",
    "href": "posts/Problem Set 4/index.html#build-a-logistic-lasso-model-using-cross-validation-on-the-training-data-to-select-the-best-𝞴.-view-the-coefficients-at-that-chosen-𝞴-and-see-what-features-are-in-the-model.",
    "title": "Problem Set 4",
    "section": "4. Build a logistic LASSO model using cross-validation on the training data to select the best 𝞴. View the coefficients at that chosen 𝞴 and see what features are in the model.",
    "text": "4. Build a logistic LASSO model using cross-validation on the training data to select the best 𝞴. View the coefficients at that chosen 𝞴 and see what features are in the model.\n\n# Creating dummy variables from categorical variables\ndonors &lt;- dummy_cols(donors, select_columns = c(\"incomeRating\", \"wealthRating\", \"socioEconomicStatus\",\n                                                \"gender\", \"state\", \"urbanicity\"),\n                     remove_selected_columns = TRUE) %&gt;%\n  select(-incomeRating_1, -wealthRating_0, -socioEconomicStatus_average, -gender_joint, -gender_male,\n         -state_AA, -state_AA, -urbanicity_rural)\n\n# Scaling numerical variables\nnumerical_vars &lt;- c('age', 'mailOrderPurchases', 'totalGivingAmount', \n                     'numberGifts', 'smallestGiftAmount', 'largestGiftAmount', 'averageGiftAmount', \n                     'yearsSinceFirstDonation', 'monthsSinceLastDonation')\n\ndonors[numerical_vars] &lt;- scale(donors[numerical_vars])\n\n# Partition the data.\nset.seed(1001)\nsamp = createDataPartition(donors$respondedMailing, p = 0.7, list = FALSE)\ntraining = donors[samp, ]\ntesting = donors[-samp, ]\nrm(samp)\n\n#check for class imbalance\ntraining %&gt;%\n  select(respondedMailing) %&gt;%\n  table() %&gt;%\n  prop.table()\n\nrespondedMailing\n     FALSE       TRUE \n0.94712406 0.05287594 \n\n\nThere is a significant class imbalance, I will use smote to correct it.\n\n# Smote \n\nsmote_train = smote(respondedMailing ~ .,\n                    data = training)\n\ntable(smote_train$respondedMailing)\n\n\nFALSE  TRUE \n 4920  3690 \n\n\n\n# Separate predictors and response\ny &lt;- as.vector(smote_train$respondedMailing)\nX &lt;- as.matrix(smote_train %&gt;% select(-respondedMailing))\n\n# Use cross-validation to find the best lambda\ncv.lasso &lt;- cv.glmnet(X, y, family=\"binomial\", alpha=1)\n\n# Extract best lambda\nbest_lambda &lt;- cv.lasso$lambda.1se\n\n# Fit the model using the best lambda\nLASSO_model &lt;- glmnet(X, y, family=\"binomial\", alpha=1, lambda=best_lambda, maxit = 1e6)\n\n# View the coefficients\ncoef(LASSO_model)\n\n89 x 1 sparse Matrix of class \"dgCMatrix\"\n                                      s0\n(Intercept)                 -0.598546198\nage                          0.045580706\nmailOrderPurchases           .          \ntotalGivingAmount            .          \nnumberGifts                  .          \nsmallestGiftAmount           .          \nlargestGiftAmount           -0.230728654\naverageGiftAmount           -0.117186971\nyearsSinceFirstDonation      .          \nmonthsSinceLastDonation     -0.092631409\ninHouseDonor                -0.012863467\nplannedGivingDonor           .          \nsweepstakesDonor             .          \nP3Donor                      .          \nisHomeowner                  .          \nincomeRating_2               .          \nincomeRating_3               .          \nincomeRating_4              -0.279123095\nincomeRating_5              -0.251042345\nincomeRating_6               0.361244872\nincomeRating_7              -0.135812460\nwealthRating_1               .          \nwealthRating_2               .          \nwealthRating_3               0.242357201\nwealthRating_4              -0.133213966\nwealthRating_5              -0.148504342\nwealthRating_6               0.095310582\nwealthRating_7               0.097658191\nwealthRating_8               0.319708802\nwealthRating_9              -0.219063254\nsocioEconomicStatus_highest  .          \nsocioEconomicStatus_lowest  -0.423174836\ngender_female               -0.445589926\nstate_AE                     0.575545901\nstate_AK                     .          \nstate_AL                     1.841101729\nstate_AP                     .          \nstate_AR                     .          \nstate_AZ                     .          \nstate_CA                     0.321249192\nstate_CO                     .          \nstate_CT                     .          \nstate_DE                     .          \nstate_FL                     .          \nstate_GA                     1.221292620\nstate_HI                     .          \nstate_IA                     .          \nstate_ID                     0.388287462\nstate_IL                     .          \nstate_IN                    -0.003340735\nstate_KS                     .          \nstate_KY                     .          \nstate_LA                     .          \nstate_MA                     .          \nstate_MD                     .          \nstate_ME                     .          \nstate_MI                     .          \nstate_MN                     1.583330439\nstate_MO                     .          \nstate_MS                    -0.228310896\nstate_MT                     .          \nstate_NC                     .          \nstate_ND                     .          \nstate_NE                    -0.140898223\nstate_NH                     .          \nstate_NJ                     .          \nstate_NM                     .          \nstate_NV                     .          \nstate_NY                     .          \nstate_OH                     .          \nstate_OK                     2.168223127\nstate_OR                     .          \nstate_PA                     .          \nstate_RI                     .          \nstate_SC                     0.038535568\nstate_SD                     0.892871747\nstate_TN                     .          \nstate_TX                     .          \nstate_UT                     .          \nstate_VA                     .          \nstate_VT                     .          \nstate_WA                     .          \nstate_WI                    -0.256755139\nstate_WV                     0.403782677\nstate_WY                     0.863087203\nurbanicity_city             -0.086388277\nurbanicity_suburb            0.421327308\nurbanicity_town              .          \nurbanicity_urban             .          \n\n\nAt the chosen 𝞴 = .0091, the features in our model are: age, largestGiftAmount, averageGiftAmount, monthsSinceLastDonation, inHouseDonor, incomeRating_4, incomeRating_5, incomeRating_6, incomeRating_7, wealthRating_3, wealthRating_4, wealthRating_5, wealthRating_6, wealthRating_7, wealthRating_8, wealthRating_9, socioEconomicStatus_lowest, gender_female, state_AE, state_AL, state_CA, state_GA, state_ID, state_IN, state_MN, state_MS, state_NE, state_OK, state_SC, state_SD, state_WI, state_WV, state_WY, urbanicity_city, urbanicity_suburb.\n\n# Build a decision tree model. Crossvalidate and tune over values of cp.\nset.seed(1001)\nctrl = caret::trainControl(method = \"repeatedcv\", number = 5, repeats = 30)\ntree_model = caret::train(respondedMailing ~ ., \n             data = smote_train, \n             method = \"rpart\",\n             metric = \"Kappa\",\n             trControl = ctrl,\n             tuneGrid = expand.grid(cp = seq(0.0, 0.1, 0.005)))\n\nbestCp &lt;- tree_model$bestTune$cp\n\n# Plot the cp\nplot(tree_model)\n\n\n\n\n\n# Train the regression tree model using the best cp value\ntree_model &lt;- rpart(respondedMailing ~ ., data = smote_train, control = rpart.control(cp = bestCp))\n\nrpart.plot::rpart.plot(tree_model)\n\n\n\n\nThe decision tree uses the features: gender_female, urbanicity_suburb, state_MN, state_OK, state_CA, state_GA, state_AL, wealthRating_7.\n\ntest_predictors = as.matrix(testing %&gt;% select(-respondedMailing))\n\n# Performance of LASSO\nLASSO_test_class = predict(LASSO_model, newx = test_predictors, s = best_lambda, type=\"class\")\nLASSO_test_prob = predict(LASSO_model, newx = test_predictors, s = best_lambda, type=\"response\")[,1]\n\n# Performance of tree\ntree_test_class = predict(tree_model, newdata = as.data.frame(test_predictors), type=\"class\")\ntree_test_prob = predict(tree_model, newdata = as.data.frame(test_predictors), type=\"prob\")[,1]\n\n\n# Confusion Matrices\nresponse_vector &lt;- as.vector(testing$respondedMailing)\n\nLASSO_cm = confusionMatrix(factor(LASSO_test_class), factor(response_vector), positive = \"TRUE\")\ntree_cm = confusionMatrix(factor(tree_test_class), factor(response_vector), positive = \"TRUE\")\n\n\nLASSO_cm\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction FALSE TRUE\n     FALSE  7985  437\n     TRUE   1456   90\n                                          \n               Accuracy : 0.8101          \n                 95% CI : (0.8023, 0.8178)\n    No Information Rate : 0.9471          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.0087          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.170778        \n            Specificity : 0.845779        \n         Pos Pred Value : 0.058215        \n         Neg Pred Value : 0.948112        \n             Prevalence : 0.052869        \n         Detection Rate : 0.009029        \n   Detection Prevalence : 0.155096        \n      Balanced Accuracy : 0.508279        \n                                          \n       'Positive' Class : TRUE            \n                                          \n\n\n\ntree_cm\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction FALSE TRUE\n     FALSE  9376  520\n     TRUE     65    7\n                                          \n               Accuracy : 0.9413          \n                 95% CI : (0.9365, 0.9458)\n    No Information Rate : 0.9471          \n    P-Value [Acc &gt; NIR] : 0.9951          \n                                          \n                  Kappa : 0.0108          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.0132827       \n            Specificity : 0.9931151       \n         Pos Pred Value : 0.0972222       \n         Neg Pred Value : 0.9474535       \n             Prevalence : 0.0528692       \n         Detection Rate : 0.0007022       \n   Detection Prevalence : 0.0072231       \n      Balanced Accuracy : 0.5031989       \n                                          \n       'Positive' Class : TRUE            \n                                          \n\n\n\ntree_roc = roc(testing$respondedMailing ~ tree_test_prob,\n                plot=TRUE, print.auc=TRUE, print.auc.y=0.3,\n                col = \"black\", lwd=3, legacy.axes=TRUE)"
  },
  {
    "objectID": "PS7.html",
    "href": "PS7.html",
    "title": "Problem Set 7",
    "section": "",
    "text": "The LHBA depends on the revenue generated from the sale of homes. In order to make a profit, they need to be able to make good predictions for the prices the homes will sell for. There is some debate about whether or not to trust the county-appraised values, because before the housing bubble burst houses were selling for prices that were radically different than these estimates. It will be important to figure out if since the bubble burst, home prices have fallen closer in line to the county-appraised values, or if there are still factors affecting prices that the government doesn’t take into account. It may be possible to build a model which does a much better job at predicting the prices homes will sell for."
  },
  {
    "objectID": "PS7.html#lhbas-big-picture-business-problem",
    "href": "PS7.html#lhbas-big-picture-business-problem",
    "title": "Problem Set 7",
    "section": "",
    "text": "The LHBA depends on the revenue generated from the sale of homes. In order to make a profit, they need to be able to make good predictions for the prices the homes will sell for. There is some debate about whether or not to trust the county-appraised values, because before the housing bubble burst houses were selling for prices that were radically different than these estimates. It will be important to figure out if since the bubble burst, home prices have fallen closer in line to the county-appraised values, or if there are still factors affecting prices that the government doesn’t take into account. It may be possible to build a model which does a much better job at predicting the prices homes will sell for."
  },
  {
    "objectID": "PS7.html#questions-for-lhba-shareholders",
    "href": "PS7.html#questions-for-lhba-shareholders",
    "title": "Problem Set 7",
    "section": "2. Questions for LHBA Shareholders",
    "text": "2. Questions for LHBA Shareholders\n\nWhere is there disagreement on how to price homes?\nIn which locations does LHBA conduct most of its business?\nDoes the company primarily sell single-family homes, or are there other types of properties?"
  },
  {
    "objectID": "PS7.html#the-analytics-problem",
    "href": "PS7.html#the-analytics-problem",
    "title": "Problem Set 7",
    "section": "3. The Analytics problem",
    "text": "3. The Analytics problem\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyverse)\n\n\npre &lt;- read_csv(\"posts/Problem Set 7/PreCrisisCV.csv\")\npost &lt;- read_csv(\"posts/Problem Set 7/PostCrisisCV.csv\")\ntest &lt;- read_csv(\"posts/Problem Set 7/OnMarketTest-1.csv\")"
  },
  {
    "objectID": "posts/Problem Set 6/Problem Set 6.html",
    "href": "posts/Problem Set 6/Problem Set 6.html",
    "title": "Problem Set 6",
    "section": "",
    "text": "rm(list = ls())\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(performanceEstimation)\nlibrary(PRROC)\nlibrary(rpart)\n# Read in data and remove unneccessary features\nbank = read_csv(\"UniversalBank.csv\") %&gt;%\n  select(-ID, -`ZIP Code`, -Experience) %&gt;%\n  rename(Loan = `Personal Loan`,\n         Securities = `Securities Account`,\n         CD = `CD Account`) %&gt;%\n  mutate_at(vars(Loan, Education), .fun = factor)\n# MISSING DATA\n# =============\n# Calculate percent of missing values for features\nmissing_df =  as.numeric(purrr::map(bank, ~mean(is.na(.))))*100\n# Assign values to data frame for easy viewing\ndf = data.frame(PercentMissing = missing_df,\n                row.names = names(bank)) %&gt;%\n  arrange(desc(PercentMissing))\n\nprint(df)\n\n           PercentMissing\nAge                     0\nIncome                  0\nFamily                  0\nCCAvg                   0\nEducation               0\nMortgage                0\nLoan                    0\nSecurities              0\nCD                      0\nOnline                  0\nCreditCard              0\nFortunately, we don’t have any missing values."
  },
  {
    "objectID": "posts/Problem Set 6/Problem Set 6.html#partition-data",
    "href": "posts/Problem Set 6/Problem Set 6.html#partition-data",
    "title": "Problem Set 6",
    "section": "Partition Data",
    "text": "Partition Data\n\n# Partition the Data\nset.seed(453)\nsamp = createDataPartition(bank$Loan, p = 0.7, list = FALSE)\ntrain = bank[samp, ]\ntest = bank[-samp, ]\nrm(samp)\n\n\nAddress Class Imbalance\n\n# Address class imbalance\ntable(train$Loan)\n\n\n   0    1 \n3164  336 \n\n\n\nbalanced_train = smote(Loan ~ .,\n              data = train,\n              perc.over = 6,\n              perc.under = 1.5)\ntable(balanced_train$Loan)\n\n\n   0    1 \n3024 2352"
  },
  {
    "objectID": "posts/Problem Set 6/Problem Set 6.html#best-tuned-decision-tree",
    "href": "posts/Problem Set 6/Problem Set 6.html#best-tuned-decision-tree",
    "title": "Problem Set 6",
    "section": "Best Tuned Decision Tree",
    "text": "Best Tuned Decision Tree\n\n# training and evaluation\nctrl = caret::trainControl(method = \"repeatedcv\", number = 7, repeats = 15)\nset.seed(890)\ntree = caret::train(Loan ~ .,\n             data = balanced_train,\n             method = \"rpart\",\n             metric = \"Kappa\",\n             trControl = ctrl,\n             tuneGrid = expand.grid(cp = seq(0.0, 0.1, 0.001)),\n             control = rpart.control(minsplit = 1, minbucket = 1, maxdepth = 8)\n             )\n\n\nrpart.plot::rpart.plot(tree$finalModel)"
  },
  {
    "objectID": "posts/Problem Set 6/Problem Set 6.html#best-tuned-random-forest",
    "href": "posts/Problem Set 6/Problem Set 6.html#best-tuned-random-forest",
    "title": "Problem Set 6",
    "section": "Best Tuned Random Forest",
    "text": "Best Tuned Random Forest\n\n# set.seed(285)\n# forest = caret::train(Loan ~ .,\n#                       data = balanced_train,\n#                       method = \"rf\",\n#                       metric = \"Kappa\",\n#                       trControl = ctrl,\n#                       ntree = 500,\n#                       tuneGrid = expand.grid(.mtry = seq(2,8,1))\n#                       )\n# saveRDS(forest, \"forest.rds\")\nforest = readRDS(\"forest.rds\")\nplot(forest)"
  },
  {
    "objectID": "posts/Problem Set 6/Problem Set 6.html#best-tuned-boosting-adaboost-gradient-boosting",
    "href": "posts/Problem Set 6/Problem Set 6.html#best-tuned-boosting-adaboost-gradient-boosting",
    "title": "Problem Set 6",
    "section": "Best Tuned Boosting (Adaboost / Gradient Boosting)",
    "text": "Best Tuned Boosting (Adaboost / Gradient Boosting)\n\n# boost_grid = expand.grid(\n#   maxdepth = c(2, 3, 4, 5, 6, 7, 8),\n#   iter = c(100, 150, 200, 250, 300),\n#   nu = 0.1\n# )\n# \n# boost_ctrl = caret::trainControl(method = \"cv\",\n#                           number = 10,\n#                           allowParallel = TRUE)\n# \n# set.seed(623)\n# boosted_trees = caret::train(Loan ~ .,\n#                       data = balanced_train,\n#                       trControl = boost_ctrl,\n#                       tuneGrid = boost_grid,\n#                       method = \"ada\",\n#                       metric = \"Kappa\")\n\n# saveRDS(boosted_trees, \"boosted_trees.rds\")\nboosted_trees = readRDS(\"boosted_trees.rds\")\n\n\nplot(boosted_trees)"
  },
  {
    "objectID": "posts/Problem Set 6/Problem Set 6.html#compare-precision-and-sensitivity",
    "href": "posts/Problem Set 6/Problem Set 6.html#compare-precision-and-sensitivity",
    "title": "Problem Set 6",
    "section": "Compare Precision and Sensitivity",
    "text": "Compare Precision and Sensitivity\n\n# Convert Y in test data to numeric 0, 1.\ntest = mutate(test, Loan = as.numeric(ifelse(Loan==\"1\", 1, 0)))\n\n# Create explainers\ntree_explain = DALEX::explain(tree,\n                              data = test,\n                              y = test$Loan,\n                              type = \"classification\",\n                              label = \"Decision Tree\")\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  Decision Tree \n  -&gt; data              :  1500  rows  11  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  1500  values \n  -&gt; predict function  :  yhat.train  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package caret , ver. 6.0.94 , task classification (  default  ) \n  -&gt; model_info        :  type set to  classification \n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.1038884 , max =  1  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -1 , mean =  -0.007888389 , max =  1  \n  A new explainer has been created!  \n\nforest_explain = DALEX::explain(forest,\n                                data = test,\n                                y = test$Loan,\n                                label = \"Random Forest\")\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  Random Forest \n  -&gt; data              :  1500  rows  11  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  1500  values \n  -&gt; predict function  :  yhat.train  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package caret , ver. 6.0.94 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.1142973 , max =  1  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.838 , mean =  -0.01829733 , max =  0.9  \n  A new explainer has been created!  \n\nadaboost_explain = DALEX::explain(boosted_trees,\n                                  data = test,\n                                  y = test$Loan,\n                                  label = \"AdaBoost\")\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  AdaBoost \n  -&gt; data              :  1500  rows  11  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  1500  values \n  -&gt; predict function  :  yhat.train  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package caret , ver. 6.0.94 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  9.799935e-16 , mean =  0.09698423 , max =  1  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9999999 , mean =  -0.0009842281 , max =  1  \n  A new explainer has been created!"
  },
  {
    "objectID": "posts/Problem Set 6/Problem Set 6.html#roc-plot-and-comparing-auc",
    "href": "posts/Problem Set 6/Problem Set 6.html#roc-plot-and-comparing-auc",
    "title": "Problem Set 6",
    "section": "ROC Plot and Comparing AUC",
    "text": "ROC Plot and Comparing AUC\n\n# Model Performance\ntree_perf = DALEX::model_performance(tree_explain)\nforest_perf = DALEX::model_performance(forest_explain)\nadaboost_perf = DALEX::model_performance(adaboost_explain)\n\n# Plot the Precision Recall Curve\nplot(tree_perf, forest_perf, adaboost_perf, geom = 'prc')\n\n\n\n\nRandom Forest performed the best, with both high precision and high recall. Adaboost did slightly worse, but overall still performed well. The decision tree was clearly the worst, as the graph shows a significant drop in precision and recall compared to the other models.\n\n# Plot the ROC\nplot(tree_perf, forest_perf, adaboost_perf, geom = 'roc')\n\n\n\n\n\n# Compare the AUCs\nmatrix(c(\"Model\",\n         \"Decision Tree\",\n         \"Random Forest\",\n         \"Adaboost\",\n         \"AUC\",\n         round(tree_perf$measures$auc, 5),\n         round(forest_perf$measures$auc, 5),\n         round(adaboost_perf$measures$auc, 5)),\n       ncol = 2\n         )\n\n     [,1]            [,2]     \n[1,] \"Model\"         \"AUC\"    \n[2,] \"Decision Tree\" \"0.9367\" \n[3,] \"Random Forest\" \"0.99701\"\n[4,] \"Adaboost\"      \"0.99665\"\n\n\nAgain, it appears Random Forest performed the best as it had the largest AUC. However, for practical purposes it is essentially the same as Adaboost. Decision Tree was also clearly the worse, again."
  },
  {
    "objectID": "posts/Problem Set 6/Problem Set 6.html#importance-of-partitioning",
    "href": "posts/Problem Set 6/Problem Set 6.html#importance-of-partitioning",
    "title": "Problem Set 6",
    "section": "Importance of partitioning",
    "text": "Importance of partitioning\nPartitioning the data means randomly splitting our customer records into two sets: a training set and a testing set. This happens randomly, so there shouldn’t be significant differences between the features of the customers in one set versus the other. Splitting the records up randomly is important because, for example, if we trained our model solely on low-income customers we may not get good predictions for high-income customers, and vice versa. Once the was partitioned, we could create our models to predict whether or not customers will accept a loan.\nThe models were trained solely on the training dataset. During training, the models would “practice” predicting outcomes for customers in the training dataset, and that is how they would learn. However, these practice tests couldn’t necessarily tell us how the model would perform on customer records that weren’t used in training. In the real world our models will be used on new customer data, and that is why we must evaluate our models on data it hasn’t seen before. That is why it is so important we don’t feed all of our data into training the model: we need a separate testing set which is left out of training to get an idea how it will actually perform on new customers."
  },
  {
    "objectID": "posts/Problem Set 6/Problem Set 6.html#value-of-bagging-and-ensemble-models",
    "href": "posts/Problem Set 6/Problem Set 6.html#value-of-bagging-and-ensemble-models",
    "title": "Problem Set 6",
    "section": "Value of bagging and ensemble models",
    "text": "Value of bagging and ensemble models\nAs we’ve seen, regular decision trees can be outperformed by more sophisticated models. Bagging, which is short for “bootstrap aggregation” allows us to train many models at once by creating random subsets of the training data to work with. When a record from the training set is randomly chosen to be part of a new training subset, it can be resampled and become part of other training subsets. By resampling records we can create many subsets of the training data, and try to make predictions using each of them. Once we have models trained on these subsets, we aggregate their output and see which outcome had more “votes.” Basically, instead of one prediction, i.e. accepting or rejecting the loan, we have a whole list of predictions and see which category had more votes.\nEnsemble methods such as random forest and adaboost combine different types of models together, whereas bagging relies on many instances of the same model. The advantage of ensemble methods is that we can take advantage of the strengths of different models. The result is better predictions, which we can see in our own implementation: the ensemble methods are clearly superior."
  }
]