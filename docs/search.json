[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joe Penders",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nThe page your viewing is the “index” page, or the landing page of your site. The site is just a quarto document. And you can put anything on it that could be in a quarto document.\nThere are also settings to get a list of site items you may want people to have easy access to. For example, a list of your blog posts, projects, etc.\nThere are different styles of landing pages. In this particular style, the index.qmd page is used as an about page. Instead of immediately showcasing your work, it showcases you! You can introduce yourself and include your contact information right by your image and description.\nYou can then also decide to have some content show up below the about portion as well."
  },
  {
    "objectID": "posts/Demo Post 1/index.html",
    "href": "posts/Demo Post 1/index.html",
    "title": "Demo Post 1",
    "section": "",
    "text": "This is a demo post in which we begin the blog. The idea here is that you create one post with this quarto document. The quarto document for a post will be named “index.qmd” insides of folder with the name of the post. For example, if I wanted my post to be titled “Demo Post 1” then I would do the following.\nAfter doing that, you can then edit the index.qmd document for that new post to your heart’s content. Lets do a little of that now so you can see how this might work."
  },
  {
    "objectID": "posts/Demo Post 1/index.html#including-resources",
    "href": "posts/Demo Post 1/index.html#including-resources",
    "title": "Demo Post 1",
    "section": "Including Resources",
    "text": "Including Resources\nSuppose you wanted to discuss something, like the CRISP-DM process for analytics projects. You might wish to refer to an image of the process and you could include the image in the “Demo Post 1” folder and reference it here in the document.\n\n\n\n\n\nYou can easily insert the image through the visual editor in Posit / RStudio."
  },
  {
    "objectID": "posts/Demo Post 1/index.html#data-and-output",
    "href": "posts/Demo Post 1/index.html#data-and-output",
    "title": "Demo Post 1",
    "section": "Data and Output",
    "text": "Data and Output\nLets look at some data.\n\nlibrary(tidyverse)\nlibrary(ggthemes)\ndata(\"USArrests\")\n\nUSArrests %&gt;%\n  ggplot(aes(x = Assault, y = Murder)) +\n  geom_point(pch = 21, color = \"coral3\", bg = \"coral\", size=3) +\n  labs(title = \"Arrests for Murder vs. Assault in US States\",\n       x = \"Arrests for assault per 100,000\",\n       y = \"Arrests for murder per 100,000\") +\n  theme_clean()\n\n\n\n\nThis would show us a relationship that we could then spend some paragraphs analyzing and interpreting."
  },
  {
    "objectID": "posts/Demo Post 2/index.html",
    "href": "posts/Demo Post 2/index.html",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %&gt;%\n  mutate(predicted_murder = predict(dt, USArrests)) %&gt;%\n  select(Murder, predicted_murder) %&gt;%\n  cor() -&gt; corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "posts/Demo Post 2/index.html#understanding-the-data",
    "href": "posts/Demo Post 2/index.html#understanding-the-data",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.…\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24…\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6…\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2…\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %&gt;%\n  mutate(predicted_murder = predict(dt, USArrests)) %&gt;%\n  select(Murder, predicted_murder) %&gt;%\n  cor() -&gt; corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects are different than posts. Projects should be more expansive, impressive and generally more professional in nature compared to posts. Posts can be works in progress. Small ideas or things you did that you thought were interesting. Projects should really showcase your professional abilities. You don’t need to have too many, just make them good. And try to always have one “in the works” so that employers and collaborators can see that you’re driven.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Problem Set 3\n\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2023\n\n\nJoe Penders\n\n\n\n\n\n\n  \n\n\n\n\nDemo Post 2\n\n\n\n\n\n\n\ndecision trees\n\n\nmachine learning\n\n\narrests\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nJane Doe\n\n\n\n\n\n\n  \n\n\n\n\nDemo Post 1\n\n\n\n\n\n\n\nquarto\n\n\ncrisp-dm\n\n\nscatterplot\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nJane Doe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Problem Set 3/index.html",
    "href": "posts/Problem Set 3/index.html",
    "title": "Problem Set 3",
    "section": "",
    "text": "1. Create a new Rmarkdown document that will show case your data exploration, modeling, and error testing.\n\nrm(list = ls())\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8\n\nlibrary(lubridate)\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(dummy)\n\ndummy 0.1.3\ndummyNews()\n\nlibrary(lattice)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(iml)\nlibrary(patchwork)\ntc = read_csv(\"ToyotaCorolla.csv\")\n\nRows: 1436 Columns: 39\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Model, Fuel_Type, Color\ndbl (36): Id, Price, Age_08_04, Mfg_Month, Mfg_Year, KM, HP, Met_Color, Auto...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n2. Explore the data and determine the number of variables and the quantity of any missing values.\nIf values are missing, prescribe a plan to deal with the problem.\n\nstr(tc)\n\nspc_tbl_ [1,436 × 39] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Id               : num [1:1436] 1 2 3 4 5 6 7 8 9 10 ...\n $ Model            : chr [1:1436] \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" ...\n $ Price            : num [1:1436] 13500 13750 13950 14950 13750 ...\n $ Age_08_04        : num [1:1436] 23 23 24 26 30 32 27 30 27 23 ...\n $ Mfg_Month        : num [1:1436] 10 10 9 7 3 1 6 3 6 10 ...\n $ Mfg_Year         : num [1:1436] 2002 2002 2002 2002 2002 ...\n $ KM               : num [1:1436] 46986 72937 41711 48000 38500 ...\n $ Fuel_Type        : chr [1:1436] \"Diesel\" \"Diesel\" \"Diesel\" \"Diesel\" ...\n $ HP               : num [1:1436] 90 90 90 90 90 90 90 90 192 69 ...\n $ Met_Color        : num [1:1436] 1 1 1 0 0 0 1 1 0 0 ...\n $ Color            : chr [1:1436] \"Blue\" \"Silver\" \"Blue\" \"Black\" ...\n $ Automatic        : num [1:1436] 0 0 0 0 0 0 0 0 0 0 ...\n $ CC               : num [1:1436] 2000 2000 2000 2000 2000 2000 2000 2000 1800 1900 ...\n $ Doors            : num [1:1436] 3 3 3 3 3 3 3 3 3 3 ...\n $ Cylinders        : num [1:1436] 4 4 4 4 4 4 4 4 4 4 ...\n $ Gears            : num [1:1436] 5 5 5 5 5 5 5 5 5 5 ...\n $ Quarterly_Tax    : num [1:1436] 210 210 210 210 210 210 210 210 100 185 ...\n $ Weight           : num [1:1436] 1165 1165 1165 1165 1170 ...\n $ Mfr_Guarantee    : num [1:1436] 0 0 1 1 1 0 0 1 0 0 ...\n $ BOVAG_Guarantee  : num [1:1436] 1 1 1 1 1 1 1 1 1 1 ...\n $ Guarantee_Period : num [1:1436] 3 3 3 3 3 3 3 3 3 3 ...\n $ ABS              : num [1:1436] 1 1 1 1 1 1 1 1 1 1 ...\n $ Airbag_1         : num [1:1436] 1 1 1 1 1 1 1 1 1 1 ...\n $ Airbag_2         : num [1:1436] 1 1 1 1 1 1 1 1 0 1 ...\n $ Airco            : num [1:1436] 0 1 0 0 1 1 1 1 1 1 ...\n $ Automatic_airco  : num [1:1436] 0 0 0 0 0 0 0 0 0 0 ...\n $ Boardcomputer    : num [1:1436] 1 1 1 1 1 1 1 1 0 1 ...\n $ CD_Player        : num [1:1436] 0 1 0 0 0 0 0 1 0 0 ...\n $ Central_Lock     : num [1:1436] 1 1 0 0 1 1 1 1 1 0 ...\n $ Powered_Windows  : num [1:1436] 1 0 0 0 1 1 1 1 1 0 ...\n $ Power_Steering   : num [1:1436] 1 1 1 1 1 1 1 1 1 1 ...\n $ Radio            : num [1:1436] 0 0 0 0 0 0 0 0 1 0 ...\n $ Mistlamps        : num [1:1436] 0 0 0 0 1 1 0 0 0 0 ...\n $ Sport_Model      : num [1:1436] 0 0 0 0 0 0 1 0 0 0 ...\n $ Backseat_Divider : num [1:1436] 1 1 1 1 1 1 1 1 0 1 ...\n $ Metallic_Rim     : num [1:1436] 0 0 0 0 0 0 0 0 1 0 ...\n $ Radio_cassette   : num [1:1436] 0 0 0 0 0 0 0 0 1 0 ...\n $ Parking_Assistant: num [1:1436] 0 0 0 0 0 0 0 0 0 0 ...\n $ Tow_Bar          : num [1:1436] 0 0 0 0 0 0 0 0 0 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Id = col_double(),\n  ..   Model = col_character(),\n  ..   Price = col_double(),\n  ..   Age_08_04 = col_double(),\n  ..   Mfg_Month = col_double(),\n  ..   Mfg_Year = col_double(),\n  ..   KM = col_double(),\n  ..   Fuel_Type = col_character(),\n  ..   HP = col_double(),\n  ..   Met_Color = col_double(),\n  ..   Color = col_character(),\n  ..   Automatic = col_double(),\n  ..   CC = col_double(),\n  ..   Doors = col_double(),\n  ..   Cylinders = col_double(),\n  ..   Gears = col_double(),\n  ..   Quarterly_Tax = col_double(),\n  ..   Weight = col_double(),\n  ..   Mfr_Guarantee = col_double(),\n  ..   BOVAG_Guarantee = col_double(),\n  ..   Guarantee_Period = col_double(),\n  ..   ABS = col_double(),\n  ..   Airbag_1 = col_double(),\n  ..   Airbag_2 = col_double(),\n  ..   Airco = col_double(),\n  ..   Automatic_airco = col_double(),\n  ..   Boardcomputer = col_double(),\n  ..   CD_Player = col_double(),\n  ..   Central_Lock = col_double(),\n  ..   Powered_Windows = col_double(),\n  ..   Power_Steering = col_double(),\n  ..   Radio = col_double(),\n  ..   Mistlamps = col_double(),\n  ..   Sport_Model = col_double(),\n  ..   Backseat_Divider = col_double(),\n  ..   Metallic_Rim = col_double(),\n  ..   Radio_cassette = col_double(),\n  ..   Parking_Assistant = col_double(),\n  ..   Tow_Bar = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\ncolSums(is.na(tc))\n\n               Id             Model             Price         Age_08_04 \n                0                 0                 0                 0 \n        Mfg_Month          Mfg_Year                KM         Fuel_Type \n                0                 0                 0                 0 \n               HP         Met_Color             Color         Automatic \n                0                 0                 0                 0 \n               CC             Doors         Cylinders             Gears \n                0                 0                 0                 0 \n    Quarterly_Tax            Weight     Mfr_Guarantee   BOVAG_Guarantee \n                0                 0                 0                 0 \n Guarantee_Period               ABS          Airbag_1          Airbag_2 \n                0                 0                 0                 0 \n            Airco   Automatic_airco     Boardcomputer         CD_Player \n                0                 0                 0                 0 \n     Central_Lock   Powered_Windows    Power_Steering             Radio \n                0                 0                 0                 0 \n        Mistlamps       Sport_Model  Backseat_Divider      Metallic_Rim \n                0                 0                 0                 0 \n   Radio_cassette Parking_Assistant           Tow_Bar \n                0                 0                 0 \n\n\nWe have 1436 observations across 39 variables. There are no missing values to deal with.\n\n\n3. Analyze whether the Price variable is appropriate for a linear regression model and discuss its distribution.\nAre there any transformations that we might apply to the price variable?\n\n# Price Distribution\n\nhist(tc$Price)\n\n\n\n# Rearrange the columns to place Log_Price immediately after Price\n\ntc &lt;- tc %&gt;%\n  mutate(Log_Price = log(Price)) %&gt;%\n  select(1:3, Log_Price, 4:ncol(tc))\n  \nhist(tc$Log_Price)\n\n\n\n\nPrice is definitely skewed to the right, therefore I used a log transformation. After the transformation, the distribution of price is much closer to being symmetric.\n\n\n4. Is there a relationship between any of the features in the data and the Price feature?\nPerform some exploratory analysis to determine some features that are related using a feature plot.\n\n# Factor categorical variables, get rid of some unnecessary ones.\n  \ntc &lt;- tc %&gt;% \n  select(-Id, -Model, -Mfg_Month, -Cylinders, -Quarterly_Tax) %&gt;%\n  rename(Age = Age_08_04) %&gt;%\n  mutate(CC = if_else(CC == 16000, 2000, CC)) %&gt;%\n  mutate_at(vars(-one_of(\n    c('Price',\n      'Log_Price',\n      'Age',\n      'KM',\n      'HP',\n      'CC',\n      'Weight')\n  )), .funs = factor)\n\nOne Corolla has a 16000 CC engine, and that does not make sense. At the very least, it’s a massive outlier. I replaced this outlier with the value of 2000, which is the maximum otherwise.\n\n# Separate predictors and response\n\ntc_numeric &lt;- tc[, sapply(tc, is.numeric)]\nx &lt;- tc_numeric[, setdiff(names(tc_numeric), \"Log_Price\")]\ny &lt;- tc_numeric$Log_Price\n\n# Use featurePlot\n\ncaret::featurePlot(x, y, plot = \"scatter\")\n\n\n\n\nIt appears there is a negative relationship between Age, KM andLog_Price. There may be a positive relationship between Weight and Log_Price. I speculate that Weight may be a confounding variable with other variables that increase both weight and the price of the car, such as additional safety features.\n\ntc_temp &lt;- tc\n\n# Add a new column to the data that categorizes cars as \"30 years or newer\" or \"older than 30 years\"\ntc_temp$AgeGroup &lt;- ifelse(tc_temp$Age &lt;= 30, \"30 years or newer\", \"older than 30 years\")\n\n# Plot the Weight vs Age\nggplot(tc_temp, aes(x=Age, y=Weight, color=AgeGroup)) + \n  geom_point() +\n  geom_smooth(data=subset(tc, Age &lt;= 30), aes(group=1), method=\"lm\", formula=y~1, se=FALSE, linetype=\"solid\", color=\"blue\") +\n  geom_smooth(data=subset(tc, Age &gt; 30), aes(group=2), method=\"lm\", formula=y~1, se=FALSE, linetype=\"solid\", color=\"orange\") +\n  labs(title=\"Weight vs. Age of Cars\", x=\"Age\", y=\"Weight\") +\n  scale_color_manual(values=c(\"30 years or newer\"=\"blue\", \"older than 30 years\"=\"red\"), name=\"Age Group\") +\n  theme_minimal()\n\n\n\nrm(tc_temp)\n\nIt is very clear based on the Weight vs. Age of Cars graph that newer cars are significantly heavier. Therefore, I am removing the weight variable and leaving age.\n\ntc &lt;- select(tc, -'Weight')\n\n\n# Check categorical variables\n\ncategorical_list &lt;- list()\n\nfactor_cols &lt;- names(tc)[sapply(tc, is.factor)]\n\nfor(i in factor_cols) {\n  categorical_list[[i]] &lt;- table(tc[[i]])\n}\n\n# Convert all factors to dummy vars.\ntc_dum = dummy(tc, int = TRUE)\ntc_num = tc %&gt;%\n  keep(is.numeric)\ntc = bind_cols(tc_num, tc_dum)\nrm(tc_dum, tc_num)\n\n# remove one dummy from each categorical var\ntc &lt;- tc %&gt;%\n  select(-Mfg_Year_1998,\n-Mfg_Year_1999,\n-Mfg_Year_2000,\n-Mfg_Year_2001,\n-Mfg_Year_2002,\n-Mfg_Year_2003,\n-Mfg_Year_2004,\n-Fuel_Type_CNG,\n-Met_Color_0,\n-Color_Beige,\n-Color_Yellow,\n-Color_Violet,\n-Automatic_0,\n-Doors_2,\n-Gears_3,\n-Mfr_Guarantee_0,\n-BOVAG_Guarantee_0,\n-Guarantee_Period_13,\n-Guarantee_Period_18,\n-Guarantee_Period_20,\n-Guarantee_Period_24,\n-Guarantee_Period_28,\n-Guarantee_Period_36,\n-ABS_0,\n-Airbag_1_0,\n-Airbag_2_0,\n-Airco_0,\n-Automatic_airco_0,\n-Boardcomputer_0,\n-CD_Player_0,\n-Central_Lock_0,\n-Powered_Windows_0,\n-Power_Steering_0,\n-Radio_0,\n-Mistlamps_0,\n-Sport_Model_0,\n-Backseat_Divider_0,\n-Metallic_Rim_0,\n-Radio_cassette_0,\n-Parking_Assistant_0,\n-Tow_Bar_0)\n\n\n\n5. Are there any predictor variables in the data that are potentially too strongly related to each other?\nMake sure to use reference any visualizations, tables, or numbers to show this.\n\n# Compute the correlation matrix\ntc_minus_price &lt;- select(tc, -\"Price\", -\"Log_Price\")\ncor_matrix &lt;- cor(tc_minus_price, use = \"pairwise.complete.obs\")\n\n# Compute the correlation matrix\ncor_matrix &lt;- cor(tc_minus_price, use=\"complete.obs\", method=\"pearson\")\n\n# Find pairs with correlation coefficient greater than 0.7 (in absolute value)\nhigh_cor &lt;- which(abs(cor_matrix) &gt; 0.8 & cor_matrix != 1, arr.ind = TRUE)\n\n# Extract pairs and their corresponding correlation values\nresult &lt;- data.frame(\n  Variable1 = rownames(cor_matrix)[high_cor[, 1]],\n  Variable2 = rownames(cor_matrix)[high_cor[, 2]],\n  Correlation = cor_matrix[high_cor]\n)\n\n# remove duplicates\nresult &lt;- result[!duplicated(result$Correlation), ]\nprint(result)\n\n          Variable1        Variable2 Correlation\n1  Fuel_Type_Petrol Fuel_Type_Diesel  -0.9429759\n3           Doors_5          Doors_3  -0.8221205\n5           Gears_6          Gears_5  -0.9657999\n7 Powered_Windows_1   Central_Lock_1   0.8755525\n9  Radio_cassette_1          Radio_1   0.9916210\n\nrm(tc_minus_price)\n\nRadio_cassette_1 and Radio_1 are extremely correlated, having an r value of 0.99, therefore I am dropping Radio_1. Corollas that come with a cassette player almost always have a radio and vice versa. Similarly, we see a strong correlation of r = .88 between Powered_Windows_1 and Central_Lock_1, indicating those features typically come together. So, I am dropping Central_Lock_1 as well.\n\ntc &lt;- select(tc, -'Radio_1', -'Central_Lock_1')\n\n\n\n6. Partition your data into a training set with 70% of the observations and a testing set with the remaining 30%.\n\n# Partition the data.\nset.seed(1001)\nsamp = createDataPartition(tc$Log_Price, p = 0.7, list = FALSE)\ntraining = tc[samp, ]\ntraining &lt;- (select(training, -\"Price\"))\ntesting = tc[-samp, ]\ntesting &lt;- select(testing, -\"Price\")\nrm(samp)\n\n\n\n7. Based on your results and relationships in questions (4) and (5), build a regression tree model to predict car prices. Make sure to conduct cross validation to evaluate the model and choose the best cost complexity parameter for this problem (use default values for minsplit, minbucket, maxdepth, etc. But choose grid of cp values to tune over). Use rpart.plot to view your tree and discuss its complexity, usefulness, etc. What role is pre-pruning and post-pruning playing here?\n\n# Use cross-validation to determine the best cp value\ncontrol &lt;- trainControl(method = \"cv\", number = 10) \ncpGrid &lt;- expand.grid(.cp = seq(0.001, 0.05, by = 0.001))\ntrainResult &lt;- caret::train(Log_Price ~ ., data = training, method = \"rpart\", \n                     trControl = control, tuneGrid = cpGrid)\n\nbestCp &lt;- trainResult$bestTune$cp\nprint(bestCp)\n\n[1] 0.001\n\n# Train the regression tree model using the best cp value\ntreeModel &lt;- rpart(Log_Price ~ ., data = training, control = rpart.control(cp = bestCp))\n\n# Plot the tree\nrpart.plot(treeModel)\n\n\n\n\nThe best cp value to use for the model is .001. The regression tree has 8 levels. Pre-pruning occurs here by choosing a larger value for cp to get a smaller tree.\n\n\n8. Look at the feature importance (using permuted feature importance in “iml” package, with loss = “rmse” and compare = “ratio”) and determine which features have the biggest effect, and which might be okay to remove.\n\ntree_predictor = iml::Predictor$new(treeModel, data = training)\ntree_imp = iml::FeatureImp$new(tree_predictor, loss = \"rmse\", compare = \"ratio\", n.repetitions = 30)\nplot(tree_imp)\n\n\n\n\n\ntree_imp$results\n\n               feature importance.05 importance importance.95 permutation.error\n1                  Age      3.453734   3.541249      3.623929         0.3602261\n2                   KM      1.244454   1.267938      1.292348         0.1289783\n3                   HP      1.217618   1.246837      1.281325         0.1268318\n4      Mfr_Guarantee_1      1.017297   1.027802      1.037196         0.1045510\n5   Backseat_Divider_1      1.014711   1.023065      1.046020         0.1040691\n6    Powered_Windows_1      1.014042   1.023047      1.030123         0.1040673\n7              Airco_1      1.011434   1.020294      1.027566         0.1037872\n8    Automatic_airco_1      1.015513   1.017502      1.020919         0.1035033\n9              Doors_5      1.006945   1.016358      1.027445         0.1033869\n10      Metallic_Rim_1      1.009220   1.014366      1.020588         0.1031842\n11         Mistlamps_1      1.007987   1.011906      1.016627         0.1029340\n12   BOVAG_Guarantee_1      1.003002   1.008347      1.014604         0.1025719\n13  Guarantee_Period_6      1.002825   1.006808      1.010863         0.1024154\n14                  CC      1.000000   1.000000      1.000000         0.1017229\n15    Fuel_Type_Diesel      1.000000   1.000000      1.000000         0.1017229\n16    Fuel_Type_Petrol      1.000000   1.000000      1.000000         0.1017229\n17         Met_Color_1      1.000000   1.000000      1.000000         0.1017229\n18         Color_Black      1.000000   1.000000      1.000000         0.1017229\n19          Color_Blue      1.000000   1.000000      1.000000         0.1017229\n20         Color_Green      1.000000   1.000000      1.000000         0.1017229\n21          Color_Grey      1.000000   1.000000      1.000000         0.1017229\n22           Color_Red      1.000000   1.000000      1.000000         0.1017229\n23        Color_Silver      1.000000   1.000000      1.000000         0.1017229\n24         Color_White      1.000000   1.000000      1.000000         0.1017229\n25         Automatic_1      1.000000   1.000000      1.000000         0.1017229\n26             Doors_3      1.000000   1.000000      1.000000         0.1017229\n27             Doors_4      1.000000   1.000000      1.000000         0.1017229\n28             Gears_4      1.000000   1.000000      1.000000         0.1017229\n29             Gears_5      1.000000   1.000000      1.000000         0.1017229\n30             Gears_6      1.000000   1.000000      1.000000         0.1017229\n31  Guarantee_Period_3      1.000000   1.000000      1.000000         0.1017229\n32 Guarantee_Period_12      1.000000   1.000000      1.000000         0.1017229\n33               ABS_1      1.000000   1.000000      1.000000         0.1017229\n34          Airbag_1_1      1.000000   1.000000      1.000000         0.1017229\n35          Airbag_2_1      1.000000   1.000000      1.000000         0.1017229\n36     Boardcomputer_1      1.000000   1.000000      1.000000         0.1017229\n37         CD_Player_1      1.000000   1.000000      1.000000         0.1017229\n38    Power_Steering_1      1.000000   1.000000      1.000000         0.1017229\n39       Sport_Model_1      1.000000   1.000000      1.000000         0.1017229\n40    Radio_cassette_1      1.000000   1.000000      1.000000         0.1017229\n41 Parking_Assistant_1      1.000000   1.000000      1.000000         0.1017229\n42           Tow_Bar_1      1.000000   1.000000      1.000000         0.1017229\n\n\nAge, KM, and HP are the most important features. Any features with an importance value of 1 would probably be okay to be removed.\n\n\n9. Parsimony is about obtaining the simplest model possible, without oversimplifying. Remove a few of the less useful features and retrain / cross validate / tune your tree.\n\ntraining_simp &lt;- subset(training, select = -c(Tow_Bar_1, Parking_Assistant_1, Metallic_Rim_1, Boardcomputer_1,\n                                              BOVAG_Guarantee_1, Automatic_airco_1)) \ntesting_simp &lt;- subset(testing, select = -c(Tow_Bar_1, Parking_Assistant_1, Metallic_Rim_1, Boardcomputer_1,\n                                              BOVAG_Guarantee_1, Automatic_airco_1)) \n\n# Use cross-validation to determine the best cp value\ncontrol &lt;- trainControl(method = \"cv\", number = 10) \ncpGrid &lt;- expand.grid(.cp = seq(0.001, 0.05, by = 0.001))\ntrainResult &lt;- caret::train(Log_Price ~ ., data = training_simp, method = \"rpart\", \n                     trControl = control, tuneGrid = cpGrid)\n\nbestCp &lt;- trainResult$bestTune$cp\nprint(bestCp)\n\n[1] 0.001\n\n#  Train the regression tree model using the best cp value\ntreeModel &lt;- rpart(Log_Price ~ ., data = training_simp, control = rpart.control(cp = bestCp))\n\n# 5. Plot the tree\nrpart.plot(treeModel)\n\n\n\n\n\n\n10. Use the model resulting from question 9 and test predictions on the testing data. Compare the cross validation error and and testing data. Spend some time interpreting what this prediction error means for your pricing model and its use for CorollaCrowd.\n\n#  Use the trained regression tree model to make predictions on the testing data\npredictions &lt;- predict(treeModel, newdata = testing_simp)\n\n# Calculate the root mean squared error (RMSE) for the testing data\nrmse_test &lt;- sqrt(mean((predictions - testing_simp$Log_Price)^2))\n\n# Obtain the cross-validation error (RMSE) from the training phase\nrmse_cv &lt;- trainResult$results[which.min(trainResult$results$RMSE),]$RMSE\n\n# Print the results\nprint(paste(\"Cross-validation RMSE: \", rmse_cv))\n\n[1] \"Cross-validation RMSE:  0.121312437887304\"\n\nprint(paste(\"Testing RMSE: \", rmse_test))\n\n[1] \"Testing RMSE:  0.125736281299052\"\n\n# Compare the errors\nif (rmse_test &lt; rmse_cv) {\n  print(\"Testing error is lower than cross-validation error.\")\n} else if (rmse_test &gt; rmse_cv) {\n  print(\"Testing error is higher than cross-validation error.\")\n} else {\n  print(\"Testing error and cross-validation error are the same.\")\n}\n\n[1] \"Testing error is higher than cross-validation error.\"\n\n\nThe model performed slightly worse on the testing data, but the difference isn’t substantial. For CorollaCrowd, this model could be used to estimate car value."
  }
]