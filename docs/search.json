[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joe Penders",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nThe page your viewing is the ‚Äúindex‚Äù page, or the landing page of your site. The site is just a quarto document. And you can put anything on it that could be in a quarto document.\nThere are also settings to get a list of site items you may want people to have easy access to. For example, a list of your blog posts, projects, etc.\nThere are different styles of landing pages. In this particular style, the index.qmd page is used as an about page. Instead of immediately showcasing your work, it showcases you! You can introduce yourself and include your contact information right by your image and description.\nYou can then also decide to have some content show up below the about portion as well."
  },
  {
    "objectID": "posts/Demo Post 1/index.html",
    "href": "posts/Demo Post 1/index.html",
    "title": "Demo Post 1",
    "section": "",
    "text": "This is a demo post in which we begin the blog. The idea here is that you create one post with this quarto document. The quarto document for a post will be named ‚Äúindex.qmd‚Äù insides of folder with the name of the post. For example, if I wanted my post to be titled ‚ÄúDemo Post 1‚Äù then I would do the following.\nAfter doing that, you can then edit the index.qmd document for that new post to your heart‚Äôs content. Lets do a little of that now so you can see how this might work."
  },
  {
    "objectID": "posts/Demo Post 1/index.html#including-resources",
    "href": "posts/Demo Post 1/index.html#including-resources",
    "title": "Demo Post 1",
    "section": "Including Resources",
    "text": "Including Resources\nSuppose you wanted to discuss something, like the CRISP-DM process for analytics projects. You might wish to refer to an image of the process and you could include the image in the ‚ÄúDemo Post 1‚Äù folder and reference it here in the document.\n\n\n\n\n\nYou can easily insert the image through the visual editor in Posit / RStudio."
  },
  {
    "objectID": "posts/Demo Post 1/index.html#data-and-output",
    "href": "posts/Demo Post 1/index.html#data-and-output",
    "title": "Demo Post 1",
    "section": "Data and Output",
    "text": "Data and Output\nLets look at some data.\n\nlibrary(tidyverse)\nlibrary(ggthemes)\ndata(\"USArrests\")\n\nUSArrests %&gt;%\n  ggplot(aes(x = Assault, y = Murder)) +\n  geom_point(pch = 21, color = \"coral3\", bg = \"coral\", size=3) +\n  labs(title = \"Arrests for Murder vs. Assault in US States\",\n       x = \"Arrests for assault per 100,000\",\n       y = \"Arrests for murder per 100,000\") +\n  theme_clean()\n\n\n\n\nThis would show us a relationship that we could then spend some paragraphs analyzing and interpreting."
  },
  {
    "objectID": "posts/Demo Post 2/index.html",
    "href": "posts/Demo Post 2/index.html",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.‚Ä¶\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24‚Ä¶\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6‚Ä¶\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2‚Ä¶\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %&gt;%\n  mutate(predicted_murder = predict(dt, USArrests)) %&gt;%\n  select(Murder, predicted_murder) %&gt;%\n  cor() -&gt; corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "posts/Demo Post 2/index.html#understanding-the-data",
    "href": "posts/Demo Post 2/index.html#understanding-the-data",
    "title": "Demo Post 2",
    "section": "",
    "text": "We are looking at arrests data by state. The data set has 50 rows (one for each state) and four variables.\n\nglimpse(USArrests)\n\nRows: 50\nColumns: 4\n$ Murder   &lt;dbl&gt; 13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.‚Ä¶\n$ Assault  &lt;int&gt; 236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 24‚Ä¶\n$ UrbanPop &lt;int&gt; 58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 83, 65, 57, 6‚Ä¶\n$ Rape     &lt;dbl&gt; 21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 2‚Ä¶\n\n\nEach of the variables are a numeric-continuous data type. We have arrests per 100,000 people for three violent crimes: assault, murder, and rape. We also have a column indicating the degree of urban population in that state. Before preceding with prediction, we note that tree-based techniques can be more unstable if the variables are too correlated with one another. We can also see if there are any extreme skews in the data.\n\nlibrary(GGally)\nggpairs(USArrests)\n\n\n\n\nWe do see some positive relationships and stronger correlations, but mayne not quite enough to get us in trouble.\nNow lets try and predict Murder using the other features.\n\ndt = rpart(Murder ~.,\n           data=USArrests)\nrpart.plot(dt)\n\n\n\n\nWe can calculate a kind of R-squared measure of accuracy by squaring the correlation between the actual Murder values with our predicted ones.\n\nUSArrests %&gt;%\n  mutate(predicted_murder = predict(dt, USArrests)) %&gt;%\n  select(Murder, predicted_murder) %&gt;%\n  cor() -&gt; corrmat\n\nrsq = corrmat[[\"Murder\", \"predicted_murder\"]]^2\nprint(paste(\"The r-square for our model is\", round(rsq,2), sep=\": \"))\n\n[1] \"The r-square for our model is: 0.78\""
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects are different than posts. Projects should be more expansive, impressive and generally more professional in nature compared to posts. Posts can be works in progress. Small ideas or things you did that you thought were interesting. Projects should really showcase your professional abilities. You don‚Äôt need to have too many, just make them good. And try to always have one ‚Äúin the works‚Äù so that employers and collaborators can see that you‚Äôre driven.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nTraffic Accidents in the Twin Cities Before and After COVID\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Problem Set 6\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nProblem Set 4\n\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2023\n\n\nJoe Penders\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Problem Set 3/index.html",
    "href": "posts/Problem Set 3/index.html",
    "title": "Problem Set 3",
    "section": "",
    "text": "1. Create a new Rmarkdown document that will show case your data exploration, modeling, and error testing.\n\nrm(list = ls())\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.3     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.0\n‚úî ggplot2   3.4.3     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.2     ‚úî tidyr     1.3.0\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8\n\nlibrary(lubridate)\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(dummy)\n\ndummy 0.1.3\ndummyNews()\n\nlibrary(lattice)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(iml)\nlibrary(patchwork)\ntc = read_csv(\"ToyotaCorolla.csv\")\n\nRows: 1436 Columns: 39\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (3): Model, Fuel_Type, Color\ndbl (36): Id, Price, Age_08_04, Mfg_Month, Mfg_Year, KM, HP, Met_Color, Auto...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n2. Explore the data and determine the number of variables and the quantity of any missing values.\nIf values are missing, prescribe a plan to deal with the problem.\n\nstr(tc)\n\nspc_tbl_ [1,436 √ó 39] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Id               : num [1:1436] 1 2 3 4 5 6 7 8 9 10 ...\n $ Model            : chr [1:1436] \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" \"TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\" ...\n $ Price            : num [1:1436] 13500 13750 13950 14950 13750 ...\n $ Age_08_04        : num [1:1436] 23 23 24 26 30 32 27 30 27 23 ...\n $ Mfg_Month        : num [1:1436] 10 10 9 7 3 1 6 3 6 10 ...\n $ Mfg_Year         : num [1:1436] 2002 2002 2002 2002 2002 ...\n $ KM               : num [1:1436] 46986 72937 41711 48000 38500 ...\n $ Fuel_Type        : chr [1:1436] \"Diesel\" \"Diesel\" \"Diesel\" \"Diesel\" ...\n $ HP               : num [1:1436] 90 90 90 90 90 90 90 90 192 69 ...\n $ Met_Color        : num [1:1436] 1 1 1 0 0 0 1 1 0 0 ...\n $ Color            : chr [1:1436] \"Blue\" \"Silver\" \"Blue\" \"Black\" ...\n $ Automatic        : num [1:1436] 0 0 0 0 0 0 0 0 0 0 ...\n $ CC               : num [1:1436] 2000 2000 2000 2000 2000 2000 2000 2000 1800 1900 ...\n $ Doors            : num [1:1436] 3 3 3 3 3 3 3 3 3 3 ...\n $ Cylinders        : num [1:1436] 4 4 4 4 4 4 4 4 4 4 ...\n $ Gears            : num [1:1436] 5 5 5 5 5 5 5 5 5 5 ...\n $ Quarterly_Tax    : num [1:1436] 210 210 210 210 210 210 210 210 100 185 ...\n $ Weight           : num [1:1436] 1165 1165 1165 1165 1170 ...\n $ Mfr_Guarantee    : num [1:1436] 0 0 1 1 1 0 0 1 0 0 ...\n $ BOVAG_Guarantee  : num [1:1436] 1 1 1 1 1 1 1 1 1 1 ...\n $ Guarantee_Period : num [1:1436] 3 3 3 3 3 3 3 3 3 3 ...\n $ ABS              : num [1:1436] 1 1 1 1 1 1 1 1 1 1 ...\n $ Airbag_1         : num [1:1436] 1 1 1 1 1 1 1 1 1 1 ...\n $ Airbag_2         : num [1:1436] 1 1 1 1 1 1 1 1 0 1 ...\n $ Airco            : num [1:1436] 0 1 0 0 1 1 1 1 1 1 ...\n $ Automatic_airco  : num [1:1436] 0 0 0 0 0 0 0 0 0 0 ...\n $ Boardcomputer    : num [1:1436] 1 1 1 1 1 1 1 1 0 1 ...\n $ CD_Player        : num [1:1436] 0 1 0 0 0 0 0 1 0 0 ...\n $ Central_Lock     : num [1:1436] 1 1 0 0 1 1 1 1 1 0 ...\n $ Powered_Windows  : num [1:1436] 1 0 0 0 1 1 1 1 1 0 ...\n $ Power_Steering   : num [1:1436] 1 1 1 1 1 1 1 1 1 1 ...\n $ Radio            : num [1:1436] 0 0 0 0 0 0 0 0 1 0 ...\n $ Mistlamps        : num [1:1436] 0 0 0 0 1 1 0 0 0 0 ...\n $ Sport_Model      : num [1:1436] 0 0 0 0 0 0 1 0 0 0 ...\n $ Backseat_Divider : num [1:1436] 1 1 1 1 1 1 1 1 0 1 ...\n $ Metallic_Rim     : num [1:1436] 0 0 0 0 0 0 0 0 1 0 ...\n $ Radio_cassette   : num [1:1436] 0 0 0 0 0 0 0 0 1 0 ...\n $ Parking_Assistant: num [1:1436] 0 0 0 0 0 0 0 0 0 0 ...\n $ Tow_Bar          : num [1:1436] 0 0 0 0 0 0 0 0 0 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Id = col_double(),\n  ..   Model = col_character(),\n  ..   Price = col_double(),\n  ..   Age_08_04 = col_double(),\n  ..   Mfg_Month = col_double(),\n  ..   Mfg_Year = col_double(),\n  ..   KM = col_double(),\n  ..   Fuel_Type = col_character(),\n  ..   HP = col_double(),\n  ..   Met_Color = col_double(),\n  ..   Color = col_character(),\n  ..   Automatic = col_double(),\n  ..   CC = col_double(),\n  ..   Doors = col_double(),\n  ..   Cylinders = col_double(),\n  ..   Gears = col_double(),\n  ..   Quarterly_Tax = col_double(),\n  ..   Weight = col_double(),\n  ..   Mfr_Guarantee = col_double(),\n  ..   BOVAG_Guarantee = col_double(),\n  ..   Guarantee_Period = col_double(),\n  ..   ABS = col_double(),\n  ..   Airbag_1 = col_double(),\n  ..   Airbag_2 = col_double(),\n  ..   Airco = col_double(),\n  ..   Automatic_airco = col_double(),\n  ..   Boardcomputer = col_double(),\n  ..   CD_Player = col_double(),\n  ..   Central_Lock = col_double(),\n  ..   Powered_Windows = col_double(),\n  ..   Power_Steering = col_double(),\n  ..   Radio = col_double(),\n  ..   Mistlamps = col_double(),\n  ..   Sport_Model = col_double(),\n  ..   Backseat_Divider = col_double(),\n  ..   Metallic_Rim = col_double(),\n  ..   Radio_cassette = col_double(),\n  ..   Parking_Assistant = col_double(),\n  ..   Tow_Bar = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\ncolSums(is.na(tc))\n\n               Id             Model             Price         Age_08_04 \n                0                 0                 0                 0 \n        Mfg_Month          Mfg_Year                KM         Fuel_Type \n                0                 0                 0                 0 \n               HP         Met_Color             Color         Automatic \n                0                 0                 0                 0 \n               CC             Doors         Cylinders             Gears \n                0                 0                 0                 0 \n    Quarterly_Tax            Weight     Mfr_Guarantee   BOVAG_Guarantee \n                0                 0                 0                 0 \n Guarantee_Period               ABS          Airbag_1          Airbag_2 \n                0                 0                 0                 0 \n            Airco   Automatic_airco     Boardcomputer         CD_Player \n                0                 0                 0                 0 \n     Central_Lock   Powered_Windows    Power_Steering             Radio \n                0                 0                 0                 0 \n        Mistlamps       Sport_Model  Backseat_Divider      Metallic_Rim \n                0                 0                 0                 0 \n   Radio_cassette Parking_Assistant           Tow_Bar \n                0                 0                 0 \n\n\nWe have 1436 observations across 39 variables. There are no missing values to deal with.\n\n\n3. Analyze whether the Price variable is appropriate for a linear regression model and discuss its distribution.\nAre there any transformations that we might apply to the price variable?\n\n# Price Distribution\n\nhist(tc$Price)\n\n\n\n# Rearrange the columns to place Log_Price immediately after Price\n\ntc &lt;- tc %&gt;%\n  mutate(Log_Price = log(Price)) %&gt;%\n  select(1:3, Log_Price, 4:ncol(tc))\n  \nhist(tc$Log_Price)\n\n\n\n\nPrice is definitely skewed to the right, therefore I used a log transformation. After the transformation, the distribution of price is much closer to being symmetric.\n\n\n4. Is there a relationship between any of the features in the data and the Price feature?\nPerform some exploratory analysis to determine some features that are related using a feature plot.\n\n# Factor categorical variables, get rid of some unnecessary ones.\n  \ntc &lt;- tc %&gt;% \n  select(-Id, -Model, -Mfg_Month, -Cylinders, -Quarterly_Tax) %&gt;%\n  rename(Age = Age_08_04) %&gt;%\n  mutate(CC = if_else(CC == 16000, 2000, CC)) %&gt;%\n  mutate_at(vars(-one_of(\n    c('Price',\n      'Log_Price',\n      'Age',\n      'KM',\n      'HP',\n      'CC',\n      'Weight')\n  )), .funs = factor)\n\nOne Corolla has a 16000 CC engine, and that does not make sense. At the very least, it‚Äôs a massive outlier. I replaced this outlier with the value of 2000, which is the maximum otherwise.\n\n# Separate predictors and response\n\ntc_numeric &lt;- tc[, sapply(tc, is.numeric)]\nx &lt;- tc_numeric[, setdiff(names(tc_numeric), \"Log_Price\")]\ny &lt;- tc_numeric$Log_Price\n\n# Use featurePlot\n\ncaret::featurePlot(x, y, plot = \"scatter\")\n\n\n\n\nIt appears there is a negative relationship between Age, KM andLog_Price. There may be a positive relationship between Weight and Log_Price. I speculate that Weight may be a confounding variable with other variables that increase both weight and the price of the car, such as additional safety features.\n\ntc_temp &lt;- tc\n\n# Add a new column to the data that categorizes cars as \"30 years or newer\" or \"older than 30 years\"\ntc_temp$AgeGroup &lt;- ifelse(tc_temp$Age &lt;= 30, \"30 years or newer\", \"older than 30 years\")\n\n# Plot the Weight vs Age\nggplot(tc_temp, aes(x=Age, y=Weight, color=AgeGroup)) + \n  geom_point() +\n  geom_smooth(data=subset(tc, Age &lt;= 30), aes(group=1), method=\"lm\", formula=y~1, se=FALSE, linetype=\"solid\", color=\"blue\") +\n  geom_smooth(data=subset(tc, Age &gt; 30), aes(group=2), method=\"lm\", formula=y~1, se=FALSE, linetype=\"solid\", color=\"orange\") +\n  labs(title=\"Weight vs. Age of Cars\", x=\"Age\", y=\"Weight\") +\n  scale_color_manual(values=c(\"30 years or newer\"=\"blue\", \"older than 30 years\"=\"red\"), name=\"Age Group\") +\n  theme_minimal()\n\n\n\nrm(tc_temp)\n\nIt is very clear based on the Weight vs.¬†Age of Cars graph that newer cars are significantly heavier. Therefore, I am removing the weight variable and leaving age.\n\ntc &lt;- select(tc, -'Weight')\n\n\n# Check categorical variables\n\ncategorical_list &lt;- list()\n\nfactor_cols &lt;- names(tc)[sapply(tc, is.factor)]\n\nfor(i in factor_cols) {\n  categorical_list[[i]] &lt;- table(tc[[i]])\n}\n\n# Convert all factors to dummy vars.\ntc_dum = dummy(tc, int = TRUE)\ntc_num = tc %&gt;%\n  keep(is.numeric)\ntc = bind_cols(tc_num, tc_dum)\nrm(tc_dum, tc_num)\n\n# remove one dummy from each categorical var\ntc &lt;- tc %&gt;%\n  select(-Mfg_Year_1998,\n-Mfg_Year_1999,\n-Mfg_Year_2000,\n-Mfg_Year_2001,\n-Mfg_Year_2002,\n-Mfg_Year_2003,\n-Mfg_Year_2004,\n-Fuel_Type_CNG,\n-Met_Color_0,\n-Color_Beige,\n-Color_Yellow,\n-Color_Violet,\n-Automatic_0,\n-Doors_2,\n-Gears_3,\n-Mfr_Guarantee_0,\n-BOVAG_Guarantee_0,\n-Guarantee_Period_13,\n-Guarantee_Period_18,\n-Guarantee_Period_20,\n-Guarantee_Period_24,\n-Guarantee_Period_28,\n-Guarantee_Period_36,\n-ABS_0,\n-Airbag_1_0,\n-Airbag_2_0,\n-Airco_0,\n-Automatic_airco_0,\n-Boardcomputer_0,\n-CD_Player_0,\n-Central_Lock_0,\n-Powered_Windows_0,\n-Power_Steering_0,\n-Radio_0,\n-Mistlamps_0,\n-Sport_Model_0,\n-Backseat_Divider_0,\n-Metallic_Rim_0,\n-Radio_cassette_0,\n-Parking_Assistant_0,\n-Tow_Bar_0)\n\n\n\n5. Are there any predictor variables in the data that are potentially too strongly related to each other?\nMake sure to use reference any visualizations, tables, or numbers to show this.\n\n# Compute the correlation matrix\ntc_minus_price &lt;- select(tc, -\"Price\", -\"Log_Price\")\ncor_matrix &lt;- cor(tc_minus_price, use = \"pairwise.complete.obs\")\n\n# Compute the correlation matrix\ncor_matrix &lt;- cor(tc_minus_price, use=\"complete.obs\", method=\"pearson\")\n\n# Find pairs with correlation coefficient greater than 0.7 (in absolute value)\nhigh_cor &lt;- which(abs(cor_matrix) &gt; 0.8 & cor_matrix != 1, arr.ind = TRUE)\n\n# Extract pairs and their corresponding correlation values\nresult &lt;- data.frame(\n  Variable1 = rownames(cor_matrix)[high_cor[, 1]],\n  Variable2 = rownames(cor_matrix)[high_cor[, 2]],\n  Correlation = cor_matrix[high_cor]\n)\n\n# remove duplicates\nresult &lt;- result[!duplicated(result$Correlation), ]\nprint(result)\n\n          Variable1        Variable2 Correlation\n1  Fuel_Type_Petrol Fuel_Type_Diesel  -0.9429759\n3           Doors_5          Doors_3  -0.8221205\n5           Gears_6          Gears_5  -0.9657999\n7 Powered_Windows_1   Central_Lock_1   0.8755525\n9  Radio_cassette_1          Radio_1   0.9916210\n\nrm(tc_minus_price)\n\nRadio_cassette_1 and Radio_1 are extremely correlated, having an r value of 0.99, therefore I am dropping Radio_1. Corollas that come with a cassette player almost always have a radio and vice versa. Similarly, we see a strong correlation of r = .88 between Powered_Windows_1 and Central_Lock_1, indicating those features typically come together. So, I am dropping Central_Lock_1 as well.\n\ntc &lt;- select(tc, -'Radio_1', -'Central_Lock_1')\n\n\n\n6. Partition your data into a training set with 70% of the observations and a testing set with the remaining 30%.\n\n# Partition the data.\nset.seed(1001)\nsamp = createDataPartition(tc$Log_Price, p = 0.7, list = FALSE)\ntraining = tc[samp, ]\ntraining &lt;- (select(training, -\"Price\"))\ntesting = tc[-samp, ]\ntesting &lt;- select(testing, -\"Price\")\nrm(samp)\n\n\n\n7. Based on your results and relationships in questions (4) and (5), build a regression tree model to predict car prices. Make sure to conduct cross validation to evaluate the model and choose the best cost complexity parameter for this problem (use default values for minsplit, minbucket, maxdepth, etc. But choose grid of cp values to tune over). Use rpart.plot to view your tree and discuss its complexity, usefulness, etc. What role is pre-pruning and post-pruning playing here?\n\n# Use cross-validation to determine the best cp value\ncontrol &lt;- trainControl(method = \"cv\", number = 10) \ncpGrid &lt;- expand.grid(.cp = seq(0.001, 0.05, by = 0.001))\ntrainResult &lt;- caret::train(Log_Price ~ ., data = training, method = \"rpart\", \n                     trControl = control, tuneGrid = cpGrid)\n\nbestCp &lt;- trainResult$bestTune$cp\nprint(bestCp)\n\n[1] 0.001\n\n# Train the regression tree model using the best cp value\ntreeModel &lt;- rpart(Log_Price ~ ., data = training, control = rpart.control(cp = bestCp))\n\n# Plot the tree\nrpart.plot(treeModel)\n\n\n\n\nThe best cp value to use for the model is .001. The regression tree has 8 levels. Pre-pruning occurs here by choosing a larger value for cp to get a smaller tree.\n\n\n8. Look at the feature importance (using permuted feature importance in ‚Äúiml‚Äù package, with loss = ‚Äúrmse‚Äù and compare = ‚Äúratio‚Äù) and determine which features have the biggest effect, and which might be okay to remove.\n\ntree_predictor = iml::Predictor$new(treeModel, data = training)\ntree_imp = iml::FeatureImp$new(tree_predictor, loss = \"rmse\", compare = \"ratio\", n.repetitions = 30)\nplot(tree_imp)\n\n\n\n\n\ntree_imp$results\n\n               feature importance.05 importance importance.95 permutation.error\n1                  Age      3.453734   3.541249      3.623929         0.3602261\n2                   KM      1.244454   1.267938      1.292348         0.1289783\n3                   HP      1.217618   1.246837      1.281325         0.1268318\n4      Mfr_Guarantee_1      1.017297   1.027802      1.037196         0.1045510\n5   Backseat_Divider_1      1.014711   1.023065      1.046020         0.1040691\n6    Powered_Windows_1      1.014042   1.023047      1.030123         0.1040673\n7              Airco_1      1.011434   1.020294      1.027566         0.1037872\n8    Automatic_airco_1      1.015513   1.017502      1.020919         0.1035033\n9              Doors_5      1.006945   1.016358      1.027445         0.1033869\n10      Metallic_Rim_1      1.009220   1.014366      1.020588         0.1031842\n11         Mistlamps_1      1.007987   1.011906      1.016627         0.1029340\n12   BOVAG_Guarantee_1      1.003002   1.008347      1.014604         0.1025719\n13  Guarantee_Period_6      1.002825   1.006808      1.010863         0.1024154\n14                  CC      1.000000   1.000000      1.000000         0.1017229\n15    Fuel_Type_Diesel      1.000000   1.000000      1.000000         0.1017229\n16    Fuel_Type_Petrol      1.000000   1.000000      1.000000         0.1017229\n17         Met_Color_1      1.000000   1.000000      1.000000         0.1017229\n18         Color_Black      1.000000   1.000000      1.000000         0.1017229\n19          Color_Blue      1.000000   1.000000      1.000000         0.1017229\n20         Color_Green      1.000000   1.000000      1.000000         0.1017229\n21          Color_Grey      1.000000   1.000000      1.000000         0.1017229\n22           Color_Red      1.000000   1.000000      1.000000         0.1017229\n23        Color_Silver      1.000000   1.000000      1.000000         0.1017229\n24         Color_White      1.000000   1.000000      1.000000         0.1017229\n25         Automatic_1      1.000000   1.000000      1.000000         0.1017229\n26             Doors_3      1.000000   1.000000      1.000000         0.1017229\n27             Doors_4      1.000000   1.000000      1.000000         0.1017229\n28             Gears_4      1.000000   1.000000      1.000000         0.1017229\n29             Gears_5      1.000000   1.000000      1.000000         0.1017229\n30             Gears_6      1.000000   1.000000      1.000000         0.1017229\n31  Guarantee_Period_3      1.000000   1.000000      1.000000         0.1017229\n32 Guarantee_Period_12      1.000000   1.000000      1.000000         0.1017229\n33               ABS_1      1.000000   1.000000      1.000000         0.1017229\n34          Airbag_1_1      1.000000   1.000000      1.000000         0.1017229\n35          Airbag_2_1      1.000000   1.000000      1.000000         0.1017229\n36     Boardcomputer_1      1.000000   1.000000      1.000000         0.1017229\n37         CD_Player_1      1.000000   1.000000      1.000000         0.1017229\n38    Power_Steering_1      1.000000   1.000000      1.000000         0.1017229\n39       Sport_Model_1      1.000000   1.000000      1.000000         0.1017229\n40    Radio_cassette_1      1.000000   1.000000      1.000000         0.1017229\n41 Parking_Assistant_1      1.000000   1.000000      1.000000         0.1017229\n42           Tow_Bar_1      1.000000   1.000000      1.000000         0.1017229\n\n\nAge, KM, and HP are the most important features. Any features with an importance value of 1 would probably be okay to be removed.\n\n\n9. Parsimony is about obtaining the simplest model possible, without oversimplifying. Remove a few of the less useful features and retrain / cross validate / tune your tree.\n\ntraining_simp &lt;- subset(training, select = -c(Tow_Bar_1, Parking_Assistant_1, Metallic_Rim_1, Boardcomputer_1,\n                                              BOVAG_Guarantee_1, Automatic_airco_1)) \ntesting_simp &lt;- subset(testing, select = -c(Tow_Bar_1, Parking_Assistant_1, Metallic_Rim_1, Boardcomputer_1,\n                                              BOVAG_Guarantee_1, Automatic_airco_1)) \n\n# Use cross-validation to determine the best cp value\ncontrol &lt;- trainControl(method = \"cv\", number = 10) \ncpGrid &lt;- expand.grid(.cp = seq(0.001, 0.05, by = 0.001))\ntrainResult &lt;- caret::train(Log_Price ~ ., data = training_simp, method = \"rpart\", \n                     trControl = control, tuneGrid = cpGrid)\n\nbestCp &lt;- trainResult$bestTune$cp\nprint(bestCp)\n\n[1] 0.001\n\n#  Train the regression tree model using the best cp value\ntreeModel &lt;- rpart(Log_Price ~ ., data = training_simp, control = rpart.control(cp = bestCp))\n\n# 5. Plot the tree\nrpart.plot(treeModel)\n\n\n\n\n\n\n10. Use the model resulting from question 9 and test predictions on the testing data. Compare the cross validation error and and testing data. Spend some time interpreting what this prediction error means for your pricing model and its use for CorollaCrowd.\n\n#  Use the trained regression tree model to make predictions on the testing data\npredictions &lt;- predict(treeModel, newdata = testing_simp)\n\n# Calculate the root mean squared error (RMSE) for the testing data\nrmse_test &lt;- sqrt(mean((predictions - testing_simp$Log_Price)^2))\n\n# Obtain the cross-validation error (RMSE) from the training phase\nrmse_cv &lt;- trainResult$results[which.min(trainResult$results$RMSE),]$RMSE\n\n# Print the results\nprint(paste(\"Cross-validation RMSE: \", rmse_cv))\n\n[1] \"Cross-validation RMSE:  0.121312437887304\"\n\nprint(paste(\"Testing RMSE: \", rmse_test))\n\n[1] \"Testing RMSE:  0.125736281299052\"\n\n# Compare the errors\nif (rmse_test &lt; rmse_cv) {\n  print(\"Testing error is lower than cross-validation error.\")\n} else if (rmse_test &gt; rmse_cv) {\n  print(\"Testing error is higher than cross-validation error.\")\n} else {\n  print(\"Testing error and cross-validation error are the same.\")\n}\n\n[1] \"Testing error is higher than cross-validation error.\"\n\n\nThe model performed slightly worse on the testing data, but the difference isn‚Äôt substantial. For CorollaCrowd, this model could be used to estimate car value."
  },
  {
    "objectID": "posts/Problem Set 4/index.html",
    "href": "posts/Problem Set 4/index.html",
    "title": "Problem Set 4",
    "section": "",
    "text": "The objective of the NVO is to predict a binary outcome: whether a person will respond to a mailing or not. Classification is the right approach to accomplish this task. By utilizing classification, the organization can take advantage of historical data to identify patterns or characteristics that are indicative of a positive response. This will enable the NVO to target individuals who are more likely to respond, increasing the overall response rate."
  },
  {
    "objectID": "posts/Problem Set 4/index.html#section",
    "href": "posts/Problem Set 4/index.html#section",
    "title": "Problem Set 4",
    "section": "",
    "text": "The objective of the NVO is to predict a binary outcome: whether a person will respond to a mailing or not. Classification is the right approach to accomplish this task. By utilizing classification, the organization can take advantage of historical data to identify patterns or characteristics that are indicative of a positive response. This will enable the NVO to target individuals who are more likely to respond, increasing the overall response rate."
  },
  {
    "objectID": "posts/Problem Set 4/index.html#section-1",
    "href": "posts/Problem Set 4/index.html#section-1",
    "title": "Problem Set 4",
    "section": "2.",
    "text": "2.\nOnce a classifier is built, the National Veterans Organization (NVO) can use it to predict the likelihood of potential donors responding to their mailings. Figuring out which variables contribute to a person being more likely to respond enables the organization to optimize their outreach efforts. If the classifier is accurate, this approach can lead to a higher response rate and a more efficient allocation of resources compared to blanket mailings. Additionally, by relying on data-driven predictions, the NVO can minimize biases or assumptions that might have influenced their previous outreach strategies. Having a more systematic and objective method to identify potential donors could lead to the NVO collecting more donations while also reducing the cost of outreach."
  },
  {
    "objectID": "posts/Problem Set 4/index.html#section-2",
    "href": "posts/Problem Set 4/index.html#section-2",
    "title": "Problem Set 4",
    "section": "3.",
    "text": "3.\nTo evaluate the classifiers performance I will look at several measures from the confusion matrix: precision, sensitivity and the F-measure. The precision of the classifier will tell me the proportion of the positive response predictions that are actually true. Sensitivity will indicate how well the classifier can detect positive responses overall. The F-measure is number derived from precision and sensitivity. By focusing on these metrics, NVO can ensure that they‚Äôre both maximizing the number of donation opportunities and ensuring that their outreach efforts are effective.\n\n# Import the data, and view\ndonors &lt;- read_csv(\"donors.csv\")\nstr(donors)\n\nspc_tbl_ [95,412 √ó 22] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ age                    : num [1:95412] 60 46 NA 70 78 NA 38 NA NA 65 ...\n $ numberChildren         : num [1:95412] NA 1 NA NA 1 NA 1 NA NA NA ...\n $ incomeRating           : num [1:95412] NA 6 3 1 3 NA 4 2 3 NA ...\n $ wealthRating           : num [1:95412] NA 9 1 4 2 NA 6 9 2 NA ...\n $ mailOrderPurchases     : num [1:95412] 0 16 2 2 60 0 0 1 0 0 ...\n $ totalGivingAmount      : num [1:95412] 240 47 202 109 254 51 107 31 199 28 ...\n $ numberGifts            : num [1:95412] 31 3 27 16 37 4 14 5 11 3 ...\n $ smallestGiftAmount     : num [1:95412] 5 10 2 2 3 10 3 5 10 3 ...\n $ largestGiftAmount      : num [1:95412] 12 25 16 11 15 16 12 11 22 15 ...\n $ averageGiftAmount      : num [1:95412] 7.74 15.67 7.48 6.81 6.86 ...\n $ yearsSinceFirstDonation: num [1:95412] 8 3 7 10 11 3 10 3 9 3 ...\n $ monthsSinceLastDonation: num [1:95412] 14 14 14 14 13 20 22 18 19 22 ...\n $ inHouseDonor           : logi [1:95412] FALSE FALSE FALSE FALSE TRUE FALSE ...\n $ plannedGivingDonor     : logi [1:95412] FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ sweepstakesDonor       : logi [1:95412] FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ P3Donor                : logi [1:95412] FALSE FALSE FALSE FALSE TRUE FALSE ...\n $ state                  : chr [1:95412] \"IL\" \"CA\" \"NC\" \"CA\" ...\n $ urbanicity             : chr [1:95412] \"town\" \"suburb\" \"rural\" \"rural\" ...\n $ socioEconomicStatus    : chr [1:95412] \"average\" \"highest\" \"average\" \"average\" ...\n $ isHomeowner            : logi [1:95412] NA TRUE NA NA TRUE NA ...\n $ gender                 : chr [1:95412] \"female\" \"male\" \"male\" \"female\" ...\n $ respondedMailing       : logi [1:95412] FALSE FALSE FALSE FALSE FALSE FALSE ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   age = col_double(),\n  ..   numberChildren = col_double(),\n  ..   incomeRating = col_double(),\n  ..   wealthRating = col_double(),\n  ..   mailOrderPurchases = col_double(),\n  ..   totalGivingAmount = col_double(),\n  ..   numberGifts = col_double(),\n  ..   smallestGiftAmount = col_double(),\n  ..   largestGiftAmount = col_double(),\n  ..   averageGiftAmount = col_double(),\n  ..   yearsSinceFirstDonation = col_double(),\n  ..   monthsSinceLastDonation = col_double(),\n  ..   inHouseDonor = col_logical(),\n  ..   plannedGivingDonor = col_logical(),\n  ..   sweepstakesDonor = col_logical(),\n  ..   P3Donor = col_logical(),\n  ..   state = col_character(),\n  ..   urbanicity = col_character(),\n  ..   socioEconomicStatus = col_character(),\n  ..   isHomeowner = col_logical(),\n  ..   gender = col_character(),\n  ..   respondedMailing = col_logical()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(donors)\n\n      age        numberChildren   incomeRating    wealthRating  \n Min.   : 1.00   Min.   :1.00    Min.   :1.000   Min.   :0.00   \n 1st Qu.:48.00   1st Qu.:1.00    1st Qu.:2.000   1st Qu.:3.00   \n Median :62.00   Median :1.00    Median :4.000   Median :6.00   \n Mean   :61.61   Mean   :1.53    Mean   :3.886   Mean   :5.35   \n 3rd Qu.:75.00   3rd Qu.:2.00    3rd Qu.:5.000   3rd Qu.:8.00   \n Max.   :98.00   Max.   :7.00    Max.   :7.000   Max.   :9.00   \n NA's   :23665   NA's   :83026   NA's   :21286   NA's   :44732  \n mailOrderPurchases totalGivingAmount  numberGifts      smallestGiftAmount\n Min.   :  0.000    Min.   :  13.0    Min.   :  1.000   Min.   :   0.000  \n 1st Qu.:  0.000    1st Qu.:  40.0    1st Qu.:  3.000   1st Qu.:   3.000  \n Median :  0.000    Median :  78.0    Median :  7.000   Median :   5.000  \n Mean   :  3.321    Mean   : 104.5    Mean   :  9.602   Mean   :   7.934  \n 3rd Qu.:  3.000    3rd Qu.: 131.0    3rd Qu.: 13.000   3rd Qu.:  10.000  \n Max.   :241.000    Max.   :9485.0    Max.   :237.000   Max.   :1000.000  \n                                                                          \n largestGiftAmount averageGiftAmount  yearsSinceFirstDonation\n Min.   :   5      Min.   :   1.286   Min.   : 0.000         \n 1st Qu.:  14      1st Qu.:   8.385   1st Qu.: 2.000         \n Median :  17      Median :  11.636   Median : 5.000         \n Mean   :  20      Mean   :  13.348   Mean   : 5.596         \n 3rd Qu.:  23      3rd Qu.:  15.478   3rd Qu.: 9.000         \n Max.   :5000      Max.   :1000.000   Max.   :13.000         \n                                                             \n monthsSinceLastDonation inHouseDonor    plannedGivingDonor sweepstakesDonor\n Min.   : 0.00           Mode :logical   Mode :logical      Mode :logical   \n 1st Qu.:12.00           FALSE:88709     FALSE:95298        FALSE:93795     \n Median :14.00           TRUE :6703      TRUE :114          TRUE :1617      \n Mean   :14.36                                                              \n 3rd Qu.:17.00                                                              \n Max.   :23.00                                                              \n                                                                            \n  P3Donor           state            urbanicity        socioEconomicStatus\n Mode :logical   Length:95412       Length:95412       Length:95412       \n FALSE:93395     Class :character   Class :character   Class :character   \n TRUE :2017      Mode  :character   Mode  :character   Mode  :character   \n                                                                          \n                                                                          \n                                                                          \n                                                                          \n isHomeowner       gender          respondedMailing\n Mode:logical   Length:95412       Mode :logical   \n TRUE:52354     Class :character   FALSE:90569     \n NA's:43058     Mode  :character   TRUE :4843      \n                                                   \n                                                   \n                                                   \n                                                   \n\n# Compute the number and percentage of NAs for each column\nna_summary &lt;- donors %&gt;%\n  summarise_all(~sum(is.na(.))) %&gt;%\n  gather(column, na_count) %&gt;%\n  mutate(na_percentage = (na_count / nrow(donors)) * 100) %&gt;%\n  filter(na_count &gt; 1)\nprint(na_summary)\n\n# A tibble: 8 √ó 3\n  column              na_count na_percentage\n  &lt;chr&gt;                  &lt;int&gt;         &lt;dbl&gt;\n1 age                    23665         24.8 \n2 numberChildren         83026         87.0 \n3 incomeRating           21286         22.3 \n4 wealthRating           44732         46.9 \n5 urbanicity              2316          2.43\n6 socioEconomicStatus     2316          2.43\n7 isHomeowner            43058         45.1 \n8 gender                  4676          4.90\n\n\n\n# Drop number of children\ndonors &lt;- donors %&gt;% select(-numberChildren) %&gt;%\n  na.omit(donors)\n\n87% of the data in numberChildren is missing, so I dropped it due to it not being useful as a predictor. Since we have such a large amount of data I have decided to drop every record which has missing values. There is a lot of missing data, but even after omitting every incomplete record we still have 33230 observations to work with.\n\n# Change categorical types to factor variables\ndonors &lt;- donors %&gt;%\n  mutate_at(vars(incomeRating, wealthRating, socioEconomicStatus, \n                 gender, state, urbanicity, respondedMailing), .funs=factor)\n\n# Remove rows where respondedMailing or any numeric column is NA\ndonors_num_cleaned &lt;- donors %&gt;%\n  filter(!is.na(respondedMailing)) %&gt;%\n  select(age, mailOrderPurchases, \n         totalGivingAmount, numberGifts, smallestGiftAmount, largestGiftAmount, \n         averageGiftAmount, yearsSinceFirstDonation, monthsSinceLastDonation, \n         respondedMailing) %&gt;%\n              na.omit()\n\n\n## Histogram Plots\n\n# Set a transparent theme for better visualization\ntransparentTheme(trans = 0.9)\n\n# Create Histogram plots\nfeaturePlot(x = donors_num_cleaned %&gt;% select(-respondedMailing),\n            y = donors_num_cleaned$respondedMailing,\n            plot = \"density\",\n            scale = list(x = list(relation='free'),\n                         y = list(relation='free')),\n            adjust = 1.5,\n            pch = \"|\",\n            layout = c(3,4),\n            auto.key = list(columns = 2))\n\n\n\n\n\n# Boxplots\ncaret::featurePlot(x = donors_num_cleaned %&gt;% select(-respondedMailing),\n                   y = donors_num_cleaned$respondedMailing,\n                   plot = \"box\",\n                   scales = list(y = list(relation='free'),\n                                 x = list(rot=90)),\n                   layout = c(3,4),\n                   auto.key = list(columns = 2))\n\n\n\n\n\n# Association between wealth and income ratings\ntable(donors$incomeRating, donors$wealthRating)\n\n   \n       0    1    2    3    4    5    6    7    8    9\n  1  282  335  310  259  241  258  228  211  150   84\n  2  338  558  616  574  597  548  552  441  316  134\n  3  158  252  343  363  387  420  459  453  384  238\n  4  208  364  461  498  654  806  872  974  931  699\n  5  175  284  409  546  711  914 1019 1228 1406 1205\n  6   46   77  117  138  206  313  481  568  867 1389\n  7   49   58   80   95  138  183  277  526  866 1903\n\nvcd::assocstats(table(donors$incomeRating, donors$wealthRating))\n\n                    X^2 df P(&gt; X^2)\nLikelihood Ratio 7960.4 54        0\nPearson          8155.4 54        0\n\nPhi-Coefficient   : NA \nContingency Coeff.: 0.444 \nCramer's V        : 0.202 \n\n\nI suspected there would be a relationship between incomeRating and wealthRating. A Cramer‚Äôs V value of 0.202 indicates that there is a weak to moderate relationship between these two variables. I am going to leave them both in.\n\n# Look for correlation between numeric variables\n\ndonors %&gt;%\n  keep(is.numeric) %&gt;%\n  cor(use = \"pairwise.complete.obs\") %&gt;%\n  corrplot::corrplot()\n\n\n\n\nWe see positive correlation between averageGiftAmount with both largestGiftAmount and smallestGiftAmount. There‚Äôs also positive correlation between yearsSinceFirstDonation and numberGifts. These are all medium strength correlations, so for now I will leave them in."
  },
  {
    "objectID": "posts/Problem Set 4/index.html#build-a-logistic-lasso-model-using-cross-validation-on-the-training-data-to-select-the-best-ùû¥.-view-the-coefficients-at-that-chosen-ùû¥-and-see-what-features-are-in-the-model.",
    "href": "posts/Problem Set 4/index.html#build-a-logistic-lasso-model-using-cross-validation-on-the-training-data-to-select-the-best-ùû¥.-view-the-coefficients-at-that-chosen-ùû¥-and-see-what-features-are-in-the-model.",
    "title": "Problem Set 4",
    "section": "4. Build a logistic LASSO model using cross-validation on the training data to select the best ùû¥. View the coefficients at that chosen ùû¥ and see what features are in the model.",
    "text": "4. Build a logistic LASSO model using cross-validation on the training data to select the best ùû¥. View the coefficients at that chosen ùû¥ and see what features are in the model.\n\n# Creating dummy variables from categorical variables\ndonors &lt;- dummy_cols(donors, select_columns = c(\"incomeRating\", \"wealthRating\", \"socioEconomicStatus\",\n                                                \"gender\", \"state\", \"urbanicity\"),\n                     remove_selected_columns = TRUE) %&gt;%\n  select(-incomeRating_1, -wealthRating_0, -socioEconomicStatus_average, -gender_joint, -gender_male,\n         -state_AA, -state_AA, -urbanicity_rural)\n\n# Scaling numerical variables\nnumerical_vars &lt;- c('age', 'mailOrderPurchases', 'totalGivingAmount', \n                     'numberGifts', 'smallestGiftAmount', 'largestGiftAmount', 'averageGiftAmount', \n                     'yearsSinceFirstDonation', 'monthsSinceLastDonation')\n\ndonors[numerical_vars] &lt;- scale(donors[numerical_vars])\n\n# Partition the data.\nset.seed(1001)\nsamp = createDataPartition(donors$respondedMailing, p = 0.7, list = FALSE)\ntraining = donors[samp, ]\ntesting = donors[-samp, ]\nrm(samp)\n\n#check for class imbalance\ntraining %&gt;%\n  select(respondedMailing) %&gt;%\n  table() %&gt;%\n  prop.table()\n\nrespondedMailing\n     FALSE       TRUE \n0.94712406 0.05287594 \n\n\nThere is a significant class imbalance, I will use smote to correct it.\n\n# Smote \n\nsmote_train = smote(respondedMailing ~ .,\n                    data = training)\n\ntable(smote_train$respondedMailing)\n\n\nFALSE  TRUE \n 4920  3690 \n\n\n\n# Separate predictors and response\ny &lt;- as.vector(smote_train$respondedMailing)\nX &lt;- as.matrix(smote_train %&gt;% select(-respondedMailing))\n\n# Use cross-validation to find the best lambda\ncv.lasso &lt;- cv.glmnet(X, y, family=\"binomial\", alpha=1)\n\n# Extract best lambda\nbest_lambda &lt;- cv.lasso$lambda.1se\n\n# Fit the model using the best lambda\nLASSO_model &lt;- glmnet(X, y, family=\"binomial\", alpha=1, lambda=best_lambda, maxit = 1e6)\n\n# View the coefficients\ncoef(LASSO_model)\n\n89 x 1 sparse Matrix of class \"dgCMatrix\"\n                                      s0\n(Intercept)                 -0.598546198\nage                          0.045580706\nmailOrderPurchases           .          \ntotalGivingAmount            .          \nnumberGifts                  .          \nsmallestGiftAmount           .          \nlargestGiftAmount           -0.230728654\naverageGiftAmount           -0.117186971\nyearsSinceFirstDonation      .          \nmonthsSinceLastDonation     -0.092631409\ninHouseDonor                -0.012863467\nplannedGivingDonor           .          \nsweepstakesDonor             .          \nP3Donor                      .          \nisHomeowner                  .          \nincomeRating_2               .          \nincomeRating_3               .          \nincomeRating_4              -0.279123095\nincomeRating_5              -0.251042345\nincomeRating_6               0.361244872\nincomeRating_7              -0.135812460\nwealthRating_1               .          \nwealthRating_2               .          \nwealthRating_3               0.242357201\nwealthRating_4              -0.133213966\nwealthRating_5              -0.148504342\nwealthRating_6               0.095310582\nwealthRating_7               0.097658191\nwealthRating_8               0.319708802\nwealthRating_9              -0.219063254\nsocioEconomicStatus_highest  .          \nsocioEconomicStatus_lowest  -0.423174836\ngender_female               -0.445589926\nstate_AE                     0.575545901\nstate_AK                     .          \nstate_AL                     1.841101729\nstate_AP                     .          \nstate_AR                     .          \nstate_AZ                     .          \nstate_CA                     0.321249192\nstate_CO                     .          \nstate_CT                     .          \nstate_DE                     .          \nstate_FL                     .          \nstate_GA                     1.221292620\nstate_HI                     .          \nstate_IA                     .          \nstate_ID                     0.388287462\nstate_IL                     .          \nstate_IN                    -0.003340735\nstate_KS                     .          \nstate_KY                     .          \nstate_LA                     .          \nstate_MA                     .          \nstate_MD                     .          \nstate_ME                     .          \nstate_MI                     .          \nstate_MN                     1.583330439\nstate_MO                     .          \nstate_MS                    -0.228310896\nstate_MT                     .          \nstate_NC                     .          \nstate_ND                     .          \nstate_NE                    -0.140898223\nstate_NH                     .          \nstate_NJ                     .          \nstate_NM                     .          \nstate_NV                     .          \nstate_NY                     .          \nstate_OH                     .          \nstate_OK                     2.168223127\nstate_OR                     .          \nstate_PA                     .          \nstate_RI                     .          \nstate_SC                     0.038535568\nstate_SD                     0.892871747\nstate_TN                     .          \nstate_TX                     .          \nstate_UT                     .          \nstate_VA                     .          \nstate_VT                     .          \nstate_WA                     .          \nstate_WI                    -0.256755139\nstate_WV                     0.403782677\nstate_WY                     0.863087203\nurbanicity_city             -0.086388277\nurbanicity_suburb            0.421327308\nurbanicity_town              .          \nurbanicity_urban             .          \n\n\nAt the chosen ùû¥ = .0091, the features in our model are: age, largestGiftAmount, averageGiftAmount, monthsSinceLastDonation, inHouseDonor, incomeRating_4, incomeRating_5, incomeRating_6, incomeRating_7, wealthRating_3, wealthRating_4, wealthRating_5, wealthRating_6, wealthRating_7, wealthRating_8, wealthRating_9, socioEconomicStatus_lowest, gender_female, state_AE, state_AL, state_CA, state_GA, state_ID, state_IN, state_MN, state_MS, state_NE, state_OK, state_SC, state_SD, state_WI, state_WV, state_WY, urbanicity_city, urbanicity_suburb.\n\n# Build a decision tree model. Crossvalidate and tune over values of cp.\nset.seed(1001)\nctrl = caret::trainControl(method = \"repeatedcv\", number = 5, repeats = 30)\ntree_model = caret::train(respondedMailing ~ ., \n             data = smote_train, \n             method = \"rpart\",\n             metric = \"Kappa\",\n             trControl = ctrl,\n             tuneGrid = expand.grid(cp = seq(0.0, 0.1, 0.005)))\n\nbestCp &lt;- tree_model$bestTune$cp\n\n# Plot the cp\nplot(tree_model)\n\n\n\n\n\n# Train the regression tree model using the best cp value\ntree_model &lt;- rpart(respondedMailing ~ ., data = smote_train, control = rpart.control(cp = bestCp))\n\nrpart.plot::rpart.plot(tree_model)\n\n\n\n\nThe decision tree uses the features: gender_female, urbanicity_suburb, state_MN, state_OK, state_CA, state_GA, state_AL, wealthRating_7.\n\ntest_predictors = as.matrix(testing %&gt;% select(-respondedMailing))\n\n# Performance of LASSO\nLASSO_test_class = predict(LASSO_model, newx = test_predictors, s = best_lambda, type=\"class\")\nLASSO_test_prob = predict(LASSO_model, newx = test_predictors, s = best_lambda, type=\"response\")[,1]\n\n# Performance of tree\ntree_test_class = predict(tree_model, newdata = as.data.frame(test_predictors), type=\"class\")\ntree_test_prob = predict(tree_model, newdata = as.data.frame(test_predictors), type=\"prob\")[,1]\n\n\n# Confusion Matrices\nresponse_vector &lt;- as.vector(testing$respondedMailing)\n\nLASSO_cm = confusionMatrix(factor(LASSO_test_class), factor(response_vector), positive = \"TRUE\")\ntree_cm = confusionMatrix(factor(tree_test_class), factor(response_vector), positive = \"TRUE\")\n\n\nLASSO_cm\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction FALSE TRUE\n     FALSE  7985  437\n     TRUE   1456   90\n                                          \n               Accuracy : 0.8101          \n                 95% CI : (0.8023, 0.8178)\n    No Information Rate : 0.9471          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.0087          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.170778        \n            Specificity : 0.845779        \n         Pos Pred Value : 0.058215        \n         Neg Pred Value : 0.948112        \n             Prevalence : 0.052869        \n         Detection Rate : 0.009029        \n   Detection Prevalence : 0.155096        \n      Balanced Accuracy : 0.508279        \n                                          \n       'Positive' Class : TRUE            \n                                          \n\n\n\ntree_cm\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction FALSE TRUE\n     FALSE  9376  520\n     TRUE     65    7\n                                          \n               Accuracy : 0.9413          \n                 95% CI : (0.9365, 0.9458)\n    No Information Rate : 0.9471          \n    P-Value [Acc &gt; NIR] : 0.9951          \n                                          \n                  Kappa : 0.0108          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.0132827       \n            Specificity : 0.9931151       \n         Pos Pred Value : 0.0972222       \n         Neg Pred Value : 0.9474535       \n             Prevalence : 0.0528692       \n         Detection Rate : 0.0007022       \n   Detection Prevalence : 0.0072231       \n      Balanced Accuracy : 0.5031989       \n                                          \n       'Positive' Class : TRUE            \n                                          \n\n\n\ntree_roc = roc(testing$respondedMailing ~ tree_test_prob,\n                plot=TRUE, print.auc=TRUE, print.auc.y=0.3,\n                col = \"black\", lwd=3, legacy.axes=TRUE)"
  },
  {
    "objectID": "PS7.html",
    "href": "PS7.html",
    "title": "Problem Set 7",
    "section": "",
    "text": "The LHBA depends on the revenue generated from the sale of homes. In order to make a profit, they need to be able to make good predictions for the prices the homes will sell for. There is some debate about whether or not to trust the county-appraised values, because before the housing bubble burst houses were selling for prices that were radically different than these estimates. It will be important to figure out if since the bubble burst, home prices have fallen closer in line to the county-appraised values, or if there are still factors affecting prices that the government doesn‚Äôt take into account. It may be possible to build a model which does a much better job at predicting the prices homes will sell for."
  },
  {
    "objectID": "PS7.html#lhbas-big-picture-business-problem",
    "href": "PS7.html#lhbas-big-picture-business-problem",
    "title": "Problem Set 7",
    "section": "",
    "text": "The LHBA depends on the revenue generated from the sale of homes. In order to make a profit, they need to be able to make good predictions for the prices the homes will sell for. There is some debate about whether or not to trust the county-appraised values, because before the housing bubble burst houses were selling for prices that were radically different than these estimates. It will be important to figure out if since the bubble burst, home prices have fallen closer in line to the county-appraised values, or if there are still factors affecting prices that the government doesn‚Äôt take into account. It may be possible to build a model which does a much better job at predicting the prices homes will sell for."
  },
  {
    "objectID": "PS7.html#questions-for-lhba-shareholders",
    "href": "PS7.html#questions-for-lhba-shareholders",
    "title": "Problem Set 7",
    "section": "2. Questions for LHBA Shareholders",
    "text": "2. Questions for LHBA Shareholders\n\nWhere is there disagreement on how to price homes?\nIn which locations does LHBA conduct most of its business?\nDoes the company primarily sell single-family homes, or are there other types of properties?"
  },
  {
    "objectID": "PS7.html#the-analytics-problem",
    "href": "PS7.html#the-analytics-problem",
    "title": "Problem Set 7",
    "section": "3. The Analytics problem",
    "text": "3. The Analytics problem\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyverse)\n\n\npre &lt;- read_csv(\"posts/Problem Set 7/PreCrisisCV.csv\")\npost &lt;- read_csv(\"posts/Problem Set 7/PostCrisisCV.csv\")\ntest &lt;- read_csv(\"posts/Problem Set 7/OnMarketTest-1.csv\")"
  },
  {
    "objectID": "posts/Problem Set 6/Problem Set 6.html",
    "href": "posts/Problem Set 6/Problem Set 6.html",
    "title": "Problem Set 6",
    "section": "",
    "text": "rm(list = ls())\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(performanceEstimation)\nlibrary(PRROC)\nlibrary(rpart)\n# Read in data and remove unneccessary features\nbank = read_csv(\"UniversalBank.csv\") %&gt;%\n  select(-ID, -`ZIP Code`, -Experience) %&gt;%\n  rename(Loan = `Personal Loan`,\n         Securities = `Securities Account`,\n         CD = `CD Account`) %&gt;%\n  mutate_at(vars(Loan, Education), .fun = factor)\n# MISSING DATA\n# =============\n# Calculate percent of missing values for features\nmissing_df =  as.numeric(purrr::map(bank, ~mean(is.na(.))))*100\n# Assign values to data frame for easy viewing\ndf = data.frame(PercentMissing = missing_df,\n                row.names = names(bank)) %&gt;%\n  arrange(desc(PercentMissing))\n\nprint(df)\n\n           PercentMissing\nAge                     0\nIncome                  0\nFamily                  0\nCCAvg                   0\nEducation               0\nMortgage                0\nLoan                    0\nSecurities              0\nCD                      0\nOnline                  0\nCreditCard              0\nFortunately, we don‚Äôt have any missing values."
  },
  {
    "objectID": "posts/Problem Set 6/Problem Set 6.html#partition-data",
    "href": "posts/Problem Set 6/Problem Set 6.html#partition-data",
    "title": "Problem Set 6",
    "section": "Partition Data",
    "text": "Partition Data\n\n# Partition the Data\nset.seed(453)\nsamp = createDataPartition(bank$Loan, p = 0.7, list = FALSE)\ntrain = bank[samp, ]\ntest = bank[-samp, ]\nrm(samp)\n\n\nAddress Class Imbalance\n\n# Address class imbalance\ntable(train$Loan)\n\n\n   0    1 \n3164  336 \n\n\n\nbalanced_train = smote(Loan ~ .,\n              data = train,\n              perc.over = 6,\n              perc.under = 1.5)\ntable(balanced_train$Loan)\n\n\n   0    1 \n3024 2352"
  },
  {
    "objectID": "posts/Problem Set 6/Problem Set 6.html#best-tuned-decision-tree",
    "href": "posts/Problem Set 6/Problem Set 6.html#best-tuned-decision-tree",
    "title": "Problem Set 6",
    "section": "Best Tuned Decision Tree",
    "text": "Best Tuned Decision Tree\n\n# training and evaluation\nctrl = caret::trainControl(method = \"repeatedcv\", number = 7, repeats = 15)\nset.seed(890)\ntree = caret::train(Loan ~ .,\n             data = balanced_train,\n             method = \"rpart\",\n             metric = \"Kappa\",\n             trControl = ctrl,\n             tuneGrid = expand.grid(cp = seq(0.0, 0.1, 0.001)),\n             control = rpart.control(minsplit = 1, minbucket = 1, maxdepth = 8)\n             )\n\n\nrpart.plot::rpart.plot(tree$finalModel)"
  },
  {
    "objectID": "posts/Problem Set 6/Problem Set 6.html#best-tuned-random-forest",
    "href": "posts/Problem Set 6/Problem Set 6.html#best-tuned-random-forest",
    "title": "Problem Set 6",
    "section": "Best Tuned Random Forest",
    "text": "Best Tuned Random Forest\n\n# set.seed(285)\n# forest = caret::train(Loan ~ .,\n#                       data = balanced_train,\n#                       method = \"rf\",\n#                       metric = \"Kappa\",\n#                       trControl = ctrl,\n#                       ntree = 500,\n#                       tuneGrid = expand.grid(.mtry = seq(2,8,1))\n#                       )\n# saveRDS(forest, \"forest.rds\")\nforest = readRDS(\"forest.rds\")\nplot(forest)"
  },
  {
    "objectID": "posts/Problem Set 6/Problem Set 6.html#best-tuned-boosting-adaboost-gradient-boosting",
    "href": "posts/Problem Set 6/Problem Set 6.html#best-tuned-boosting-adaboost-gradient-boosting",
    "title": "Problem Set 6",
    "section": "Best Tuned Boosting (Adaboost / Gradient Boosting)",
    "text": "Best Tuned Boosting (Adaboost / Gradient Boosting)\n\n# boost_grid = expand.grid(\n#   maxdepth = c(2, 3, 4, 5, 6, 7, 8),\n#   iter = c(100, 150, 200, 250, 300),\n#   nu = 0.1\n# )\n# \n# boost_ctrl = caret::trainControl(method = \"cv\",\n#                           number = 10,\n#                           allowParallel = TRUE)\n# \n# set.seed(623)\n# boosted_trees = caret::train(Loan ~ .,\n#                       data = balanced_train,\n#                       trControl = boost_ctrl,\n#                       tuneGrid = boost_grid,\n#                       method = \"ada\",\n#                       metric = \"Kappa\")\n\n# saveRDS(boosted_trees, \"boosted_trees.rds\")\nboosted_trees = readRDS(\"boosted_trees.rds\")\n\n\nplot(boosted_trees)"
  },
  {
    "objectID": "posts/Problem Set 6/Problem Set 6.html#compare-precision-and-sensitivity",
    "href": "posts/Problem Set 6/Problem Set 6.html#compare-precision-and-sensitivity",
    "title": "Problem Set 6",
    "section": "Compare Precision and Sensitivity",
    "text": "Compare Precision and Sensitivity\n\n# Convert Y in test data to numeric 0, 1.\ntest = mutate(test, Loan = as.numeric(ifelse(Loan==\"1\", 1, 0)))\n\n# Create explainers\ntree_explain = DALEX::explain(tree,\n                              data = test,\n                              y = test$Loan,\n                              type = \"classification\",\n                              label = \"Decision Tree\")\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  Decision Tree \n  -&gt; data              :  1500  rows  11  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  1500  values \n  -&gt; predict function  :  yhat.train  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package caret , ver. 6.0.94 , task classification (  default  ) \n  -&gt; model_info        :  type set to  classification \n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.1038884 , max =  1  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -1 , mean =  -0.007888389 , max =  1  \n  A new explainer has been created!  \n\nforest_explain = DALEX::explain(forest,\n                                data = test,\n                                y = test$Loan,\n                                label = \"Random Forest\")\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  Random Forest \n  -&gt; data              :  1500  rows  11  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  1500  values \n  -&gt; predict function  :  yhat.train  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package caret , ver. 6.0.94 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.1142973 , max =  1  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.838 , mean =  -0.01829733 , max =  0.9  \n  A new explainer has been created!  \n\nadaboost_explain = DALEX::explain(boosted_trees,\n                                  data = test,\n                                  y = test$Loan,\n                                  label = \"AdaBoost\")\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  AdaBoost \n  -&gt; data              :  1500  rows  11  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  1500  values \n  -&gt; predict function  :  yhat.train  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package caret , ver. 6.0.94 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  9.799935e-16 , mean =  0.09698423 , max =  1  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9999999 , mean =  -0.0009842281 , max =  1  \n  A new explainer has been created!"
  },
  {
    "objectID": "posts/Problem Set 6/Problem Set 6.html#roc-plot-and-comparing-auc",
    "href": "posts/Problem Set 6/Problem Set 6.html#roc-plot-and-comparing-auc",
    "title": "Problem Set 6",
    "section": "ROC Plot and Comparing AUC",
    "text": "ROC Plot and Comparing AUC\n\n# Model Performance\ntree_perf = DALEX::model_performance(tree_explain)\nforest_perf = DALEX::model_performance(forest_explain)\nadaboost_perf = DALEX::model_performance(adaboost_explain)\n\n# Plot the Precision Recall Curve\nplot(tree_perf, forest_perf, adaboost_perf, geom = 'prc')\n\n\n\n\nRandom Forest performed the best, with both high precision and high recall. Adaboost did slightly worse, but overall still performed well. The decision tree was clearly the worst, as the graph shows a significant drop in precision and recall compared to the other models.\n\n# Plot the ROC\nplot(tree_perf, forest_perf, adaboost_perf, geom = 'roc')\n\n\n\n\n\n# Compare the AUCs\nmatrix(c(\"Model\",\n         \"Decision Tree\",\n         \"Random Forest\",\n         \"Adaboost\",\n         \"AUC\",\n         round(tree_perf$measures$auc, 5),\n         round(forest_perf$measures$auc, 5),\n         round(adaboost_perf$measures$auc, 5)),\n       ncol = 2\n         )\n\n     [,1]            [,2]     \n[1,] \"Model\"         \"AUC\"    \n[2,] \"Decision Tree\" \"0.9367\" \n[3,] \"Random Forest\" \"0.99701\"\n[4,] \"Adaboost\"      \"0.99665\"\n\n\nAgain, it appears Random Forest performed the best as it had the largest AUC. However, for practical purposes it is essentially the same as Adaboost. Decision Tree was also clearly the worse, again."
  },
  {
    "objectID": "posts/Problem Set 6/Problem Set 6.html#importance-of-partitioning",
    "href": "posts/Problem Set 6/Problem Set 6.html#importance-of-partitioning",
    "title": "Problem Set 6",
    "section": "Importance of partitioning",
    "text": "Importance of partitioning\nPartitioning the data means randomly splitting our customer records into two sets: a training set and a testing set. This happens randomly, so there shouldn‚Äôt be significant differences between the features of the customers in one set versus the other. Splitting the records up randomly is important because, for example, if we trained our model solely on low-income customers we may not get good predictions for high-income customers, and vice versa. Once the was partitioned, we could create our models to predict whether or not customers will accept a loan.\nThe models were trained solely on the training dataset. During training, the models would ‚Äúpractice‚Äù predicting outcomes for customers in the training dataset, and that is how they would learn. However, these practice tests couldn‚Äôt necessarily tell us how the model would perform on customer records that weren‚Äôt used in training. In the real world our models will be used on new customer data, and that is why we must evaluate our models on data it hasn‚Äôt seen before. That is why it is so important we don‚Äôt feed all of our data into training the model: we need a separate testing set which is left out of training to get an idea how it will actually perform on new customers."
  },
  {
    "objectID": "posts/Problem Set 6/Problem Set 6.html#value-of-bagging-and-ensemble-models",
    "href": "posts/Problem Set 6/Problem Set 6.html#value-of-bagging-and-ensemble-models",
    "title": "Problem Set 6",
    "section": "Value of bagging and ensemble models",
    "text": "Value of bagging and ensemble models\nAs we‚Äôve seen, regular decision trees can be outperformed by more sophisticated models. Bagging, which is short for ‚Äúbootstrap aggregation‚Äù allows us to train many models at once by creating random subsets of the training data to work with. When a record from the training set is randomly chosen to be part of a new training subset, it can be resampled and become part of other training subsets. By resampling records we can create many subsets of the training data, and try to make predictions using each of them. Once we have models trained on these subsets, we aggregate their output and see which outcome had more ‚Äúvotes.‚Äù Basically, instead of one prediction, i.e.¬†accepting or rejecting the loan, we have a whole list of predictions and see which category had more votes.\nEnsemble methods such as random forest and adaboost combine different types of models together, whereas bagging relies on many instances of the same model. The advantage of ensemble methods is that we can take advantage of the strengths of different models. The result is better predictions, which we can see in our own implementation: the ensemble methods are clearly superior."
  },
  {
    "objectID": "posts/Problem Set 6/index.html",
    "href": "posts/Problem Set 6/index.html",
    "title": "Problem Set 6",
    "section": "",
    "text": "rm(list = ls())\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(performanceEstimation)\nlibrary(PRROC)\nlibrary(rpart)\n# Read in data and remove unneccessary features\nbank = read_csv(\"UniversalBank.csv\") %&gt;%\n  select(-ID, -`ZIP Code`, -Experience) %&gt;%\n  rename(Loan = `Personal Loan`,\n         Securities = `Securities Account`,\n         CD = `CD Account`) %&gt;%\n  mutate_at(vars(Loan, Education), .fun = factor)\n# MISSING DATA\n# =============\n# Calculate percent of missing values for features\nmissing_df =  as.numeric(purrr::map(bank, ~mean(is.na(.))))*100\n# Assign values to data frame for easy viewing\ndf = data.frame(PercentMissing = missing_df,\n                row.names = names(bank)) %&gt;%\n  arrange(desc(PercentMissing))\n\nprint(df)\n\n           PercentMissing\nAge                     0\nIncome                  0\nFamily                  0\nCCAvg                   0\nEducation               0\nMortgage                0\nLoan                    0\nSecurities              0\nCD                      0\nOnline                  0\nCreditCard              0\nFortunately, we don‚Äôt have any missing values."
  },
  {
    "objectID": "posts/Problem Set 6/index.html#partition-data",
    "href": "posts/Problem Set 6/index.html#partition-data",
    "title": "Problem Set 6",
    "section": "Partition Data",
    "text": "Partition Data\n\n# Partition the Data\nset.seed(453)\nsamp = createDataPartition(bank$Loan, p = 0.7, list = FALSE)\ntrain = bank[samp, ]\ntest = bank[-samp, ]\nrm(samp)\n\n\nAddress Class Imbalance\n\n# Address class imbalance\ntable(train$Loan)\n\n\n   0    1 \n3164  336 \n\n\n\nbalanced_train = smote(Loan ~ .,\n              data = train,\n              perc.over = 6,\n              perc.under = 1.5)\ntable(balanced_train$Loan)\n\n\n   0    1 \n3024 2352"
  },
  {
    "objectID": "posts/Problem Set 6/index.html#best-tuned-decision-tree",
    "href": "posts/Problem Set 6/index.html#best-tuned-decision-tree",
    "title": "Problem Set 6",
    "section": "Best Tuned Decision Tree",
    "text": "Best Tuned Decision Tree\n\n# training and evaluation\nctrl = caret::trainControl(method = \"repeatedcv\", number = 7, repeats = 15)\nset.seed(890)\ntree = caret::train(Loan ~ .,\n             data = balanced_train,\n             method = \"rpart\",\n             metric = \"Kappa\",\n             trControl = ctrl,\n             tuneGrid = expand.grid(cp = seq(0.0, 0.1, 0.001)),\n             control = rpart.control(minsplit = 1, minbucket = 1, maxdepth = 8)\n             )\n\n\nrpart.plot::rpart.plot(tree$finalModel)"
  },
  {
    "objectID": "posts/Problem Set 6/index.html#best-tuned-random-forest",
    "href": "posts/Problem Set 6/index.html#best-tuned-random-forest",
    "title": "Problem Set 6",
    "section": "Best Tuned Random Forest",
    "text": "Best Tuned Random Forest\n\n# set.seed(285)\n# forest = caret::train(Loan ~ .,\n#                       data = balanced_train,\n#                       method = \"rf\",\n#                       metric = \"Kappa\",\n#                       trControl = ctrl,\n#                       ntree = 500,\n#                       tuneGrid = expand.grid(.mtry = seq(2,8,1))\n#                       )\n# saveRDS(forest, \"forest.rds\")\nforest = readRDS(\"forest.rds\")\nplot(forest)"
  },
  {
    "objectID": "posts/Problem Set 6/index.html#best-tuned-boosting-adaboost-gradient-boosting",
    "href": "posts/Problem Set 6/index.html#best-tuned-boosting-adaboost-gradient-boosting",
    "title": "Problem Set 6",
    "section": "Best Tuned Boosting (Adaboost / Gradient Boosting)",
    "text": "Best Tuned Boosting (Adaboost / Gradient Boosting)\n\n# boost_grid = expand.grid(\n#   maxdepth = c(2, 3, 4, 5, 6, 7, 8),\n#   iter = c(100, 150, 200, 250, 300),\n#   nu = 0.1\n# )\n# \n# boost_ctrl = caret::trainControl(method = \"cv\",\n#                           number = 10,\n#                           allowParallel = TRUE)\n# \n# set.seed(623)\n# boosted_trees = caret::train(Loan ~ .,\n#                       data = balanced_train,\n#                       trControl = boost_ctrl,\n#                       tuneGrid = boost_grid,\n#                       method = \"ada\",\n#                       metric = \"Kappa\")\n\n# saveRDS(boosted_trees, \"boosted_trees.rds\")\nboosted_trees = readRDS(\"boosted_trees.rds\")\n\n\nplot(boosted_trees)"
  },
  {
    "objectID": "posts/Problem Set 6/index.html#compare-precision-and-sensitivity",
    "href": "posts/Problem Set 6/index.html#compare-precision-and-sensitivity",
    "title": "Problem Set 6",
    "section": "Compare Precision and Sensitivity",
    "text": "Compare Precision and Sensitivity\n\n# Convert Y in test data to numeric 0, 1.\ntest = mutate(test, Loan = as.numeric(ifelse(Loan==\"1\", 1, 0)))\n\n# Create explainers\ntree_explain = DALEX::explain(tree,\n                              data = test,\n                              y = test$Loan,\n                              type = \"classification\",\n                              label = \"Decision Tree\")\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  Decision Tree \n  -&gt; data              :  1500  rows  11  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  1500  values \n  -&gt; predict function  :  yhat.train  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package caret , ver. 6.0.94 , task classification (  default  ) \n  -&gt; model_info        :  type set to  classification \n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.1038884 , max =  1  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -1 , mean =  -0.007888389 , max =  1  \n  A new explainer has been created!  \n\nforest_explain = DALEX::explain(forest,\n                                data = test,\n                                y = test$Loan,\n                                label = \"Random Forest\")\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  Random Forest \n  -&gt; data              :  1500  rows  11  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  1500  values \n  -&gt; predict function  :  yhat.train  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package caret , ver. 6.0.94 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.1142973 , max =  1  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.838 , mean =  -0.01829733 , max =  0.9  \n  A new explainer has been created!  \n\nadaboost_explain = DALEX::explain(boosted_trees,\n                                  data = test,\n                                  y = test$Loan,\n                                  label = \"AdaBoost\")\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  AdaBoost \n  -&gt; data              :  1500  rows  11  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  1500  values \n  -&gt; predict function  :  yhat.train  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package caret , ver. 6.0.94 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  9.799935e-16 , mean =  0.09698423 , max =  1  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9999999 , mean =  -0.0009842281 , max =  1  \n  A new explainer has been created!"
  },
  {
    "objectID": "posts/Problem Set 6/index.html#roc-plot-and-comparing-auc",
    "href": "posts/Problem Set 6/index.html#roc-plot-and-comparing-auc",
    "title": "Problem Set 6",
    "section": "ROC Plot and Comparing AUC",
    "text": "ROC Plot and Comparing AUC\n\n# Model Performance\ntree_perf = DALEX::model_performance(tree_explain)\nforest_perf = DALEX::model_performance(forest_explain)\nadaboost_perf = DALEX::model_performance(adaboost_explain)\n\n# Plot the Precision Recall Curve\nplot(tree_perf, forest_perf, adaboost_perf, geom = 'prc')\n\n\n\n\nRandom Forest performed the best, with both high precision and high recall. Adaboost did slightly worse, but overall still performed well. The decision tree was clearly the worst, as the graph shows a significant drop in precision and recall compared to the other models.\n\n# Plot the ROC\nplot(tree_perf, forest_perf, adaboost_perf, geom = 'roc')\n\n\n\n\n\n# Compare the AUCs\nmatrix(c(\"Model\",\n         \"Decision Tree\",\n         \"Random Forest\",\n         \"Adaboost\",\n         \"AUC\",\n         round(tree_perf$measures$auc, 5),\n         round(forest_perf$measures$auc, 5),\n         round(adaboost_perf$measures$auc, 5)),\n       ncol = 2\n         )\n\n     [,1]            [,2]     \n[1,] \"Model\"         \"AUC\"    \n[2,] \"Decision Tree\" \"0.9367\" \n[3,] \"Random Forest\" \"0.99701\"\n[4,] \"Adaboost\"      \"0.99665\"\n\n\nAgain, it appears Random Forest performed the best as it had the largest AUC. However, for practical purposes it is essentially the same as Adaboost. Decision Tree was also clearly the worse, again."
  },
  {
    "objectID": "posts/Problem Set 6/index.html#importance-of-partitioning",
    "href": "posts/Problem Set 6/index.html#importance-of-partitioning",
    "title": "Problem Set 6",
    "section": "Importance of partitioning",
    "text": "Importance of partitioning\nPartitioning the data means randomly splitting our customer records into two sets: a training set and a testing set. This happens randomly, so there shouldn‚Äôt be significant differences between the features of the customers in one set versus the other. Splitting the records up randomly is important because, for example, if we trained our model solely on low-income customers we may not get good predictions for high-income customers, and vice versa. Once the was partitioned, we could create our models to predict whether or not customers will accept a loan.\nThe models were trained solely on the training dataset. During training, the models would ‚Äúpractice‚Äù predicting outcomes for customers in the training dataset, and that is how they would learn. However, these practice tests couldn‚Äôt necessarily tell us how the model would perform on customer records that weren‚Äôt used in training. In the real world our models will be used on new customer data, and that is why we must evaluate our models on data it hasn‚Äôt seen before. That is why it is so important we don‚Äôt feed all of our data into training the model: we need a separate testing set which is left out of training to get an idea how it will actually perform on new customers."
  },
  {
    "objectID": "posts/Problem Set 6/index.html#value-of-bagging-and-ensemble-models",
    "href": "posts/Problem Set 6/index.html#value-of-bagging-and-ensemble-models",
    "title": "Problem Set 6",
    "section": "Value of bagging and ensemble models",
    "text": "Value of bagging and ensemble models\nAs we‚Äôve seen, regular decision trees can be outperformed by more sophisticated models. Bagging, which is short for ‚Äúbootstrap aggregation‚Äù allows us to train many models at once by creating random subsets of the training data to work with. When a record from the training set is randomly chosen to be part of a new training subset, it can be resampled and become part of other training subsets. By resampling records we can create many subsets of the training data, and try to make predictions using each of them. Once we have models trained on these subsets, we aggregate their output and see which outcome had more ‚Äúvotes.‚Äù Basically, instead of one prediction, i.e.¬†accepting or rejecting the loan, we have a whole list of predictions and see which category had more votes.\nEnsemble methods such as random forest and adaboost combine different types of models together, whereas bagging relies on many instances of the same model. The advantage of ensemble methods is that we can take advantage of the strengths of different models. The result is better predictions, which we can see in our own implementation: the ensemble methods are clearly superior."
  },
  {
    "objectID": "posts/DM Project/ProjScript.html",
    "href": "posts/DM Project/ProjScript.html",
    "title": "Traffic Accidents in the Twin Cities Before and After COVID",
    "section": "",
    "text": "This dataset contains records of traffic accidents for the seven-county region overseen by the metropolitan council of the Twin Cities.\nTraffic Accident data source: Sobhan Moosavi. (2023). US Accidents (2016 - 2023) [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DS/199387\nMinnesota shape files came from https://gisdata.mn.gov/organization/us-mn-state-dot\nThis dataset contains records of traffic accidents for the seven-county region overseen by the metropolitan council of the Twin Cities. Minnesota shape files came from https://gisdata.met.gov/dataset/trans-roads-metdot-tis\nThe variables are:\nID: This is a unique identifier of the accident record.\nSource: Source of raw accident data\nSeverity: Shows the severity of the accident, a number between 1 and 4, where 1 indicates the least impact on traffic (i.e., short delay as a result of the accident) and 4 indicates a significant impact on traffic (i.e., long delay).\nStart_Time: Shows start time of the accident in local time zone.\nEnd_Time: Shows end time of the accident in local time zone. End time here refers to when the impact of accident on traffic flow was dismissed.\nStart_Lat: Shows latitude in GPS coordinate of the start point.\nStart_Lng: Shows longitude in GPS coordinate of the start point.\nEnd_Lat: Shows latitude in GPS coordinate of the end point.\nEnd_Lng: Shows longitude in GPS coordinate of the end point.\nDistance(mi): The length of the road extent affected by the accident in miles.\nDescription: Shows a human provided description of the accident.\nStreet: Shows the street name in address field.\nCity: Shows the city in address field.\nCounty: Shows the county in address field.\nState: Shows the state in address field.\nZipcode: Shows the zipcode in address field.\nCountry: Shows the country in address field.\nTimezone: Shows timezone based on the location of the accident (eastern, central, etc.).\nAirport_Code: Denotes an airport-based weather station which is the closest one to location of the accident.\nWeather_Timestamp:Shows the time-stamp of weather observation record (in local time).\nTemperature(F): Shows the temperature (in Fahrenheit).\nWind_Chill(F): Shows the wind chill (in Fahrenheit).\nHumidity(%): Shows the humidity (in percentage).\nPressure(in): Shows the air pressure (in inches).\nVisibility(mi): Shows visibility (in miles).\nWind_Direction: Shows wind direction.\nWind_Speed(mph): Shows wind speed (in miles per hour).\nPrecipitation(in): Shows precipitation amount in inches, if there is any.\nWeather_Condition: Shows the weather condition (rain, snow, thunderstorm, fog, etc.)\nAmenity: A POI annotation which indicates presence of amenity in a nearby location.\nBump: A POI annotation which indicates presence of speed bump or hump in a nearby location.\nCrossing: A POI annotation which indicates presence of crossing in a nearby location.\nGive_Way: A POI annotation which indicates presence of give_way in a nearby location.\nJunction: A POI annotation which indicates presence of junction in a nearby location.\nNo_Exit: A POI annotation which indicates presence of no_exit in a nearby location.\nRailway: A POI annotation which indicates presence of railway in a nearby location.\nRoundabout: A POI annotation which indicates presence of roundabout in a nearby location.\nStation: A POI annotation which indicates presence of station in a nearby location.\nStop: A POI annotation which indicates presence of stop in a nearby location.\nTraffic_Calming: A POI annotation which indicates presence of traffic_calming in a nearby location.\nTraffic_Signal: A POI annotation which indicates presence of traffic_signal in a nearby location.\nTurning_Loop: A POI annotation which indicates presence of turning_loop in a nearby location.\nSunrise_Sunset: Shows the period of day (i.e.¬†day or night) based on sunrise/sunset.\nCivil_Twilight: Shows the period of day (i.e.¬†day or night) based on civil twilight.\nNautical_Twilight: Shows the period of day (i.e.¬†day or night) based on nautical twilight.\nAstronomical_Twilight: Shows the period of day (i.e.¬†day or night) based on astronomical twilight.\ncovid: indicates if accident occurred before COVID or during/after COVID (March 2020)\n\n# Creating New Variables\n# Create COVID variable\nmet$post_covid &lt;- ifelse(met$Start_Time &lt; as.POSIXct(\"2020-03-01\", tz = \"UTC-6\"), \n                      0, 1)\n\n# Day of week variable (0 = Sunday, 6 = Saturday)\nmet$Day_of_Week &lt;- as.integer(format(met$Start_Time, \"%w\"))\n\n# Create the binary 'weekday' variable (1 for weekdays, 0 for weekends)\nmet$is_weekday &lt;- ifelse(met$Day_of_Week &gt;= 1 & met$Day_of_Week &lt;= 5, 1, 0)\n\n# Month variable\nmet$Month &lt;- as.integer(format(met$Start_Time, \"%m\"))\n\n# Create the 'season' variable\nmet$season &lt;- ifelse(met$Month %in% c(3, 4, 5), \"Spring\",\n                ifelse(met$Month %in% c(6, 7, 8), \"Summer\",\n                  ifelse(met$Month %in% c(9, 10, 11), \"Autumn\",\n                    \"Winter\")))\n\n# Create rush hour variable\n# Extract hour from the Start_Time column\nmet$start_hour &lt;- as.integer(format(met$Start_Time, \"%H\"))\n\n# rush hour variable, 1 if the hour is between 6 and 9 or between 15 and 18 on weekdays, 0 otherwise\nmet$rush_hr &lt;- ifelse(met$is_weekday == 1 & (met$start_hour &gt;= 6 & met$start_hour &lt;= 9 | met$start_hour &gt;= 15 & met$start_hour &lt;= 18), 1, 0)\n\n# Drop no longer needed variables\nmet$Day_of_Week &lt;- NULL\nmet$Month &lt;- NULL\n\n\nLooking at accident before and after COVID\n\n# Get the bounding box of the accidents data\nbbox_met &lt;- st_bbox(met_sf)\n\n# map\nggplot() +\n  geom_sf(data = streets) + \n  geom_sf(data = counties, color = \"green4\", fill = NA, size = 5) +\n  geom_sf(data = met_sf, aes(color = factor(post_covid)), size = .8, alpha = .5) +\n  scale_color_manual(\n    values = c(\"0\" = \"#0066FF\", \"1\" = \"red\"),\n    name = \"COVID Period\",\n    labels = c(\"0\" = \"Pre-COVID\", \"1\" = \"Post-COVID\")\n  ) +\n  geom_sf_label(data = counties, aes(label = CTY_NAME), size = 4, color = \"green4\") +\n  coord_sf(xlim = bbox_met[c('xmin', 'xmax')], ylim = bbox_met[c('ymin', 'ymax')], expand = FALSE) +\n  ggtitle(\"Locations of Traffic Accidents in Twin Cities (June 2016 through March 2023)\") +\n  theme(\n    plot.title = element_text(size = rel(1.5)),\n    axis.title.x = element_blank(), \n    axis.title.y = element_blank(),\n    legend.text = element_text(size = 12),\n    legend.title = element_text(size = 14)) +\n  guides(color = guide_legend(override.aes = list(size = 5)))\n\n\n\n\nIt appears that after COVID, a greater proportion of accidents may be happening in the suburbs as fewer people commute to the core cities. Indeed, pre-COVID 72.2% of accidents in the metro occurred in just Hennepin and Ramsey counties, while post-COVID the proportion was down to 69.1%\n\n# Filter accidents pre-covid\naccidents_pre_covid &lt;- met %&gt;%\n  filter(post_covid == 0)\n\n# Filter accidents post-covid\naccidents_post_covid &lt;- met %&gt;%\n  filter(post_covid == 1)\n\n# Get the count of accidents for each street pre-covid\ntop20_pre_covid &lt;- accidents_pre_covid %&gt;%\n  count(Street) %&gt;% \n  top_n(20, n)\n\n# Get the count of accidents for each street post-covid\ntop20_post_covid &lt;- accidents_post_covid %&gt;%\n  count(Street) %&gt;% \n  top_n(20, n) \n\n# Find the maximum number of accidents to set the same x-axis limit for both plots\nmax_accidents &lt;- max(c(top20_pre_covid$n, top20_post_covid$n))\n\n# Pre-COVID plot\nplot_pre_covid &lt;- ggplot(top20_pre_covid, aes(x = reorder(Street, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"#0066FF\") +\n  geom_text(aes(label = Street), vjust = 0.4, hjust = -0.2, color = \"black\", size = 3.0) +\n  coord_flip() +\n  labs(title = \"Top 20 Streets by Number of Accidents (Pre-COVID)\", x = NULL, y = NULL) +\n  ylim(0, max_accidents) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 0),  # Adjust here for horizontal labels\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank()\n  )\n\n\n# Post-COVID plot\nplot_post_covid &lt;- ggplot(top20_post_covid, aes(x = reorder(Street, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"red\") +\n  geom_text(aes(label = Street), vjust = 0.4, hjust = -0.2, color = \"black\", size = 3.0) +\n  coord_flip() +\n  labs(title = \"Top 20 Streets by Number of Accidents (Post-COVID)\", x = NULL, y = \"Number of Accidents\") +\n  ylim(0, max_accidents) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 0),  # Adjust here for horizontal labels\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank()\n  )\n\n# Align the plots vertically\ngrid.arrange(plot_pre_covid, plot_post_covid, ncol = 1)\n\n\n\n\nThese counts by street are for the entire periods of June 2016 through February 2020, and March 2020 through March 2023, pertaining to the pre and post COVID periods, respectively. This means there are 45 months worth of data for the pre-COVID period and 37 months of data for the post-COVID period.\n\n# Get weekly incidents by season\nweekly_incidents_by_season &lt;- met %&gt;%\n  mutate(Week = as.Date(floor_date(Start_Time, \"week\"))) %&gt;%\n  group_by(Week, season, post_covid) %&gt;%\n  summarise(Incidents = n(), .groups = 'drop') %&gt;%\n  ungroup() %&gt;%\n  arrange(Week, season, post_covid)\n\n# Reorder the 'season' variable\nweekly_incidents_by_season$season &lt;- factor(weekly_incidents_by_season$season, \n                                            levels = c(\"Spring\", \"Summer\", \"Autumn\", \"Winter\"))\n\n# Define date limits for the plot\nstart_date &lt;- as.Date(\"2016-06-01\")\nend_date &lt;- as.Date(\"2023-03-31\")  \n\n# Plot\nggplot(weekly_incidents_by_season, aes(x = Week, y = Incidents)) +\n  geom_segment(aes(xend = lead(Week), yend = lead(Incidents), color = season), size = .5) +\n  scale_color_manual(name = \"Season\", \n                     values = c(\"Spring\" = \"green\", \"Summer\" = \"red\", \n                                \"Autumn\" = \"orange\", \"Winter\" = \"blue\")) +\n  ggtitle(\"Weekly Accident Rate in the Twin Cities Metro\") +\n  xlab(\"\") +\n  ylab(\"Number of Accidents\") +\n  scale_x_date(date_breaks = '1 month', date_labels = \"%b %Y\",\n               limits = c(start_date, end_date),\n               expand = c(0, 0)) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5, size = rel(0.8))) +\n  geom_smooth(data = subset(weekly_incidents_by_season, post_covid == 0),\n              aes(x = Week, y = Incidents, linetype = \"Pre-Covid\"), \n              method = \"lm\", se = FALSE, color = \"darkgray\") +\n   geom_smooth(data = subset(weekly_incidents_by_season, post_covid == 1),\n              aes(x = Week, y = Incidents, linetype = \"Post-Covid\"), \n              method = \"lm\", se = FALSE, color = \"black\") +\n   scale_linetype_manual(name = \"Trend Lines\",\n                        values = c(\"Pre-Covid\" = \"dashed\", \"Post-Covid\" = \"dashed\"),\n                        labels = c(\"Pre-Covid\", \"Post-Covid\"),\n                        guide = guide_legend(override.aes = list(color = c(\"Pre-Covid\" = \"darkgray\",   \"Post-Covid\" = \"black\"))))\n\n\n\n\nThe weekly accident rate was trending upwards pre-COVID, but there‚Äôs a noticable shift during the post-COVID period as the trend seems to flatten out while the seasonality becomes more pronounced.\n\n# Create data frames with counts of incidents for each hour, pre and post Covid\nhourly_incidents_pre_covid &lt;- met %&gt;%\n  filter(post_covid == 0) %&gt;%\n  group_by(start_hour) %&gt;%\n  summarise(Count = n())\n\nhourly_incidents_post_covid &lt;- met %&gt;%\n  filter(post_covid == 1) %&gt;%\n  group_by(start_hour) %&gt;%\n  summarise(Count = n())\n\n# Combine the data\ncombined_hourly_incidents &lt;- bind_rows(\n  mutate(hourly_incidents_pre_covid, Period = \"Pre-Covid\"),\n  mutate(hourly_incidents_post_covid, Period = \"Post-Covid\")\n)\n\n# Convert 'Period' to a factor with levels in the desired order\ncombined_hourly_incidents$Period &lt;- factor(combined_hourly_incidents$Period, \n                                           levels = c(\"Pre-Covid\", \"Post-Covid\"))\n\n# Line plot of incidents by hour of the day\nggplot(combined_hourly_incidents, aes(x=start_hour, y=Count, color=Period)) +\n  geom_smooth(se=F, method = \"loess\", span = 0.2) + \n  scale_color_manual(values=c(\"#0066FF\", \"red\")) +  \n  ggtitle(\"Number of Accidents by Hour of the Day\") +\n  xlab(\"Hour of the Day\") +\n  ylab(\"Number of Accidents\") +\n  theme_minimal() +\n  theme(legend.position=\"top\")\n\n\n\n\nReminder: there are 45 months worth of data for the pre-COVID period and 37 months of data for the post-COVID period. I believe the length of these two periods are close enough for general comparison purposes, but it‚Äôs important to note that all else being equal, there should be higher counts in the pre-COVID period. However, there are actually 65,459 accidents recorded post-COVID compared with only 56,054 pre-COVID.\n\n# Boxplots for length of road affected\nggplot(met, aes(y = factor(post_covid, levels = c(1, 0)), x = `Distance(mi)`, fill = factor(post_covid, levels = c(1, 0)))) +\n  geom_boxplot(outlier.size = 1, orientation = \"y\") +\n  scale_fill_manual(values = c(\"red\", \"#0066FF\")) +\n  scale_y_discrete(labels = c(\"1\" = \"Post-Covid\", \"0\" = \"Pre-Covid\")) +\n  labs(title = \"Length of Section of Road Affected When Accidents Occur\",\n       y = \"\",\n       x = \"Distance (mi)\") +\n  theme_minimal() +\n  theme(\n    axis.title.y = element_blank(),\n    axis.text.y = element_text(angle = 45, hjust = 1),\n    legend.position = \"none\"\n  ) +\n  coord_cartesian(xlim = c(0, 2.5))\n\n\n\n\nFor each observed accident, the length in miles of the stretch of road that was affected was recorded. Before COVID, the median distance was 0 miles and the mean was 0.48 miles. After COVID, the median distance affected was 0.43 miles while the mean was 0.85 miles.\n\n# Converting logical types to binary numbers\n# Identifying logical columns\nlogical_cols &lt;- sapply(met, is.logical)\n\n# Converting logical columns to binary (0 and 1)\nmet[, logical_cols] &lt;- lapply(met[, logical_cols], as.integer)\n\n\n# Select features of interest\nmet_selected &lt;- met %&gt;%\n  select(Severity, rush_hr, is_weekday, season, post_covid,\n         County, `Temperature(F)`, `Humidity(%)`, `Visibility(mi)`, `Wind_Speed(mph)`,\n         `Precipitation(in)`, Amenity, Bump, Crossing, Give_Way, Junction, No_Exit,\n         Railway, Roundabout, Station, Stop, Traffic_Calming, Traffic_Signal,\n         Turning_Loop, season) %&gt;%\n  mutate(\n    County = factor(County),\n    season = factor(season)) %&gt;%\n  dummy_cols(select_columns = c(\"County\", \"season\"), remove_selected_columns = TRUE)\n\n# Drop Spring season and Ramsey County\nmet_selected$season_Spring &lt;- NULL\nmet_selected$County_Ramsey &lt;- NULL\n\n\n# Transform target variable\nmet_selected &lt;- met_selected %&gt;%\n    mutate(severe = as.factor(ifelse(Severity %in% 1:2, 0, 1))) %&gt;%\n    select(-Severity)\n\n\n# Compute the number and percentage of NAs for each column\nna_summary &lt;- met_selected %&gt;%\n  summarise_all(~sum(is.na(.))) %&gt;%\n  gather(column, na_count) %&gt;%\n  mutate(na_percentage = (na_count / nrow(met)) * 100) %&gt;%\n  filter(na_count &gt; 1)\nprint(na_summary)\n\n# A tibble: 5 √ó 3\n  column            na_count na_percentage\n  &lt;chr&gt;                &lt;int&gt;         &lt;dbl&gt;\n1 Temperature(F)         632         0.520\n2 Humidity(%)            722         0.594\n3 Visibility(mi)         617         0.508\n4 Wind_Speed(mph)       3590         2.95 \n5 Precipitation(in)    29028        23.9  \n\n\n\n# Remove NAs\nmet_clean &lt;- na.omit(met_selected)\n\n\n# Scaling numerical variables, except for Start_Hour\nnumerical_vars &lt;- c(\"Temperature(F)\", \"Humidity(%)\",\n         \"Visibility(mi)\", \"Wind_Speed(mph)\", \"Precipitation(in)\")\n\nmet_clean[numerical_vars] &lt;- scale(met_clean[numerical_vars])\n\n\n# Partition the data.\nset.seed(720)\nsamp = createDataPartition(met_clean$severe,\n                           p = 0.7,\n                           list = FALSE)\ntrain = met_clean[samp, ]\ntest = met_clean[-samp, ]\nrm(samp)\n\n\n# Check for class imbalance problems. \ntrain %&gt;%\n  select(severe) %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  mutate(Proportion = Freq / sum(Freq)) %&gt;%\n  rename(Counts = Freq) %&gt;%\n  t()\n\n           [,1]        [,2]       \nsevere     \"0\"         \"1\"        \nCounts     \"56029\"     \" 8078\"    \nProportion \"0.8739919\" \"0.1260081\"\n\n\n#SMOTE\nThere is a class imbalance to address: only about 12% of observations were in the severe category.\n\n# Apply SMOTE to the training data to only oversample the minority class\nsmoted &lt;- performanceEstimation::smote(severe ~ ., data = train, \n                                            perc.over = .7, k = 5, perc.under = 7)\n# Over/under values:\n# .3 and 21 gave 50883 and 10501\n\n# .5 and 11 gave 44429 and 12117\n  # Sensitivity: .439\n\n# .7 and 7 gave 39578 and 13732\n  # Sensitivity: .539\n\n# Checking the class distribution in the final dataset\nsmoted %&gt;%\n  select(severe) %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  mutate(Proportion = Freq / sum(Freq)) %&gt;%\n  rename(Counts = Freq) %&gt;%\n  t()\n\n           [,1]        [,2]       \nsevere     \"0\"         \"1\"        \nCounts     \"39578\"     \"13732\"    \nProportion \"0.7424123\" \"0.2575877\"\n\n\nI used smote to generate additional records for the severe accidents, while downsampling the less severe accidents.\n#LASSO\n\n# LASSO\n# Separate predictors and response\ny &lt;- as.vector(smoted$severe)\nX &lt;- as.matrix(smoted %&gt;% dplyr::select(-severe))\n\n# Use cross-validation to find the best lambda\ncv.lasso &lt;- cv.glmnet(X, y, family=\"binomial\", alpha=1, thresh=1e-7)\n\n# Extract best lambda\nbest_lambda &lt;- cv.lasso$lambda.1se\n\n# Fit the model using the best lambda\nLASSO_model &lt;- glmnet(X, y, family=\"binomial\", alpha=1, lambda=best_lambda,\n                      maxit = 1e6)\n\n# View the coefficients\ncoef(LASSO_model)\n\n31 x 1 sparse Matrix of class \"dgCMatrix\"\n                           s0\n(Intercept)        0.96778457\nrush_hr           -0.29167085\nis_weekday         0.29222535\npost_covid        -2.31490644\nTemperature(F)     0.08330439\nHumidity(%)       -0.16332337\nVisibility(mi)    -0.19788784\nWind_Speed(mph)    0.18725222\nPrecipitation(in)  0.00925945\nAmenity           -0.32559935\nBump               .         \nCrossing          -0.49021614\nGive_Way          -0.12711155\nJunction          -0.30332686\nNo_Exit            .         \nRailway            .         \nRoundabout         .         \nStation           -0.45659369\nStop              -0.78992782\nTraffic_Calming    .         \nTraffic_Signal     0.30013649\nTurning_Loop       .         \nCounty_Anoka      -0.44565011\nCounty_Carver     -1.49365059\nCounty_Dakota     -0.46861598\nCounty_Hennepin   -0.15591875\nCounty_Scott      -1.66569542\nCounty_Washington -0.50940604\nseason_Autumn     -0.67726857\nseason_Summer      .         \nseason_Winter     -1.48866098\n\n\n\n# LASSO output\ntest_predictors = as.matrix(test %&gt;% dplyr::select(-severe))\nLASSO_test_class = predict(LASSO_model, newx = test_predictors, s = best_lambda, type=\"class\")\nLASSO_test_prob = predict(LASSO_model, newx = test_predictors, s = best_lambda, type=\"response\")[,1]\n\nLASSO_test_class_factor = factor(LASSO_test_class, levels = c(\"0\", \"1\"))\nresponse_vector_factor = factor(test$severe, levels = c(\"0\", \"1\"))\n\n# Create and print the confusion matrix\nLASSO_cm = confusionMatrix(LASSO_test_class_factor, response_vector_factor, positive = \"1\")\n\n# Calculate the F1 Score for LASSO\nprecision_LASSO &lt;- 0.8497 #PPV\nrecall_LASSO &lt;- 0.5856 #Sensitivity\nf1_score_LASSO &lt;- 2 * (precision_LASSO * recall_LASSO) / (precision_LASSO + recall_LASSO)\n\n# Print the F1 Score for LASSO\nprint(\"LASSO Results\")\n\n[1] \"LASSO Results\"\n\nprint(LASSO_cm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 21923  1977\n         1  2089  1485\n                                          \n               Accuracy : 0.852           \n                 95% CI : (0.8478, 0.8562)\n    No Information Rate : 0.874           \n    P-Value [Acc &gt; NIR] : 1.00000         \n                                          \n                  Kappa : 0.3373          \n                                          \n Mcnemar's Test P-Value : 0.08173         \n                                          \n            Sensitivity : 0.42894         \n            Specificity : 0.91300         \n         Pos Pred Value : 0.41550         \n         Neg Pred Value : 0.91728         \n             Prevalence : 0.12601         \n         Detection Rate : 0.05405         \n   Detection Prevalence : 0.13009         \n      Balanced Accuracy : 0.67097         \n                                          \n       'Positive' Class : 1               \n                                          \n\nprint(paste(\"LASSO F1 Score:\", f1_score_LASSO))\n\n[1] \"LASSO F1 Score: 0.693352358391974\"\n\n\n#Logistic Regression\n\n# Create subset with LASSO suggested variables\nlasso_rec &lt;- smoted %&gt;%\n  select(-\"Bump\", -\"No_Exit\", -\"Railway\", -\"Roundabout\", -\"Traffic_Calming\",\n         -\"Turning_Loop\", -\"season_Summer\")\n\n\n# Fit the logistic regression model\nlogistic_trained &lt;- glm(severe ~ ., data = lasso_rec, family = \"binomial\")\n\n# View the summary of the model\nsummary(logistic_trained)\n\n\nCall:\nglm(formula = severe ~ ., family = \"binomial\", data = lasso_rec)\n\nCoefficients:\n                    Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)          1.17608    0.04538  25.916 &lt; 0.0000000000000002 ***\nrush_hr             -0.37734    0.02687 -14.045 &lt; 0.0000000000000002 ***\nis_weekday           0.40639    0.03543  11.471 &lt; 0.0000000000000002 ***\npost_covid          -2.42756    0.02699 -89.947 &lt; 0.0000000000000002 ***\n`Temperature(F)`     0.07891    0.02014   3.918        0.00008926941 ***\n`Humidity(%)`       -0.20542    0.01447 -14.193 &lt; 0.0000000000000002 ***\n`Visibility(mi)`    -0.24300    0.01471 -16.521 &lt; 0.0000000000000002 ***\n`Wind_Speed(mph)`    0.19603    0.01256  15.605 &lt; 0.0000000000000002 ***\n`Precipitation(in)`  0.02209    0.01069   2.066               0.0389 *  \nAmenity             -1.33048    0.52246  -2.547               0.0109 *  \nCrossing            -0.69479    0.06428 -10.809 &lt; 0.0000000000000002 ***\nGive_Way            -0.80961    0.37149  -2.179               0.0293 *  \nJunction            -0.38697    0.03913  -9.888 &lt; 0.0000000000000002 ***\nStation             -0.67456    0.10405  -6.483        0.00000000009 ***\nStop                -1.32825    0.27650  -4.804        0.00000155654 ***\nTraffic_Signal       0.51928    0.05269   9.854 &lt; 0.0000000000000002 ***\nCounty_Anoka        -0.69712    0.05840 -11.936 &lt; 0.0000000000000002 ***\nCounty_Carver       -2.13187    0.16641 -12.811 &lt; 0.0000000000000002 ***\nCounty_Dakota       -0.67564    0.04530 -14.916 &lt; 0.0000000000000002 ***\nCounty_Hennepin     -0.32572    0.02913 -11.182 &lt; 0.0000000000000002 ***\nCounty_Scott        -2.20874    0.12505 -17.663 &lt; 0.0000000000000002 ***\nCounty_Washington   -0.75777    0.05729 -13.226 &lt; 0.0000000000000002 ***\nseason_Autumn       -0.78572    0.03320 -23.668 &lt; 0.0000000000000002 ***\nseason_Winter       -1.63841    0.04443 -36.879 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 60829  on 53309  degrees of freedom\nResidual deviance: 45115  on 53286  degrees of freedom\nAIC: 45163\n\nNumber of Fisher Scoring iterations: 5\n\nexp(coef(logistic_trained))\n\n        (Intercept)             rush_hr          is_weekday          post_covid \n         3.24165095          0.68568500          1.50138549          0.08825223 \n   `Temperature(F)`       `Humidity(%)`    `Visibility(mi)`   `Wind_Speed(mph)` \n         1.08211158          0.81430686          0.78427452          1.21656900 \n`Precipitation(in)`             Amenity            Crossing            Give_Way \n         1.02233218          0.26435035          0.49917963          0.44503313 \n           Junction             Station                Stop      Traffic_Signal \n         0.67911201          0.50938099          0.26494148          1.68081000 \n       County_Anoka       County_Carver       County_Dakota     County_Hennepin \n         0.49801609          0.11861585          0.50883069          0.72200819 \n       County_Scott   County_Washington       season_Autumn       season_Winter \n         0.10983862          0.46871162          0.45579195          0.19428808 \n\npR2(logistic_trained)\n\nfitting null model for pseudo-r2\n\n\n           llh        llhNull             G2       McFadden           r2ML \n-22557.3221517 -30414.3448784  15714.0454535      0.2583328      0.2552951 \n          r2CU \n     0.3751513 \n\n\n\n# Predict on test set for regular LOGISTIC REGRESSION\npredicted_probs_logit &lt;- predict(logistic_trained, newdata = test, type = \"response\")\nthreshold &lt;- 0.5\npredicted_classes_logit &lt;- ifelse(predicted_probs_logit &gt; threshold, 1, 0)\n\n# Convert predicted_classes and actual classes to factors\npredicted_classes_factor_logit &lt;- factor(predicted_classes_logit, levels = c(\"0\", \"1\"))\nactual_classes_factor_logit &lt;- factor(test$severe, levels = c(\"0\", \"1\"))\n\n# Generate the confusion matrix\nconfusion_matrix_logit &lt;- confusionMatrix(predicted_classes_factor_logit,\n                                          actual_classes_factor_logit, positive = \"1\")\n\n# Calculate the F1 Score\nlogit_precision &lt;- 0.8402 #PPV\nlogit_recall &lt;- 0.5955 #Sensitivity\nf1_score_logit &lt;- 2 * (logit_precision * logit_recall) /\n  (logit_precision + logit_recall)\n\n# Print the full confusion matrix summary and F1\nprint(\"Regular Logistic Regression:\")\n\n[1] \"Regular Logistic Regression:\"\n\nprint(confusion_matrix_logit)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction     0     1\n         0 21925  1998\n         1  2087  1464\n                                          \n               Accuracy : 0.8513          \n                 95% CI : (0.8471, 0.8555)\n    No Information Rate : 0.874           \n    P-Value [Acc &gt; NIR] : 1.0000          \n                                          \n                  Kappa : 0.3323          \n                                          \n Mcnemar's Test P-Value : 0.1686          \n                                          \n            Sensitivity : 0.42288         \n            Specificity : 0.91309         \n         Pos Pred Value : 0.41228         \n         Neg Pred Value : 0.91648         \n             Prevalence : 0.12601         \n         Detection Rate : 0.05329         \n   Detection Prevalence : 0.12925         \n      Balanced Accuracy : 0.66798         \n                                          \n       'Positive' Class : 1               \n                                          \n\nprint(paste(\"F1 Score:\", f1_score_logit))\n\n[1] \"F1 Score: 0.696996726335586\"\n\n\n\n# Logit Explainer\nlogit_explain = DALEX::explain(model = logistic_trained,\n                             data = test,\n                             y = as.numeric(test$severe==\"1\"),\n                             type = \"classification\",\n                             label = \"Logit\")\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  Logit \n  -&gt; data              :  27474  rows  31  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  27474  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.3.1 , task classification (  default  ) \n  -&gt; model_info        :  type set to  classification \n  -&gt; predicted values  :  numerical, min =  0.0005712321 , mean =  0.2164065 , max =  0.928552  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.928552 , mean =  -0.09039649 , max =  0.9945333  \n  A new explainer has been created!  \n\n\n#GBM Model\n\n# Train a gradient boost model\ngbm_smoted &lt;- smoted\nsmoted$severe &lt;- factor(gbm_smoted$severe, levels = c(\"0\", \"1\"), labels = c(\"Level_0\", \"Level_1\"))\n\nset.seed(469)\n#gbm_model = train(\n#  y = gbm_smoted$severe,\n#  x = select(gbm_smoted, -severe),\n#  method = \"gbm\",\n#  verbose = FALSE,\n#  trControl = trainControl(method = \"repeatedcv\", number = 5, repeats = 5, classProbs = TRUE),\n#  tuneLength = 10\n#)\n#saveRDS(gbm_model, \"met_gbm.5.5.10.rds\")\ngbm_model = readRDS(\"met_gbm.5.5.10.rds\")\nplot(gbm_model)\n\n\n\n\n\n# GBM Explainer\ngbm_explain = DALEX::explain(model = gbm_model,\n                             data = test,\n                             y = as.numeric(test$severe==\"1\"),\n                             type = \"classification\",\n                             label = \"GBM\")\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  GBM \n  -&gt; data              :  27474  rows  31  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  27474  values \n  -&gt; predict function  :  yhat.train  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package caret , ver. 6.0.94 , task classification (  default  ) \n  -&gt; model_info        :  type set to  classification \n  -&gt; predicted values  :  numerical, min =  0.003388453 , mean =  0.1639321 , max =  0.993433  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9926813 , mean =  -0.03792203 , max =  0.9936342  \n  A new explainer has been created!  \n\n\n\n\nRandom Forest\n\n# Train a random forest model\n#set.seed(776)\n#rf_model = train(\n#   y = smoted$severe,\n#   x = select(smoted, -severe),\n#   method = \"rf\",\n#   trControl = trainControl(method = \"repeatedcv\", number = 5, repeats = 5),\n#   tuneLength = 10\n# )\n\n#saveRDS(rf_model, \"met_rf.5.5.10.rds\")\nrf_model = readRDS(\"met_rf.5.5.10.rds\")\nplot(rf_model)\n\n\n\n\n\nrf_explain = DALEX::explain(model = rf_model,\n                            data = test,\n                            y = as.numeric(test$severe==\"1\"),\n                            type = \"classification\",\n                            label = \"Random Forest\")\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  Random Forest \n  -&gt; data              :  27474  rows  31  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  27474  values \n  -&gt; predict function  :  yhat.train  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package caret , ver. 6.0.94 , task classification (  default  ) \n  -&gt; model_info        :  type set to  classification \n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.1773408 , max =  1  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -1 , mean =  -0.05133079 , max =  1  \n  A new explainer has been created!  \n\n\n\n\nPerformance\n\n# Performance of GBM and logistic regression\nlogit_perf = DALEX::model_performance(logit_explain, cutoff = 0.5)\ngbm_perf = DALEX::model_performance(gbm_explain, cutoff = 0.5)\nrf_perf = DALEX::model_performance(rf_explain, cutoff = 0.5)\n\n\nprint(\"GBM Performance\")\n\n[1] \"GBM Performance\"\n\ngbm_perf\n\nMeasures for:  classification\nrecall     : 0.39197 \nprecision  : 0.586684 \nf1         : 0.4699567 \naccuracy   : 0.8885856 \nauc        : 0.8433761\n\nResiduals:\n         0%         10%         20%         30%         40%         50% \n-0.99268132 -0.28070930 -0.17592324 -0.12791891 -0.08659678 -0.05395055 \n        60%         70%         80%         90%        100% \n-0.03217305 -0.02341745 -0.01708539  0.31384591  0.99363416 \n\ncat(\"\\n\")\nprint(\"Logistic Regression Performance\")\n\n[1] \"Logistic Regression Performance\"\n\nlogit_perf\n\nMeasures for:  classification\nrecall     : 0.4228769 \nprecision  : 0.4122782 \nf1         : 0.4175103 \naccuracy   : 0.851314 \nauc        : 0.8031435\n\nResiduals:\n         0%         10%         20%         30%         40%         50% \n-0.92855204 -0.42585652 -0.27172086 -0.19792200 -0.13620647 -0.08774507 \n        60%         70%         80%         90%        100% \n-0.05659205 -0.03819438 -0.02362172  0.30245231  0.99453333 \n\ncat(\"\\n\")\nprint(\"Random Forest Performance\")\n\n[1] \"Random Forest Performance\"\n\nrf_perf\n\nMeasures for:  classification\nrecall     : 0.4829578 \nprecision  : 0.5100671 \nf1         : 0.4961424 \naccuracy   : 0.8763922 \nauc        : 0.8354438\n\nResiduals:\n    0%    10%    20%    30%    40%    50%    60%    70%    80%    90%   100% \n-1.000 -0.364 -0.202 -0.122 -0.068 -0.034 -0.016 -0.004  0.000  0.220  1.000 \n\n\n\nroc = plot(logit_perf, gbm_perf, rf_perf, geom = \"roc\")\nprc = plot(logit_perf, gbm_perf, rf_perf, geom = \"prc\")\n\nroc + prc\n\n\n\n\n\ngbm_mp = DALEX::model_parts(gbm_explain,\n                            B = 50)\nplot(gbm_mp)"
  },
  {
    "objectID": "projects/DM Project/ProjScript.html",
    "href": "projects/DM Project/ProjScript.html",
    "title": "Traffic Accidents in the Twin Cities Before and After COVID",
    "section": "",
    "text": "This dataset contains records of traffic accidents for the seven-county region overseen by the metropolitan council of the Twin Cities.\nTraffic Accident data source: Sobhan Moosavi. (2023). US Accidents (2016 - 2023) [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DS/199387\nMinnesota shape files came from https://gisdata.mn.gov/organization/us-mn-state-dot"
  },
  {
    "objectID": "projects/DM Project/ProjScript.html#features-of-interest",
    "href": "projects/DM Project/ProjScript.html#features-of-interest",
    "title": "Traffic Accidents in the Twin Cities Before and After COVID",
    "section": "Features of interest",
    "text": "Features of interest\nFor my models I‚Äôm selecting the following variables to start:\nsevere: The original dataset measured the severity of accidents on a scale of 1 - 4, with 4 being the most severe. The source documentation indicates severity is based on the length of time traffic was impacted. I turned this into a binary variable with accidents measuring at a 3 or 4 on the original scale as ‚Äòsevere‚Äô.\nrush_hr: Computed binary variable for accidents which occured between 6 and 9 am or 3 and 6 pm.\nis_weekday: Binary variable indicating if addicent occured on weekday or not.\nseason: Spring, summer, fall, winter\npost_covid: binary variable, with post_covid being define as during or after March 1st, 2020.\nCounty: Shows the county in the address field.\nTemperature(F): Shows the temperature (in Fahrenheit).\nHumidity(%): Shows the humidity (in percentage).\nVisibility(mi): Shows visibility (in miles).\nWind_Speed(mph): Shows wind speed (in miles per hour).\nPrecipitation(in): Shows precipitation amount in inches, if there is any.\nAmenity: A POI (Point of Interest) annotation which indicates the presence of an amenity in a nearby location.\nBump: A POI annotation which indicates the presence of a speed bump or hump in a nearby location.\nCrossing: A POI annotation which indicates the presence of a crossing in a nearby location.\nGive_Way: A POI annotation which indicates the presence of a give_way sign in a nearby location.\nJunction: A POI annotation which indicates the presence of a junction in a nearby location.\nNo_Exit: A POI annotation which indicates the presence of a no_exit sign in a nearby location.\nRailway: A POI annotation which indicates the presence of a railway in a nearby location.\nRoundabout: A POI annotation which indicates the presence of a roundabout in a nearby location.\nStation: A POI annotation which indicates the presence of a station (such as a bus or train station) in a nearby location.\nStop: A POI annotation which indicates the presence of a stop sign in a nearby location.\nTraffic_Calming: A POI annotation which indicates the presence of traffic calming measures (such as speed humps) in a nearby location.\nTraffic_Signal: A POI annotation which indicates the presence of a traffic signal in a nearby location.\nTurning_Loop: A POI annotation which indicates the presence of a turning loop in a nearby location."
  },
  {
    "objectID": "projects/DM Project/ProjScript.html#data-cleaning",
    "href": "projects/DM Project/ProjScript.html#data-cleaning",
    "title": "Traffic Accidents in the Twin Cities Before and After COVID",
    "section": "Data cleaning",
    "text": "Data cleaning\n\n# Compute the number and percentage of NAs for each column\nna_summary &lt;- met_selected %&gt;%\n  summarise_all(~sum(is.na(.))) %&gt;%\n  gather(column, na_count) %&gt;%\n  mutate(na_percentage = (na_count / nrow(met)) * 100) %&gt;%\n  filter(na_count &gt; 1)\nprint(na_summary)\n\n# A tibble: 5 √ó 3\n  column            na_count na_percentage\n  &lt;chr&gt;                &lt;int&gt;         &lt;dbl&gt;\n1 Temperature(F)         632         0.520\n2 Humidity(%)            722         0.594\n3 Visibility(mi)         617         0.508\n4 Wind_Speed(mph)       3590         2.95 \n5 Precipitation(in)    29028        23.9  \n\n\n\n# Remove NAs\nmet_clean &lt;- na.omit(met_selected)\n\n\n# Scaling numerical variables, except for Start_Hour\nnumerical_vars &lt;- c(\"Temperature(F)\", \"Humidity(%)\",\n         \"Visibility(mi)\", \"Wind_Speed(mph)\", \"Precipitation(in)\")\n\nmet_clean[numerical_vars] &lt;- scale(met_clean[numerical_vars])\n\nNumerical variables except for Start_Hour were scaled. NAs were removed because imputation seemed inappropriate given the large amount of missing data for Precipitation(in). The number of NAs for other features was very small."
  },
  {
    "objectID": "projects/DM Project/ProjScript.html#partitioning-the-data-for-training-and-testing",
    "href": "projects/DM Project/ProjScript.html#partitioning-the-data-for-training-and-testing",
    "title": "Traffic Accidents in the Twin Cities Before and After COVID",
    "section": "Partitioning the data for training and testing",
    "text": "Partitioning the data for training and testing\n\n# Partition the data.\nset.seed(720)\nsamp = createDataPartition(met_clean$severe,\n                           p = 0.7,\n                           list = FALSE)\ntrain = met_clean[samp, ]\ntest = met_clean[-samp, ]\nrm(samp)\n\n\n# Check for class imbalance problems. \ntrain %&gt;%\n  select(severe) %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  mutate(Proportion = Freq / sum(Freq)) %&gt;%\n  rename(Counts = Freq) %&gt;%\n  t()\n\n           [,1]        [,2]       \nsevere     \"0\"         \"1\"        \nCounts     \"56029\"     \" 8078\"    \nProportion \"0.8739919\" \"0.1260081\""
  },
  {
    "objectID": "projects/DM Project/ProjScript.html#smote",
    "href": "projects/DM Project/ProjScript.html#smote",
    "title": "Traffic Accidents in the Twin Cities Before and After COVID",
    "section": "SMOTE",
    "text": "SMOTE\nThere is a class imbalance to address: only about 12% of observations were in the severe category.\n\n# Apply SMOTE to the training data to only oversample the minority class\nsmoted &lt;- performanceEstimation::smote(severe ~ ., data = train, \n                                            perc.over = .7, k = 5, perc.under = 7)\n# Over/under values:\n# .3 and 21 gave 50883 and 10501\n\n# .5 and 11 gave 44429 and 12117\n  # Sensitivity: .439\n\n# .7 and 7 gave 39578 and 13732\n  # Sensitivity: .539\n\n# Checking the class distribution in the final dataset\nsmoted %&gt;%\n  select(severe) %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  mutate(Proportion = Freq / sum(Freq)) %&gt;%\n  rename(Counts = Freq) %&gt;%\n  t()\n\n           [,1]        [,2]       \nsevere     \"0\"         \"1\"        \nCounts     \"39578\"     \"13732\"    \nProportion \"0.7424123\" \"0.2575877\"\n\n\nI used smote to generate additional records for the severe accidents, while downsampling the less severe accidents."
  },
  {
    "objectID": "projects/DM Project/index.html",
    "href": "projects/DM Project/index.html",
    "title": "Traffic Accidents in the Twin Cities Before and After COVID",
    "section": "",
    "text": "This dataset contains records of traffic accidents for the seven-county region overseen by the metropolitan council of the Twin Cities.\nTraffic Accident data source: Sobhan Moosavi. (2023). US Accidents (2016 - 2023) [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DS/199387\nMinnesota shape files came from https://gisdata.mn.gov/organization/us-mn-state-dot"
  },
  {
    "objectID": "projects/DM Project/index.html#features-of-interest",
    "href": "projects/DM Project/index.html#features-of-interest",
    "title": "Traffic Accidents in the Twin Cities Before and After COVID",
    "section": "Features of interest",
    "text": "Features of interest\nFor my models I‚Äôm selecting the following variables to start:\nsevere: The original dataset measured the severity of accidents on a scale of 1 - 4, with 4 being the most severe. The source documentation indicates severity is based on the length of time traffic was impacted. I turned this into a binary variable with accidents measuring at a 3 or 4 on the original scale as ‚Äòsevere‚Äô.\nrush_hr: Computed binary variable for accidents which occured between 6 and 9 am or 3 and 6 pm.\nis_weekday: Binary variable indicating if addicent occured on weekday or not.\nseason: Spring, summer, fall, winter\npost_covid: binary variable, with post_covid being define as during or after March 1st, 2020.\nCounty: Shows the county in the address field.\nTemperature(F): Shows the temperature (in Fahrenheit).\nHumidity(%): Shows the humidity (in percentage).\nVisibility(mi): Shows visibility (in miles).\nWind_Speed(mph): Shows wind speed (in miles per hour).\nPrecipitation(in): Shows precipitation amount in inches, if there is any.\nAmenity: A POI (Point of Interest) annotation which indicates the presence of an amenity in a nearby location.\nBump: A POI annotation which indicates the presence of a speed bump or hump in a nearby location.\nCrossing: A POI annotation which indicates the presence of a crossing in a nearby location.\nGive_Way: A POI annotation which indicates the presence of a give_way sign in a nearby location.\nJunction: A POI annotation which indicates the presence of a junction in a nearby location.\nNo_Exit: A POI annotation which indicates the presence of a no_exit sign in a nearby location.\nRailway: A POI annotation which indicates the presence of a railway in a nearby location.\nRoundabout: A POI annotation which indicates the presence of a roundabout in a nearby location.\nStation: A POI annotation which indicates the presence of a station (such as a bus or train station) in a nearby location.\nStop: A POI annotation which indicates the presence of a stop sign in a nearby location.\nTraffic_Calming: A POI annotation which indicates the presence of traffic calming measures (such as speed humps) in a nearby location.\nTraffic_Signal: A POI annotation which indicates the presence of a traffic signal in a nearby location.\nTurning_Loop: A POI annotation which indicates the presence of a turning loop in a nearby location."
  },
  {
    "objectID": "projects/DM Project/index.html#data-cleaning",
    "href": "projects/DM Project/index.html#data-cleaning",
    "title": "Traffic Accidents in the Twin Cities Before and After COVID",
    "section": "Data cleaning",
    "text": "Data cleaning\n\n# Compute the number and percentage of NAs for each column\nna_summary &lt;- met_selected %&gt;%\n  summarise_all(~sum(is.na(.))) %&gt;%\n  gather(column, na_count) %&gt;%\n  mutate(na_percentage = (na_count / nrow(met)) * 100) %&gt;%\n  filter(na_count &gt; 1)\nprint(na_summary)\n\n# A tibble: 5 √ó 3\n  column            na_count na_percentage\n  &lt;chr&gt;                &lt;int&gt;         &lt;dbl&gt;\n1 Temperature(F)         632         0.520\n2 Humidity(%)            722         0.594\n3 Visibility(mi)         617         0.508\n4 Wind_Speed(mph)       3590         2.95 \n5 Precipitation(in)    29028        23.9  \n\n\n\n# Remove NAs\nmet_clean &lt;- na.omit(met_selected)\n\n\n# Scaling numerical variables, except for Start_Hour\nnumerical_vars &lt;- c(\"Temperature(F)\", \"Humidity(%)\",\n         \"Visibility(mi)\", \"Wind_Speed(mph)\", \"Precipitation(in)\")\n\nmet_clean[numerical_vars] &lt;- scale(met_clean[numerical_vars])\n\nNumerical variables except for Start_Hour were scaled. NAs were removed because imputation seemed inappropriate given the large amount of missing data for Precipitation(in). The number of NAs for other features was very small."
  },
  {
    "objectID": "projects/DM Project/index.html#partitioning-the-data-for-training-and-testing",
    "href": "projects/DM Project/index.html#partitioning-the-data-for-training-and-testing",
    "title": "Traffic Accidents in the Twin Cities Before and After COVID",
    "section": "Partitioning the data for training and testing",
    "text": "Partitioning the data for training and testing\n\n# Partition the data.\nset.seed(720)\nsamp = createDataPartition(met_clean$severe,\n                           p = 0.7,\n                           list = FALSE)\ntrain = met_clean[samp, ]\ntest = met_clean[-samp, ]\nrm(samp)\n\n\n# Check for class imbalance problems. \ntrain %&gt;%\n  select(severe) %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  mutate(Proportion = Freq / sum(Freq)) %&gt;%\n  rename(Counts = Freq) %&gt;%\n  t()\n\n           [,1]        [,2]       \nsevere     \"0\"         \"1\"        \nCounts     \"56029\"     \" 8078\"    \nProportion \"0.8739919\" \"0.1260081\""
  },
  {
    "objectID": "projects/DM Project/index.html#smote",
    "href": "projects/DM Project/index.html#smote",
    "title": "Traffic Accidents in the Twin Cities Before and After COVID",
    "section": "SMOTE",
    "text": "SMOTE\nThere is a class imbalance to address: only about 12% of observations were in the severe category.\n\n# Apply SMOTE to the training data to only oversample the minority class\nsmoted &lt;- performanceEstimation::smote(severe ~ ., data = train, \n                                            perc.over = .7, k = 5, perc.under = 7)\n# Over/under values:\n# .3 and 21 gave 50883 and 10501\n\n# .5 and 11 gave 44429 and 12117\n  # Sensitivity: .439\n\n# .7 and 7 gave 39578 and 13732\n  # Sensitivity: .539\n\n# Checking the class distribution in the final dataset\nsmoted %&gt;%\n  select(severe) %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  mutate(Proportion = Freq / sum(Freq)) %&gt;%\n  rename(Counts = Freq) %&gt;%\n  t()\n\n           [,1]        [,2]       \nsevere     \"0\"         \"1\"        \nCounts     \"39578\"     \"13732\"    \nProportion \"0.7424123\" \"0.2575877\"\n\n\nI used smote to generate additional records for the severe accidents, while downsampling the less severe accidents."
  },
  {
    "objectID": "projects/DM Project/Traffic_Accidents_TC.html",
    "href": "projects/DM Project/Traffic_Accidents_TC.html",
    "title": "Traffic Accidents in the Twin Cities Before and After COVID",
    "section": "",
    "text": "In Spring of 2020 there was a noticeable shift in traffic patterns as a result of the COVID-19 pandemic. Many people were no longer commuting to work everyday, and years later this trend continues as working from home becomes more accepted as a permanent change. Understanding how the shift in traffic patterns affects the characteristics of car accidents will allow us to better allocate public funds for traffic engineering projects and hopefully minimize the rate and severity of accidents.\nThis dataset contains records of traffic accidents for the seven-county region overseen by the metropolitan council of the Twin Cities.\nTraffic Accident data source: Sobhan Moosavi. (2023). US Accidents (2016 - 2023) [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DS/199387\nMinnesota shape files came from https://gisdata.mn.gov/organization/us-mn-state-dot\n\nLoading the data and creating new variables\n\n# read in the original, national dataset\n# US_Accidents_March23 &lt;- read_csv(\"US_Accidents_March23.csv\")\n\n# subset Minnesota\n# mn &lt;- US_Accidents_March23 %&gt;%\n# filter(State == \"MN\")\n# write_csv(mn, \"mn.csv\")\n# mn &lt;- read_csv(\"mn.csv\")\n# mn &lt;- mn %&gt;% select(-\"ID\", -\"Source\", -\"State\", -\"Country\")\n\n# subset counties in the Twin Cities metro area\n# met &lt;- mn %&gt;%\n#  filter(County %in% c('Anoka', 'Carver', 'Dakota', 'Hennepin', 'Ramsey',\n#                       'Scott', 'Washington'))\n# write_csv(met, \"met.csv\")\nmet = read_csv(\"met.csv\")\n\n# Sample 1000 random rows for testing purposes\n# met_1k &lt;- met %&gt;% sample_n(1000)\n# write_csv(met_1k, \"met_1k.csv\")\n\n# Load the shapefile for streets\nstreets &lt;- st_read(\"./streets/STREETS_LOAD.shp\")\n\n# Load the shape file for county boundaries\ncounties &lt;- st_read(\"counties/mn_county_boundaries_1000.shp\")\n\n\n# Creating New Variables\n# Create COVID variable\nmet$post_covid &lt;- ifelse(met$Start_Time &lt; as.POSIXct(\"2020-03-01\", tz = \"UTC-6\"), \n                      0, 1)\n\n# Day of week variable (0 = Sunday, 6 = Saturday)\nmet$Day_of_Week &lt;- as.integer(format(met$Start_Time, \"%w\"))\n\n# Create the binary 'weekday' variable (1 for weekdays, 0 for weekends)\nmet$is_weekday &lt;- ifelse(met$Day_of_Week &gt;= 1 & met$Day_of_Week &lt;= 5, 1, 0)\n\n# Month variable\nmet$Month &lt;- as.integer(format(met$Start_Time, \"%m\"))\n\n# Create the 'season' variable\nmet$season &lt;- ifelse(met$Month %in% c(3, 4, 5), \"Spring\",\n                ifelse(met$Month %in% c(6, 7, 8), \"Summer\",\n                  ifelse(met$Month %in% c(9, 10, 11), \"Autumn\",\n                    \"Winter\")))\n\n# Create rush hour variable\n# Extract hour from the Start_Time column\nmet$start_hour &lt;- as.integer(format(met$Start_Time, \"%H\"))\n\n# rush hour variable, 1 if the hour is between 6 and 9 or between 15 and 18 on weekdays, 0 otherwise\nmet$rush_hr &lt;- ifelse(met$is_weekday == 1 & (met$start_hour &gt;= 6 & met$start_hour &lt;= 9 | met$start_hour &gt;= 15 & met$start_hour &lt;= 18), 1, 0)\n\n# Drop no longer needed variables\nmet$Day_of_Week &lt;- NULL\nmet$Month &lt;- NULL\n\n\n# Convert the data frame to an sf object\nmet_sf &lt;- st_as_sf(met, coords = c(\"Start_Lng\", \"Start_Lat\"), crs = 4326, agr = \"constant\")\n\n# Check the CRS for both datasets\ncrs_streets &lt;- st_crs(streets)\ncrs_met &lt;- st_crs(met_sf)\n\n# Transform the projection of the accidents data to match the streets data, if different\nif (crs_streets$epsg != crs_met$epsg) {\n  met_sf &lt;- st_transform(met_sf, crs_streets)\n}\n\n# Remove extra dimensions from spatial data\nstreets &lt;- sf::st_zm(streets)\ncounties &lt;- sf::st_zm(counties)\nmet_sf &lt;- sf::st_zm(met_sf)\n\n\n\nExploratory data analysis\n\nLocations of Traffic Accidents in Twin Cities Metro (June 2016 through March 2023)\n\n# Get the bounding box of the accidents data\nbbox_met &lt;- st_bbox(met_sf)\n\n# Accident Map TC metro\nggplot() +\n  geom_sf(data = streets) + \n  geom_sf(data = counties, color = \"green4\", fill = NA, size = 5) +\n  geom_sf(data = met_sf, aes(color = factor(post_covid)), size = .8, alpha = .5) +\n  scale_color_manual(\n    values = c(\"0\" = \"#0066FF\", \"1\" = \"red\"),\n    name = \"COVID Period\",\n    labels = c(\"0\" = \"Pre-COVID\", \"1\" = \"Post-COVID\")\n  ) +\n  geom_sf_label(data = counties, aes(label = CTY_NAME), size = 4, color = \"green4\") +\n  coord_sf(xlim = bbox_met[c('xmin', 'xmax')], ylim = bbox_met[c('ymin', 'ymax')], expand = FALSE) +\n  ggtitle(\"Locations of Traffic Accidents in Twin Cities Metro (June 2016 through March 2023)\") +\n  theme(\n    plot.title = element_text(size = rel(1.5)),\n    axis.title.x = element_blank(), \n    axis.title.y = element_blank(),\n    legend.text = element_text(size = 12),\n    legend.title = element_text(size = 14)) +\n  guides(color = guide_legend(override.aes = list(size = 5)))\n\n\n\n\nIt appears that after COVID, a greater proportion of accidents may be happening in the suburbs as fewer people commute to the core cities. Indeed, pre-COVID 72.2% of accidents in the metro occurred in just Hennepin and Ramsey counties, while post-COVID the proportion was down to 69.1%\n\n\nTop 20 Streets by Number of Accidents, Before & After COVID\n\n# Filter accidents pre-covid\naccidents_pre_covid &lt;- met %&gt;%\n  filter(post_covid == 0)\n\n# Filter accidents post-covid\naccidents_post_covid &lt;- met %&gt;%\n  filter(post_covid == 1)\n\n# Get the count of accidents for each street pre-covid\ntop20_pre_covid &lt;- accidents_pre_covid %&gt;%\n  count(Street) %&gt;% \n  top_n(20, n)\n\n# Get the count of accidents for each street post-covid\ntop20_post_covid &lt;- accidents_post_covid %&gt;%\n  count(Street) %&gt;% \n  top_n(20, n) \n\n# Find the maximum number of accidents to set the same x-axis limit for both plots\nmax_accidents &lt;- max(c(top20_pre_covid$n, top20_post_covid$n))\n\n# Pre-COVID plot\nplot_pre_covid &lt;- ggplot(top20_pre_covid, aes(x = reorder(Street, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"#0066FF\") +\n  geom_text(aes(label = Street), vjust = 0.4, hjust = -0.2, color = \"black\", size = 3.0) +\n  coord_flip() +\n  labs(title = \"Top 20 Streets by Number of Accidents (Pre-COVID)\", x = NULL, y = NULL) +\n  ylim(0, max_accidents) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 0),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank()\n  )\n\n\n# Post-COVID plot\nplot_post_covid &lt;- ggplot(top20_post_covid, aes(x = reorder(Street, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"red\") +\n  geom_text(aes(label = Street), vjust = 0.4, hjust = -0.2, color = \"black\", size = 3.0) +\n  coord_flip() +\n  labs(title = \"Top 20 Streets by Number of Accidents (Post-COVID)\", x = NULL, y = \"Number of Accidents\") +\n  ylim(0, max_accidents) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 0),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank()\n  )\n\n# Align the plots vertically\ngrid.arrange(plot_pre_covid, plot_post_covid, ncol = 1)\n\n\n\n\nThese counts by street are for the entire periods of June 2016 through February 2020, and March 2020 through March 2023, pertaining to the pre and post COVID periods, respectively. This means there are 45 months worth of data for the pre-COVID period and 37 months of data for the post-COVID period.\n\n\nWeekly Accidents Rate in the Twin Cities Metro\n\n# Get weekly incidents by season\nweekly_incidents_by_season &lt;- met %&gt;%\n  mutate(Week = as.Date(floor_date(Start_Time, \"week\"))) %&gt;%\n  group_by(Week, season, post_covid) %&gt;%\n  summarise(Incidents = n(), .groups = 'drop') %&gt;%\n  ungroup() %&gt;%\n  arrange(Week, season, post_covid)\n\n# Reorder the 'season' variable\nweekly_incidents_by_season$season &lt;- factor(weekly_incidents_by_season$season, \n                                            levels = c(\"Spring\", \"Summer\", \"Autumn\", \"Winter\"))\n\n# Define date limits for the plot\nstart_date &lt;- as.Date(\"2016-06-01\")\nend_date &lt;- as.Date(\"2023-03-31\")  \n\n# Line plot\nggplot(weekly_incidents_by_season, aes(x = Week, y = Incidents)) +\n  geom_segment(aes(xend = lead(Week), yend = lead(Incidents), color = season), size = .5) +\n  scale_color_manual(name = \"Season\", \n                     values = c(\"Spring\" = \"green\", \"Summer\" = \"red\", \n                                \"Autumn\" = \"orange\", \"Winter\" = \"blue\")) +\n  ggtitle(\"Weekly Accident Rate in the Twin Cities Metro\") +\n  xlab(\"\") +\n  ylab(\"Number of Accidents\") +\n  scale_x_date(date_breaks = '1 month', date_labels = \"%b %Y\",\n               limits = c(start_date, end_date),\n               expand = c(0, 0)) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5, size = rel(0.8))) +\n  geom_smooth(data = subset(weekly_incidents_by_season, post_covid == 0),\n              aes(x = Week, y = Incidents, linetype = \"Pre-Covid\"), \n              method = \"lm\", se = FALSE, color = \"darkgray\") +\n   geom_smooth(data = subset(weekly_incidents_by_season, post_covid == 1),\n              aes(x = Week, y = Incidents, linetype = \"Post-Covid\"), \n              method = \"lm\", se = FALSE, color = \"black\") +\n   scale_linetype_manual(name = \"Trend Lines\",\n                        values = c(\"Pre-Covid\" = \"dashed\", \"Post-Covid\" = \"dashed\"),\n                        labels = c(\"Pre-Covid\", \"Post-Covid\"),\n                        guide = guide_legend(override.aes = list(color = c(\"Pre-Covid\" = \"darkgray\",   \"Post-Covid\" = \"black\"))))\n\n\n\n\nThe weekly accident rate was trending upwards pre-COVID, but there‚Äôs a noticeable shift during the post-COVID period as the trend seems to flatten out while the seasonality becomes more pronounced.\n\n\nNumber of Accidents by Hour of Day\n\n# Create data frames with counts of incidents for each hour, pre and post Covid\nhourly_incidents_pre_covid &lt;- met %&gt;%\n  filter(post_covid == 0) %&gt;%\n  group_by(start_hour) %&gt;%\n  summarise(Count = n())\n\nhourly_incidents_post_covid &lt;- met %&gt;%\n  filter(post_covid == 1) %&gt;%\n  group_by(start_hour) %&gt;%\n  summarise(Count = n())\n\n# Combine the data\ncombined_hourly_incidents &lt;- bind_rows(\n  mutate(hourly_incidents_pre_covid, Period = \"Pre-Covid\"),\n  mutate(hourly_incidents_post_covid, Period = \"Post-Covid\")\n)\n\n# Convert 'Period' to a factor with levels in the desired order\ncombined_hourly_incidents$Period &lt;- factor(combined_hourly_incidents$Period, \n                                           levels = c(\"Pre-Covid\", \"Post-Covid\"))\n\n# Line plot of incidents by hour of the day\nggplot(combined_hourly_incidents, aes(x=start_hour, y=Count, color=Period)) +\n  geom_smooth(se=F, method = \"loess\", span = 0.2) + \n  scale_color_manual(values=c(\"#0066FF\", \"red\")) +  \n  ggtitle(\"Number of Accidents by Hour of the Day\") +\n  xlab(\"Hour of the Day\") +\n  ylab(\"Number of Accidents\") +\n  theme_minimal() +\n  theme(legend.position=\"top\")\n\n\n\n\nReminder: there are 45 months worth of data for the pre-COVID period and 37 months of data for the post-COVID period. I believe the length of these two periods are close enough for general comparison purposes, but it‚Äôs important to note that all else being equal, there should be higher counts in the pre-COVID period. However, there are actually 65,459 accidents recorded post-COVID compared with only 56,054 pre-COVID.\n\n\nLength of Section of Road Affected When Accidents Occur\n\n# Boxplots for length of road affected\nggplot(met, aes(y = factor(post_covid, levels = c(1, 0)), x = `Distance(mi)`, fill = factor(post_covid, levels = c(1, 0)))) +\n  geom_boxplot(outlier.size = 1, orientation = \"y\") +\n  scale_fill_manual(values = c(\"red\", \"#0066FF\")) +\n  scale_y_discrete(labels = c(\"1\" = \"Post-Covid\", \"0\" = \"Pre-Covid\")) +\n  labs(title = \"Length of Section of Road Affected When Accidents Occur\",\n       y = \"\",\n       x = \"Distance (mi)\") +\n  theme_minimal() +\n  theme(\n    axis.title.y = element_blank(),\n    axis.text.y = element_text(angle = 45, hjust = 1),\n    legend.position = \"none\"\n  ) +\n  coord_cartesian(xlim = c(0, 2.5))\n\n\n\n\nFor each observed accident, the length in miles of the stretch of road that was affected was recorded. Before COVID, the median distance was 0 miles and the mean was 0.48 miles. After COVID, the median distance affected was 0.43 miles while the mean was 0.85 miles."
  }
]